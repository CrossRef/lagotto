[
  {
    "originalTitle": "Open Access and the Skewness of Science: It Can\u0027t Be Cream All the Way Down",
    "title": "Open Access and the Skewness of Science: It Can\u0027t Be Cream All the Way Down",
    "body": "\u003cp\u003eAuthor: Stevan Harnad\u003cbr/\u003ePosition: Canada Research Chair in Cognitive Sciences\u003cbr/\u003eInstitution: Universit\u0026eacute; du Qu\u0026eacute;bec \u0026agrave; Montr\u0026eacute;al \u0026amp; University of Southampton\u003cbr/\u003eE-mail: amsciforum@gmail.com\u003cbr/\u003eSubmitted Date: October 10, 2008\u003cbr/\u003ePublished Date: October 15, 2008\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eThere are reasons to be skeptical about the conclusions of this PLoS Medicine article. It says that science is compromised by insufficient \u0026quot;high impact\u0026quot; journals to publish in. The truth is that just about everything gets published somewhere among the planet\u0027s 25,000 peer reviewed journals, just not all in the top journals, which are, by definition, reserved for the top articles -- and not all articles can be top articles. The triage (peer review) is not perfect, so sometimes an article will appear lower (or higher) in the journal quality hierarchy than it ought to. But now that funders and universities are mandating Open Access, all research, top, middle and low will be accessible to everyone. This will correct any access inequities and it will also help remedy quality misassignment (inasmuch as lower quality journals may have fewer subscribers, and users may be less likely to consult lower quality journals). But it will not change the fact that 80% of citations (and presumably usage) goes to the top 20% of articles, though it may flatten this \u0026quot;skewness of science\u0026quot; (Seglen 1992) somewhat.\u003cbr/\u003e\u003cbr/\u003eSeglen PO (1992) The skewness of science. Journal of the American Society for Information Science 43:628-38\u003c/p\u003e",
    "originalBody": "Author: Stevan Harnad\nPosition: Canada Research Chair in Cognitive Sciences\nInstitution: Université du Québec à Montréal \u0026 University of Southampton\nE-mail: amsciforum@gmail.com\nSubmitted Date: October 10, 2008\nPublished Date: October 15, 2008\nThis comment was originally posted as a “Reader Response” on the publication date indicated above. All Reader Responses are now available as comments.\n\nThere are reasons to be skeptical about the conclusions of this PLoS Medicine article. It says that science is compromised by insufficient \"high impact\" journals to publish in. The truth is that just about everything gets published somewhere among the planet\u0027s 25,000 peer reviewed journals, just not all in the top journals, which are, by definition, reserved for the top articles -- and not all articles can be top articles. The triage (peer review) is not perfect, so sometimes an article will appear lower (or higher) in the journal quality hierarchy than it ought to. But now that funders and universities are mandating Open Access, all research, top, middle and low will be accessible to everyone. This will correct any access inequities and it will also help remedy quality misassignment (inasmuch as lower quality journals may have fewer subscribers, and users may be less likely to consult lower quality journals). But it will not change the fact that 80% of citations (and presumably usage) goes to the top 20% of articles, though it may flatten this \"skewness of science\" (Seglen 1992) somewhat.\n\nSeglen PO (1992) The skewness of science. Journal of the American Society for Information Science 43:628-38",
    "truncatedBody": "\u003cp\u003eAuthor: Stevan Harnad\u003cbr/\u003ePosition: Canada Research Chair in Cognitive Sciences\u003cbr/\u003eInstitution: Universit\u0026eacute; du Qu\u0026eacute;bec \u0026agrave; Montr\u0026eacute;al \u0026amp; University of Southampton\u003cbr/\u003eE-mail: amsciforum@gmail.com\u003cbr/\u003eSubmitted Date: October...\u003c/p\u003e",
    "bodyWithUrlLinkingNoPTags": "Author: Stevan Harnad\u003cbr/\u003ePosition: Canada Research Chair in Cognitive Sciences\u003cbr/\u003eInstitution: Universit\u0026eacute; du Qu\u0026eacute;bec \u0026agrave; Montr\u0026eacute;al \u0026amp; University of Southampton\u003cbr/\u003eE-mail: amsciforum@gmail.com\u003cbr/\u003eSubmitted Date: October 10, 2008\u003cbr/\u003ePublished Date: October 15, 2008\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eThere are reasons to be skeptical about the conclusions of this PLoS Medicine article. It says that science is compromised by insufficient \u0026quot;high impact\u0026quot; journals to publish in. The truth is that just about everything gets published somewhere among the planet\u0027s 25,000 peer reviewed journals, just not all in the top journals, which are, by definition, reserved for the top articles -- and not all articles can be top articles. The triage (peer review) is not perfect, so sometimes an article will appear lower (or higher) in the journal quality hierarchy than it ought to. But now that funders and universities are mandating Open Access, all research, top, middle and low will be accessible to everyone. This will correct any access inequities and it will also help remedy quality misassignment (inasmuch as lower quality journals may have fewer subscribers, and users may be less likely to consult lower quality journals). But it will not change the fact that 80% of citations (and presumably usage) goes to the top 20% of articles, though it may flatten this \u0026quot;skewness of science\u0026quot; (Seglen 1992) somewhat.\u003cbr/\u003e\u003cbr/\u003eSeglen PO (1992) The skewness of science. Journal of the American Society for Information Science 43:628-38",
    "truncatedBodyWithUrlLinkingNoPTags": "Author: Stevan Harnad\u003cbr/\u003ePosition: Canada Research Chair in Cognitive Sciences\u003cbr/\u003eInstitution: Universit\u0026eacute; du Qu\u0026eacute;bec \u0026agrave; Montr\u0026eacute;al \u0026amp; University of Southampton\u003cbr/\u003eE-mail: amsciforum@gmail.com\u003cbr/\u003eSubmitted Date: October...",
    "bodyWithHighlightedText": "\u003cp\u003eAuthor: Stevan Harnad\u003cbr/\u003ePosition: Canada Research Chair in Cognitive Sciences\u003cbr/\u003eInstitution: Universit\u0026eacute; du Qu\u0026eacute;bec \u0026agrave; Montr\u0026eacute;al \u0026amp; University of Southampton\u003cbr/\u003eE-mail: amsciforum@gmail.com\u003cbr/\u003eSubmitted Date: October 10, 2008\u003cbr/\u003ePublished Date: October 15, 2008\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eThere are reasons to be skeptical about the conclusions of this PLoS Medicine article. It says that science is compromised by insufficient \u0026quot;high impact\u0026quot; journals to publish in. The truth is that just about everything gets published somewhere among the planet\u0027s 25,000 peer reviewed journals, just not all in the top journals, which are, by definition, reserved for the top articles -- and not all articles can be top articles. The triage (peer review) is not perfect, so sometimes an article will appear lower (or higher) in the journal quality hierarchy than it ought to. But now that funders and universities are mandating Open Access, all research, top, middle and low will be accessible to everyone. This will correct any access inequities and it will also help remedy quality misassignment (inasmuch as lower quality journals may have fewer subscribers, and users may be less likely to consult lower quality journals). But it will not change the fact that 80% of citations (and presumably usage) goes to the top 20% of articles, though it may flatten this \u0026quot;skewness of science\u0026quot; (Seglen 1992) somewhat.\u003cbr/\u003e\u003cbr/\u003eSeglen PO (1992) The skewness of science. Journal of the American Society for Information Science 43:628-38\u003c/p\u003e",
    "competingInterestStatement": "",
    "truncatedCompetingInterestStatement": "",
    "annotationUri": "10.1371/annotation/177a9a89-9723-45a3-aac1-d27ca9deb664",
    "creatorID": 172479,
    "creatorDisplayName": "plosmedicine",
    "creatorFormattedName": "PLoS Medicine Staff",
    "articleID": 16292,
    "articleDoi": "info:doi/10.1371/journal.pmed.0020124",
    "articleTitle": "Why Most Published Research Findings Are False",
    "created": "2009-03-31T00:31:12Z",
    "createdFormatted": "2009-03-31T00:31:12Z",
    "type": "COMMENT",
    "replies": [],
    "lastReplyDate": "2009-03-31T00:31:12Z",
    "totalNumReplies": 0
  },
  {
    "originalTitle": "p values for significance must be more strict (as p\u003d\u003c0.005) to decrease false positives",
    "title": "p values for significance must be more strict (as p\u003d\u0026lt;0.005) to decrease false positives",
    "body": "\u003cp\u003eSince we don\u0027t really know how many hypotheses are behind the scene of any paper, it has been a big mistake of scientific community to cosider p\u003d0.05 as a gold standard.  It has created so many useless false postive claims.  We (and journals) must set much more stringent significance level as p\u003d0.005 or even p\u003d0.001. (still not to the level of GWAS).  This will force researchers to design large well-powered studies.  It still does not solve nonrandom bias and confounding, but at least small bad studies will disappear, which will decrease the profound degree of publication bias we are seeing today. \u003c/p\u003e",
    "originalBody": "Since we don\u0027t really know how many hypotheses are behind the scene of any paper, it has been a big mistake of scientific community to cosider p\u003d0.05 as a gold standard.  It has created so many useless false postive claims.  We (and journals) must set much more stringent significance level as p\u003d0.005 or even p\u003d0.001. (still not to the level of GWAS).  This will force researchers to design large well-powered studies.  It still does not solve nonrandom bias and confounding, but at least small bad studies will disappear, which will decrease the profound degree of publication bias we are seeing today. ",
    "truncatedBody": "\u003cp\u003eSince we don\u0027t really know how many hypotheses are behind the scene of any paper, it has been a big mistake of scientific community to cosider p\u003d0.05 as a gold standard.  It has created so many useless false postive claims.  We (and journals) must set...\u003c/p\u003e",
    "bodyWithUrlLinkingNoPTags": "Since we don\u0027t really know how many hypotheses are behind the scene of any paper, it has been a big mistake of scientific community to cosider p\u003d0.05 as a gold standard.  It has created so many useless false postive claims.  We (and journals) must set much more stringent significance level as p\u003d0.005 or even p\u003d0.001. (still not to the level of GWAS).  This will force researchers to design large well-powered studies.  It still does not solve nonrandom bias and confounding, but at least small bad studies will disappear, which will decrease the profound degree of publication bias we are seeing today. ",
    "truncatedBodyWithUrlLinkingNoPTags": "Since we don\u0027t really know how many hypotheses are behind the scene of any paper, it has been a big mistake of scientific community to cosider p\u003d0.05 as a gold standard.  It has created so many useless false postive claims.  We (and journals) must set...",
    "bodyWithHighlightedText": "\u003cp\u003eSince we don\u0027t really know how many hypotheses are behind the scene of any paper, it has been a big mistake of scientific community to cosider p\u003d0.05 as a gold standard.  It has created so many useless false postive claims.  We (and journals) must set much more stringent significance level as p\u003d0.005 or even p\u003d0.001. (still not to the level of GWAS).  This will force researchers to design large well-powered studies.  It still does not solve nonrandom bias and confounding, but at least small bad studies will disappear, which will decrease the profound degree of publication bias we are seeing today. \u003c/p\u003e",
    "competingInterestStatement": "",
    "truncatedCompetingInterestStatement": "",
    "annotationUri": "10.1371/annotation/1e68ccb2-95e4-48c5-9b19-523bb246c191",
    "creatorID": 109551,
    "creatorDisplayName": "sogino",
    "creatorFormattedName": "Shuji Ogino",
    "articleID": 16292,
    "articleDoi": "info:doi/10.1371/journal.pmed.0020124",
    "articleTitle": "Why Most Published Research Findings Are False",
    "created": "2011-03-10T16:12:59Z",
    "createdFormatted": "2011-03-10T16:12:59Z",
    "type": "COMMENT",
    "replies": [],
    "lastReplyDate": "2011-03-10T16:12:59Z",
    "totalNumReplies": 0
  },
  {
    "originalTitle": "Medicine only?",
    "title": "Medicine only?",
    "body": "\u003cp\u003eAuthor: Mathias Klode\u003cbr/\u003ePosition: Graduate Student\u003cbr/\u003eInstitution: University of Hamburg\u003cbr/\u003eE-mail: MathiasKlode@lodae.de\u003cbr/\u003eSubmitted Date: February 27, 2007\u003cbr/\u003ePublished Date: February 28, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eI wonder if Mr. Ioannidis realizes the effect this essay has on the public opinion of science. What troubles me most is the way the titles seems to state that half of all research findings are false, but in the article itself, all data discussed is taken from medical studies or trials.\u003cbr/\u003e\u003cbr/\u003eI think - as the article is published in a medical journal - it is natural to the author, that mostly medicine-related publications are meant. The same holds true for the attentive reader.\u003cbr/\u003e\u003cbr/\u003eBut in any case, the title and even the abstract is leading to the false conclusion, that this essay is about scientific publications in general. And it is discussed under this premise by the media.\u003cbr/\u003e\u003cbr/\u003eAs a last humble note, I would like to point out that the longing for scientific truth (which does not really exist, as we hopefully all know) demands for papers and studies stating results that could be erroneous. What else could we falsify? I see no point in underlining this principle as if it were something undesirable and surprising. It should be obvious to every scientist.\u003c/p\u003e",
    "originalBody": "Author: Mathias Klode\nPosition: Graduate Student\nInstitution: University of Hamburg\nE-mail: MathiasKlode@lodae.de\nSubmitted Date: February 27, 2007\nPublished Date: February 28, 2007\nThis comment was originally posted as a “Reader Response” on the publication date indicated above. All Reader Responses are now available as comments.\n\nI wonder if Mr. Ioannidis realizes the effect this essay has on the public opinion of science. What troubles me most is the way the titles seems to state that half of all research findings are false, but in the article itself, all data discussed is taken from medical studies or trials.\n\nI think - as the article is published in a medical journal - it is natural to the author, that mostly medicine-related publications are meant. The same holds true for the attentive reader.\n\nBut in any case, the title and even the abstract is leading to the false conclusion, that this essay is about scientific publications in general. And it is discussed under this premise by the media.\n\nAs a last humble note, I would like to point out that the longing for scientific truth (which does not really exist, as we hopefully all know) demands for papers and studies stating results that could be erroneous. What else could we falsify? I see no point in underlining this principle as if it were something undesirable and surprising. It should be obvious to every scientist.",
    "truncatedBody": "\u003cp\u003eAuthor: Mathias Klode\u003cbr/\u003ePosition: Graduate Student\u003cbr/\u003eInstitution: University of Hamburg\u003cbr/\u003eE-mail: MathiasKlode@lodae.de\u003cbr/\u003eSubmitted Date: February 27, 2007\u003cbr/\u003ePublished Date: February 28, 2007\u003cbr/\u003eThis comment was originally posted as a...\u003c/p\u003e",
    "bodyWithUrlLinkingNoPTags": "Author: Mathias Klode\u003cbr/\u003ePosition: Graduate Student\u003cbr/\u003eInstitution: University of Hamburg\u003cbr/\u003eE-mail: MathiasKlode@lodae.de\u003cbr/\u003eSubmitted Date: February 27, 2007\u003cbr/\u003ePublished Date: February 28, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eI wonder if Mr. Ioannidis realizes the effect this essay has on the public opinion of science. What troubles me most is the way the titles seems to state that half of all research findings are false, but in the article itself, all data discussed is taken from medical studies or trials.\u003cbr/\u003e\u003cbr/\u003eI think - as the article is published in a medical journal - it is natural to the author, that mostly medicine-related publications are meant. The same holds true for the attentive reader.\u003cbr/\u003e\u003cbr/\u003eBut in any case, the title and even the abstract is leading to the false conclusion, that this essay is about scientific publications in general. And it is discussed under this premise by the media.\u003cbr/\u003e\u003cbr/\u003eAs a last humble note, I would like to point out that the longing for scientific truth (which does not really exist, as we hopefully all know) demands for papers and studies stating results that could be erroneous. What else could we falsify? I see no point in underlining this principle as if it were something undesirable and surprising. It should be obvious to every scientist.",
    "truncatedBodyWithUrlLinkingNoPTags": "Author: Mathias Klode\u003cbr/\u003ePosition: Graduate Student\u003cbr/\u003eInstitution: University of Hamburg\u003cbr/\u003eE-mail: MathiasKlode@lodae.de\u003cbr/\u003eSubmitted Date: February 27, 2007\u003cbr/\u003ePublished Date: February 28, 2007\u003cbr/\u003eThis comment was originally posted as a...",
    "bodyWithHighlightedText": "\u003cp\u003eAuthor: Mathias Klode\u003cbr/\u003ePosition: Graduate Student\u003cbr/\u003eInstitution: University of Hamburg\u003cbr/\u003eE-mail: MathiasKlode@lodae.de\u003cbr/\u003eSubmitted Date: February 27, 2007\u003cbr/\u003ePublished Date: February 28, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eI wonder if Mr. Ioannidis realizes the effect this essay has on the public opinion of science. What troubles me most is the way the titles seems to state that half of all research findings are false, but in the article itself, all data discussed is taken from medical studies or trials.\u003cbr/\u003e\u003cbr/\u003eI think - as the article is published in a medical journal - it is natural to the author, that mostly medicine-related publications are meant. The same holds true for the attentive reader.\u003cbr/\u003e\u003cbr/\u003eBut in any case, the title and even the abstract is leading to the false conclusion, that this essay is about scientific publications in general. And it is discussed under this premise by the media.\u003cbr/\u003e\u003cbr/\u003eAs a last humble note, I would like to point out that the longing for scientific truth (which does not really exist, as we hopefully all know) demands for papers and studies stating results that could be erroneous. What else could we falsify? I see no point in underlining this principle as if it were something undesirable and surprising. It should be obvious to every scientist.\u003c/p\u003e",
    "competingInterestStatement": "",
    "truncatedCompetingInterestStatement": "",
    "annotationUri": "10.1371/annotation/232aaaad-ff1b-41df-9870-ecda8908fc14",
    "creatorID": 172479,
    "creatorDisplayName": "plosmedicine",
    "creatorFormattedName": "PLoS Medicine Staff",
    "articleID": 16292,
    "articleDoi": "info:doi/10.1371/journal.pmed.0020124",
    "articleTitle": "Why Most Published Research Findings Are False",
    "created": "2009-03-31T00:06:16Z",
    "createdFormatted": "2009-03-31T00:06:16Z",
    "type": "COMMENT",
    "replies": [],
    "lastReplyDate": "2009-03-31T00:06:16Z",
    "totalNumReplies": 0
  },
  {
    "originalTitle": "Why Most Published Research Findings Are False",
    "title": "Why Most Published Research Findings Are False",
    "body": "\u003cp\u003eWould that include the research findings of this article??\u003c/p\u003e",
    "originalBody": "Would that include the research findings of this article??",
    "truncatedBody": "\u003cp\u003eWould that include the research findings of this article??\u003c/p\u003e",
    "bodyWithUrlLinkingNoPTags": "Would that include the research findings of this article??",
    "truncatedBodyWithUrlLinkingNoPTags": "Would that include the research findings of this article??",
    "bodyWithHighlightedText": "\u003cp\u003eWould that include the research findings of this article??\u003c/p\u003e",
    "competingInterestStatement": "",
    "truncatedCompetingInterestStatement": "",
    "annotationUri": "10.1371/annotation/2ed631fd-93c2-40e2-b8a6-59f2ec797e7e",
    "creatorID": 51283,
    "creatorDisplayName": "keithkojo",
    "creatorFormattedName": "keith grimaldi",
    "articleID": 16292,
    "articleDoi": "info:doi/10.1371/journal.pmed.0020124",
    "articleTitle": "Why Most Published Research Findings Are False",
    "created": "2010-01-14T10:17:26Z",
    "createdFormatted": "2010-01-14T10:17:26Z",
    "type": "COMMENT",
    "replies": [],
    "lastReplyDate": "2010-01-14T10:17:26Z",
    "totalNumReplies": 0
  },
  {
    "originalTitle": "Greek Common Sense",
    "title": "Greek Common Sense",
    "body": "\u003cp\u003eAuthor: GEOFFREY RIDLEY BARROW\u003cbr/\u003ePosition: GADFLY\u003cbr/\u003eInstitution: No affiliation was given\u003cbr/\u003eE-mail: geoffreybarrow@msn.com\u003cbr/\u003eSubmitted Date: September 16, 2007\u003cbr/\u003ePublished Date: September 18, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eOne tires of hearing self proclaimed and media pumped experts toot about their alleged truths.\u003cbr/\u003e\u003cbr/\u003eHow lucky to find a good American paper (Science Journal,  Robert Lee Hotz, WSJ 9/14/07) that once again has the courage to prick the bubbles of education and research when discussing and reflecting upon Dr. Ioannidis\u0027 Plos Medicine paper.\u003cbr/\u003e\u003cbr/\u003eYou can never judge the marmalade by the label on the jar!\u003c/p\u003e",
    "originalBody": "Author: GEOFFREY RIDLEY BARROW\nPosition: GADFLY\nInstitution: No affiliation was given\nE-mail: geoffreybarrow@msn.com\nSubmitted Date: September 16, 2007\nPublished Date: September 18, 2007\nThis comment was originally posted as a “Reader Response” on the publication date indicated above. All Reader Responses are now available as comments.\n\nOne tires of hearing self proclaimed and media pumped experts toot about their alleged truths.\n\nHow lucky to find a good American paper (Science Journal,  Robert Lee Hotz, WSJ 9/14/07) that once again has the courage to prick the bubbles of education and research when discussing and reflecting upon Dr. Ioannidis\u0027 Plos Medicine paper.\n\nYou can never judge the marmalade by the label on the jar!",
    "truncatedBody": "\u003cp\u003eAuthor: GEOFFREY RIDLEY BARROW\u003cbr/\u003ePosition: GADFLY\u003cbr/\u003eInstitution: No affiliation was given\u003cbr/\u003eE-mail: geoffreybarrow@msn.com\u003cbr/\u003eSubmitted Date: September 16, 2007\u003cbr/\u003ePublished Date: September 18, 2007\u003cbr/\u003eThis comment was originally posted as a...\u003c/p\u003e",
    "bodyWithUrlLinkingNoPTags": "Author: GEOFFREY RIDLEY BARROW\u003cbr/\u003ePosition: GADFLY\u003cbr/\u003eInstitution: No affiliation was given\u003cbr/\u003eE-mail: geoffreybarrow@msn.com\u003cbr/\u003eSubmitted Date: September 16, 2007\u003cbr/\u003ePublished Date: September 18, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eOne tires of hearing self proclaimed and media pumped experts toot about their alleged truths.\u003cbr/\u003e\u003cbr/\u003eHow lucky to find a good American paper (Science Journal,  Robert Lee Hotz, WSJ 9/14/07) that once again has the courage to prick the bubbles of education and research when discussing and reflecting upon Dr. Ioannidis\u0027 Plos Medicine paper.\u003cbr/\u003e\u003cbr/\u003eYou can never judge the marmalade by the label on the jar!",
    "truncatedBodyWithUrlLinkingNoPTags": "Author: GEOFFREY RIDLEY BARROW\u003cbr/\u003ePosition: GADFLY\u003cbr/\u003eInstitution: No affiliation was given\u003cbr/\u003eE-mail: geoffreybarrow@msn.com\u003cbr/\u003eSubmitted Date: September 16, 2007\u003cbr/\u003ePublished Date: September 18, 2007\u003cbr/\u003eThis comment was originally posted as a...",
    "bodyWithHighlightedText": "\u003cp\u003eAuthor: GEOFFREY RIDLEY BARROW\u003cbr/\u003ePosition: GADFLY\u003cbr/\u003eInstitution: No affiliation was given\u003cbr/\u003eE-mail: geoffreybarrow@msn.com\u003cbr/\u003eSubmitted Date: September 16, 2007\u003cbr/\u003ePublished Date: September 18, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eOne tires of hearing self proclaimed and media pumped experts toot about their alleged truths.\u003cbr/\u003e\u003cbr/\u003eHow lucky to find a good American paper (Science Journal,  Robert Lee Hotz, WSJ 9/14/07) that once again has the courage to prick the bubbles of education and research when discussing and reflecting upon Dr. Ioannidis\u0027 Plos Medicine paper.\u003cbr/\u003e\u003cbr/\u003eYou can never judge the marmalade by the label on the jar!\u003c/p\u003e",
    "competingInterestStatement": "",
    "truncatedCompetingInterestStatement": "",
    "annotationUri": "10.1371/annotation/33477a96-7871-43b9-84dd-1fb7f2288a44",
    "creatorID": 172479,
    "creatorDisplayName": "plosmedicine",
    "creatorFormattedName": "PLoS Medicine Staff",
    "articleID": 16292,
    "articleDoi": "info:doi/10.1371/journal.pmed.0020124",
    "articleTitle": "Why Most Published Research Findings Are False",
    "created": "2009-03-31T00:14:02Z",
    "createdFormatted": "2009-03-31T00:14:02Z",
    "type": "COMMENT",
    "replies": [],
    "lastReplyDate": "2009-03-31T00:14:02Z",
    "totalNumReplies": 0
  },
  {
    "originalTitle": "Clinical trials on human subjects can never be definitively conclusive",
    "title": "Clinical trials on human subjects can never be definitively conclusive",
    "body": "\u003cp\u003eAuthor: Vijay Singh\u003cbr/\u003ePosition: Doctor; government job\u003cbr/\u003eInstitution: CCRH\u003cbr/\u003eE-mail: drvpsingh2004@hotmail.com\u003cbr/\u003eSubmitted Date: September 01, 2005\u003cbr/\u003ePublished Date: September 12, 2005\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eNothing surprising here about clinical trials. They continue to be as inclonclusive as they were in the last century. We need to realize that, where human beings are involved as research subjects and as investigators, there is every possibility of false perception, judgmental error and individual\u0027s own interest, which may be anything from fame to money. No wonder that recently there was a headline suggesting the most drugs do not do what they are meant to do. On the contrary some of them do just opposite. The practice of medicine is, as well as a science, an art. The same drug brings about different changes when prescribed by different physicians. \u003c/p\u003e",
    "originalBody": "Author: Vijay Singh\nPosition: Doctor; government job\nInstitution: CCRH\nE-mail: drvpsingh2004@hotmail.com\nSubmitted Date: September 01, 2005\nPublished Date: September 12, 2005\nThis comment was originally posted as a “Reader Response” on the publication date indicated above. All Reader Responses are now available as comments.\n\nNothing surprising here about clinical trials. They continue to be as inclonclusive as they were in the last century. We need to realize that, where human beings are involved as research subjects and as investigators, there is every possibility of false perception, judgmental error and individual\u0027s own interest, which may be anything from fame to money. No wonder that recently there was a headline suggesting the most drugs do not do what they are meant to do. On the contrary some of them do just opposite. The practice of medicine is, as well as a science, an art. The same drug brings about different changes when prescribed by different physicians. ",
    "truncatedBody": "\u003cp\u003eAuthor: Vijay Singh\u003cbr/\u003ePosition: Doctor; government job\u003cbr/\u003eInstitution: CCRH\u003cbr/\u003eE-mail: drvpsingh2004@hotmail.com\u003cbr/\u003eSubmitted Date: September 01, 2005\u003cbr/\u003ePublished Date: September 12, 2005\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader...\u003c/p\u003e",
    "bodyWithUrlLinkingNoPTags": "Author: Vijay Singh\u003cbr/\u003ePosition: Doctor; government job\u003cbr/\u003eInstitution: CCRH\u003cbr/\u003eE-mail: drvpsingh2004@hotmail.com\u003cbr/\u003eSubmitted Date: September 01, 2005\u003cbr/\u003ePublished Date: September 12, 2005\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eNothing surprising here about clinical trials. They continue to be as inclonclusive as they were in the last century. We need to realize that, where human beings are involved as research subjects and as investigators, there is every possibility of false perception, judgmental error and individual\u0027s own interest, which may be anything from fame to money. No wonder that recently there was a headline suggesting the most drugs do not do what they are meant to do. On the contrary some of them do just opposite. The practice of medicine is, as well as a science, an art. The same drug brings about different changes when prescribed by different physicians. ",
    "truncatedBodyWithUrlLinkingNoPTags": "Author: Vijay Singh\u003cbr/\u003ePosition: Doctor; government job\u003cbr/\u003eInstitution: CCRH\u003cbr/\u003eE-mail: drvpsingh2004@hotmail.com\u003cbr/\u003eSubmitted Date: September 01, 2005\u003cbr/\u003ePublished Date: September 12, 2005\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader...",
    "bodyWithHighlightedText": "\u003cp\u003eAuthor: Vijay Singh\u003cbr/\u003ePosition: Doctor; government job\u003cbr/\u003eInstitution: CCRH\u003cbr/\u003eE-mail: drvpsingh2004@hotmail.com\u003cbr/\u003eSubmitted Date: September 01, 2005\u003cbr/\u003ePublished Date: September 12, 2005\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eNothing surprising here about clinical trials. They continue to be as inclonclusive as they were in the last century. We need to realize that, where human beings are involved as research subjects and as investigators, there is every possibility of false perception, judgmental error and individual\u0027s own interest, which may be anything from fame to money. No wonder that recently there was a headline suggesting the most drugs do not do what they are meant to do. On the contrary some of them do just opposite. The practice of medicine is, as well as a science, an art. The same drug brings about different changes when prescribed by different physicians. \u003c/p\u003e",
    "competingInterestStatement": "",
    "truncatedCompetingInterestStatement": "",
    "annotationUri": "10.1371/annotation/35b9bba3-9760-4fe7-b5ed-186a2e568ba2",
    "creatorID": 172479,
    "creatorDisplayName": "plosmedicine",
    "creatorFormattedName": "PLoS Medicine Staff",
    "articleID": 16292,
    "articleDoi": "info:doi/10.1371/journal.pmed.0020124",
    "articleTitle": "Why Most Published Research Findings Are False",
    "created": "2009-03-30T23:45:54Z",
    "createdFormatted": "2009-03-30T23:45:54Z",
    "type": "COMMENT",
    "replies": [],
    "lastReplyDate": "2009-03-30T23:45:54Z",
    "totalNumReplies": 0
  },
  {
    "originalTitle": "This only applies for p-values not corrected for multiple testing",
    "title": "This only applies for p-values not corrected for multiple testing",
    "body": "\u003cp\u003eThis calculation assumes that the p-value of 0.05 is not corrected for multiple hypothesis testing. I\u0027m not an expert in association studies, but I think it is considered a bad practice to use non-corrected p-values. If any correction for multiple hypothesis testing was applied (e.g. Bonferroni or Benjamini-Hochberg) the resulting ppv would be altered by 5 orders of magnitude...\u003c/p\u003e",
    "originalBody": "This calculation assumes that the p-value of 0.05 is not corrected for multiple hypothesis testing. I\u0027m not an expert in association studies, but I think it is considered a bad practice to use non-corrected p-values. If any correction for multiple hypothesis testing was applied (e.g. Bonferroni or Benjamini-Hochberg) the resulting ppv would be altered by 5 orders of magnitude...",
    "truncatedBody": "\u003cp\u003eThis calculation assumes that the p-value of 0.05 is not corrected for multiple hypothesis testing. I\u0027m not an expert in association studies, but I think it is considered a bad practice to use non-corrected p-values. If any correction for multiple...\u003c/p\u003e",
    "bodyWithUrlLinkingNoPTags": "This calculation assumes that the p-value of 0.05 is not corrected for multiple hypothesis testing. I\u0027m not an expert in association studies, but I think it is considered a bad practice to use non-corrected p-values. If any correction for multiple hypothesis testing was applied (e.g. Bonferroni or Benjamini-Hochberg) the resulting ppv would be altered by 5 orders of magnitude...",
    "truncatedBodyWithUrlLinkingNoPTags": "This calculation assumes that the p-value of 0.05 is not corrected for multiple hypothesis testing. I\u0027m not an expert in association studies, but I think it is considered a bad practice to use non-corrected p-values. If any correction for multiple...",
    "bodyWithHighlightedText": "\u003cp\u003e\u003cem\u003eLet us also suppose that the study has 60% power to find an association with an odds ratio of 1.3 at \u0026alpha; \u003d 0.05. Then it can be estimated that if a statistically significant association is found with the p-value barely crossing the 0.05 threshold, the post-study probability that this is true increases about 12-fold compared with the pre-study probability, but it is still only 12 \u0026times; 10\u0026minus;4.\u003c/em\u003e\u003cbr/\u003e\u003ca href\u003d\"http://plosmedicine.org/article/info:doi/10.1371/journal.pmed.0020124#article1.body1.sec5.boxed-text1.sec1.p1\"\u003ehttp://plosmedicine.org/article/info:doi/10.1371/journal.pmed.0020124#article1.body1.sec5.boxed-text1.sec1.p1\u003c/a\u003e\u003cbr/\u003e\u003cbr/\u003eThis calculation assumes that the p-value of 0.05 is not corrected for multiple hypothesis testing. I\u0027m not an expert in association studies, but I think it is considered a bad practice to use non-corrected p-values. If any correction for multiple hypothesis testing was applied (e.g. Bonferroni or Benjamini-Hochberg) the resulting ppv would be altered by 5 orders of magnitude...\u003c/p\u003e",
    "competingInterestStatement": "",
    "truncatedCompetingInterestStatement": "",
    "annotationUri": "10.1371/annotation/41bb3c51-2e80-4b0c-9a5d-6af9f9f3b966",
    "creatorID": 21969,
    "creatorDisplayName": "barwil",
    "creatorFormattedName": "Bartek Wilczynski",
    "articleID": 16292,
    "articleDoi": "info:doi/10.1371/journal.pmed.0020124",
    "articleTitle": "Why Most Published Research Findings Are False",
    "created": "2009-10-13T12:10:34Z",
    "createdFormatted": "2009-10-13T12:10:34Z",
    "type": "COMMENT",
    "replies": [],
    "lastReplyDate": "2009-10-13T12:10:34Z",
    "totalNumReplies": 0
  },
  {
    "originalTitle": "Multiple research teams with multiple research findings",
    "title": "Multiple research teams with multiple research findings",
    "body": "\u003cp\u003eAuthor: Gang Zheng\u003cbr/\u003ePosition: Mathematical Statistician\u003cbr/\u003eInstitution: Office of Biostatistical Research, National Heart, Lung and Blood Institute\u003cbr/\u003eE-mail: zhengg@nhlbi.nih.gov\u003cbr/\u003eSubmitted Date: October 03, 2006\u003cbr/\u003ePublished Date: October 9, 2006\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eIoannidis (2005) examined several important factors that lead to false research findings using the post prediction value (PPV), the probability of a true relationship (TR) given a significant research finding (SRF). One of the important factors he examined is that n research teams in the scientific field independently tested the same research hypothesis and at least one of the teams reported SFR. Ioannidis demonstrated that the PPV, defined as the probability of a TR given that at least one team obtains a SRF, decreases when n increases.\u003cbr/\u003e\u003cbr/\u003eHowever, when more research teams test the same hypothesis independently with the same statistical power (1-beta) and Type I error (alpha), it is also likely that more research teams would report the SRF. Here I examine the PPV, defined as the probability of a TR given that k research teams report SRF (1\u0026amp;lt; k \u0026amp;lt; n). Following the notation of Ioannidis, the probability of a SRF is P(SRF) \u003d [R/(R+1)](1-beta) + [1/(R+1)]alpha, where R/(R+1) and 1/(R+1) are the prior probabilities of a TR and a null relationship, respectively. Assume that all n research teams conduct the same research independently under the same conditions. Then, k/n is approximately equal to P(SRF) when n is large. This implies that k increases with n and is greater than 1 when P(SRF) is not too small (or when R is not too small). I derived a new table, like Tables 1-3 of Ioannidis, from which a new PPV is obtained, given by PPV\u003d[R(1-beta)^k beta^(n-k)]/[R(1-beta)^kbeta^(n-k)+alpha^k(1-alpha)^(n-k)], where \u0026quot;^\u0026quot; is used for power. Figures of the above PPV can be plotted against R. I found that, when Type I error alpha \u003d 0.05, power 1-beta \u003d 0.80, and R \u0026amp;gt; 0.7, the PPV increases when n increases.\u003cbr/\u003e\u003cbr/\u003eOne application of the above analysis is that we should encourage more research teams to conduct independent replication study to confirm the results of a previous study as the pre-study odds, R, is usually greater than 1. This would be particularly useful for replication studies of a genome-wide association study with 100K-500K SNPs.\u003cbr/\u003e\u003cbr/\u003eReference\u003cbr/\u003eIoannidis JPA (2005) Why most published research findings are false. PLoS Med 2(8): e124.\u003c/p\u003e",
    "originalBody": "Author: Gang Zheng\nPosition: Mathematical Statistician\nInstitution: Office of Biostatistical Research, National Heart, Lung and Blood Institute\nE-mail: zhengg@nhlbi.nih.gov\nSubmitted Date: October 03, 2006\nPublished Date: October 9, 2006\nThis comment was originally posted as a “Reader Response” on the publication date indicated above. All Reader Responses are now available as comments.\n\nIoannidis (2005) examined several important factors that lead to false research findings using the post prediction value (PPV), the probability of a true relationship (TR) given a significant research finding (SRF). One of the important factors he examined is that n research teams in the scientific field independently tested the same research hypothesis and at least one of the teams reported SFR. Ioannidis demonstrated that the PPV, defined as the probability of a TR given that at least one team obtains a SRF, decreases when n increases.\n\nHowever, when more research teams test the same hypothesis independently with the same statistical power (1-beta) and Type I error (alpha), it is also likely that more research teams would report the SRF. Here I examine the PPV, defined as the probability of a TR given that k research teams report SRF (1\u0026lt; k \u0026lt; n). Following the notation of Ioannidis, the probability of a SRF is P(SRF) \u003d [R/(R+1)](1-beta) + [1/(R+1)]alpha, where R/(R+1) and 1/(R+1) are the prior probabilities of a TR and a null relationship, respectively. Assume that all n research teams conduct the same research independently under the same conditions. Then, k/n is approximately equal to P(SRF) when n is large. This implies that k increases with n and is greater than 1 when P(SRF) is not too small (or when R is not too small). I derived a new table, like Tables 1-3 of Ioannidis, from which a new PPV is obtained, given by PPV\u003d[R(1-beta)^k beta^(n-k)]/[R(1-beta)^kbeta^(n-k)+alpha^k(1-alpha)^(n-k)], where \"^\" is used for power. Figures of the above PPV can be plotted against R. I found that, when Type I error alpha \u003d 0.05, power 1-beta \u003d 0.80, and R \u0026gt; 0.7, the PPV increases when n increases.\n\nOne application of the above analysis is that we should encourage more research teams to conduct independent replication study to confirm the results of a previous study as the pre-study odds, R, is usually greater than 1. This would be particularly useful for replication studies of a genome-wide association study with 100K-500K SNPs.\n\nReference\nIoannidis JPA (2005) Why most published research findings are false. PLoS Med 2(8): e124.",
    "truncatedBody": "\u003cp\u003eAuthor: Gang Zheng\u003cbr/\u003ePosition: Mathematical Statistician\u003cbr/\u003eInstitution: Office of Biostatistical Research, National Heart, Lung and Blood Institute\u003cbr/\u003eE-mail: zhengg@nhlbi.nih.gov\u003cbr/\u003eSubmitted Date: October 03, 2006\u003cbr/\u003ePublished Date: October 9,...\u003c/p\u003e",
    "bodyWithUrlLinkingNoPTags": "Author: Gang Zheng\u003cbr/\u003ePosition: Mathematical Statistician\u003cbr/\u003eInstitution: Office of Biostatistical Research, National Heart, Lung and Blood Institute\u003cbr/\u003eE-mail: zhengg@nhlbi.nih.gov\u003cbr/\u003eSubmitted Date: October 03, 2006\u003cbr/\u003ePublished Date: October 9, 2006\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eIoannidis (2005) examined several important factors that lead to false research findings using the post prediction value (PPV), the probability of a true relationship (TR) given a significant research finding (SRF). One of the important factors he examined is that n research teams in the scientific field independently tested the same research hypothesis and at least one of the teams reported SFR. Ioannidis demonstrated that the PPV, defined as the probability of a TR given that at least one team obtains a SRF, decreases when n increases.\u003cbr/\u003e\u003cbr/\u003eHowever, when more research teams test the same hypothesis independently with the same statistical power (1-beta) and Type I error (alpha), it is also likely that more research teams would report the SRF. Here I examine the PPV, defined as the probability of a TR given that k research teams report SRF (1\u0026amp;lt; k \u0026amp;lt; n). Following the notation of Ioannidis, the probability of a SRF is P(SRF) \u003d [R/(R+1)](1-beta) + [1/(R+1)]alpha, where R/(R+1) and 1/(R+1) are the prior probabilities of a TR and a null relationship, respectively. Assume that all n research teams conduct the same research independently under the same conditions. Then, k/n is approximately equal to P(SRF) when n is large. This implies that k increases with n and is greater than 1 when P(SRF) is not too small (or when R is not too small). I derived a new table, like Tables 1-3 of Ioannidis, from which a new PPV is obtained, given by PPV\u003d[R(1-beta)^k beta^(n-k)]/[R(1-beta)^kbeta^(n-k)+alpha^k(1-alpha)^(n-k)], where \u0026quot;^\u0026quot; is used for power. Figures of the above PPV can be plotted against R. I found that, when Type I error alpha \u003d 0.05, power 1-beta \u003d 0.80, and R \u0026amp;gt; 0.7, the PPV increases when n increases.\u003cbr/\u003e\u003cbr/\u003eOne application of the above analysis is that we should encourage more research teams to conduct independent replication study to confirm the results of a previous study as the pre-study odds, R, is usually greater than 1. This would be particularly useful for replication studies of a genome-wide association study with 100K-500K SNPs.\u003cbr/\u003e\u003cbr/\u003eReference\u003cbr/\u003eIoannidis JPA (2005) Why most published research findings are false. PLoS Med 2(8): e124.",
    "truncatedBodyWithUrlLinkingNoPTags": "Author: Gang Zheng\u003cbr/\u003ePosition: Mathematical Statistician\u003cbr/\u003eInstitution: Office of Biostatistical Research, National Heart, Lung and Blood Institute\u003cbr/\u003eE-mail: zhengg@nhlbi.nih.gov\u003cbr/\u003eSubmitted Date: October 03, 2006\u003cbr/\u003ePublished Date: October 9,...",
    "bodyWithHighlightedText": "\u003cp\u003eAuthor: Gang Zheng\u003cbr/\u003ePosition: Mathematical Statistician\u003cbr/\u003eInstitution: Office of Biostatistical Research, National Heart, Lung and Blood Institute\u003cbr/\u003eE-mail: zhengg@nhlbi.nih.gov\u003cbr/\u003eSubmitted Date: October 03, 2006\u003cbr/\u003ePublished Date: October 9, 2006\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eIoannidis (2005) examined several important factors that lead to false research findings using the post prediction value (PPV), the probability of a true relationship (TR) given a significant research finding (SRF). One of the important factors he examined is that n research teams in the scientific field independently tested the same research hypothesis and at least one of the teams reported SFR. Ioannidis demonstrated that the PPV, defined as the probability of a TR given that at least one team obtains a SRF, decreases when n increases.\u003cbr/\u003e\u003cbr/\u003eHowever, when more research teams test the same hypothesis independently with the same statistical power (1-beta) and Type I error (alpha), it is also likely that more research teams would report the SRF. Here I examine the PPV, defined as the probability of a TR given that k research teams report SRF (1\u0026amp;lt; k \u0026amp;lt; n). Following the notation of Ioannidis, the probability of a SRF is P(SRF) \u003d [R/(R+1)](1-beta) + [1/(R+1)]alpha, where R/(R+1) and 1/(R+1) are the prior probabilities of a TR and a null relationship, respectively. Assume that all n research teams conduct the same research independently under the same conditions. Then, k/n is approximately equal to P(SRF) when n is large. This implies that k increases with n and is greater than 1 when P(SRF) is not too small (or when R is not too small). I derived a new table, like Tables 1-3 of Ioannidis, from which a new PPV is obtained, given by PPV\u003d[R(1-beta)^k beta^(n-k)]/[R(1-beta)^kbeta^(n-k)+alpha^k(1-alpha)^(n-k)], where \u0026quot;^\u0026quot; is used for power. Figures of the above PPV can be plotted against R. I found that, when Type I error alpha \u003d 0.05, power 1-beta \u003d 0.80, and R \u0026amp;gt; 0.7, the PPV increases when n increases.\u003cbr/\u003e\u003cbr/\u003eOne application of the above analysis is that we should encourage more research teams to conduct independent replication study to confirm the results of a previous study as the pre-study odds, R, is usually greater than 1. This would be particularly useful for replication studies of a genome-wide association study with 100K-500K SNPs.\u003cbr/\u003e\u003cbr/\u003eReference\u003cbr/\u003eIoannidis JPA (2005) Why most published research findings are false. PLoS Med 2(8): e124.\u003c/p\u003e",
    "competingInterestStatement": "I declare that I have no competing interests.",
    "truncatedCompetingInterestStatement": "I declare that I have no competing interests.",
    "annotationUri": "10.1371/annotation/45c16cf7-7741-4f29-9a9a-a39fb30791b9",
    "creatorID": 172479,
    "creatorDisplayName": "plosmedicine",
    "creatorFormattedName": "PLoS Medicine Staff",
    "articleID": 16292,
    "articleDoi": "info:doi/10.1371/journal.pmed.0020124",
    "articleTitle": "Why Most Published Research Findings Are False",
    "created": "2009-03-31T00:00:35Z",
    "createdFormatted": "2009-03-31T00:00:35Z",
    "type": "COMMENT",
    "replies": [],
    "lastReplyDate": "2009-03-31T00:00:35Z",
    "totalNumReplies": 0
  },
  {
    "originalTitle": "Truth, probability and frameworks",
    "title": "Truth, probability and frameworks",
    "body": "\u003cp\u003eAuthor: Jonathan Wren\u003cbr/\u003eInstitution: Dept. of Botany \u0026amp; Microbiology University of Oklahoma\u003cbr/\u003eE-mail: jonathan.wren@ou.edu\u003cbr/\u003eSubmitted Date: September 12, 2005\u003cbr/\u003ePublished Date: September 12, 2005\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eJames T. Kirk: Harry lied to you, Norman. Everything Harry says is a lie. Remember that, Norman: Everything he says is a lie.\u003cbr/\u003e\u003cbr/\u003eHarry Mudd: Now I want you to listen to me very carefully, Norman: I... am... lying.\u003cbr/\u003e\u003cbr/\u003e-From Star Trek, the episode \u0026quot;I, Mudd\u0026quot;\u003cbr/\u003e\u003cbr/\u003eAlthough John Ioannidis[1] brings up several good points about over-reliance upon formal - yet arbitrary - statistical cut-offs and bias against the reporting of negative results, his claim that most published research findings are false is somewhat paradoxical. Ironically, the truer his premise is, the less likely his conclusions are. He, after all, relies heavily upon other studies to support his premise, so if most (i.e. \u0026gt;50%) of his cited studies are themselves false (including the 8/37 that are to his own work), then his argument is automatically on shaky grounds. As mentioned in the editorial[2], scientific studies don\u0027t offer truth, per se. Even when studies appear in the best journals, they offer probabilistic assertions. Ioannidis\u0027 statement that \u0027the probability that a research finding is indeed true depends on the prior probability of it being true\u0027 is really begging the question; this, after all, is the problem. We cannot know such probabilities a priori, and guessing at such probabilities and/or parameters (as he does in his SNP association example) surely could not be less biased than any statistical test of significance. The key problem in Ioannidis\u0027 PPV formula to calculate the post-study probability that a relationship is true (PPV \u003d (1 - ??)R/(R - ??R + ??), where R is the ratio of true to non-relationships), is that one can postulate a near-infinite number of non-relationships. Just extending his SNP example, why assume each SNP acts independently? So rather than 99,990 SNPs not being associated with schizophrenia, we have potentially 99,990n not associated, where n is the number of potentially interacting SNPs. As n grows, R becomes very small very quickly and PPV effectively zero. Taken to the extreme, this would imply that all empirical studies are fruitless. One of the most important factors in moving towards the truth that was not discussed is fitting discoveries into a framework. Optimally, if a relationship is true, it should have more than one implication, permitting validation from multiple angles. For example, a SNP causally associated with schizophrenia must affect something on the molecular level, whether genomic, transcriptional, post-transcriptional, translational or post-translational. In turn, these molecules should interact differently with each other and/or with other molecules within the cell and/or within a tissue and/or system as a whole. If Norman, the android from Star Trek mentioned in the beginning quote, had been equipped with the capacity to evaluate statements within a framework, he never would have short-circuited as a result of Kirk\u0027s paradox. He could have entertained the possibility that either Kirk was lying about Harry or that Harry\u0027s statement was incomplete (i.e. lying about what?) Similarly, repeatedly examining and re-examining any particular study to resolve the true/not-true paradox via statistical arguments alone can short-circuit our patience. We should simultaneously seek to identify the framework by which implications can be tested and I would argue that the more important the finding, the more testable implications it has.\u003cbr/\u003e\u003cbr/\u003eReferences\u003cbr/\u003e[1] Ioannidis JP (2005). Why Most Published Research Findings Are False. PLoS Med. Aug 9;2(8):e124.\u003cbr/\u003e[2] PLoS Medicine Editors (2005. Minimizing mistakes and embracing uncertainty. PLoS Med 2(8):e272.\u003c/p\u003e",
    "originalBody": "Author: Jonathan Wren\nInstitution: Dept. of Botany \u0026 Microbiology University of Oklahoma\nE-mail: jonathan.wren@ou.edu\nSubmitted Date: September 12, 2005\nPublished Date: September 12, 2005\nThis comment was originally posted as a “Reader Response” on the publication date indicated above. All Reader Responses are now available as comments.\n\nJames T. Kirk: Harry lied to you, Norman. Everything Harry says is a lie. Remember that, Norman: Everything he says is a lie.\n\nHarry Mudd: Now I want you to listen to me very carefully, Norman: I... am... lying.\n\n-From Star Trek, the episode \"I, Mudd\"\n\nAlthough John Ioannidis[1] brings up several good points about over-reliance upon formal - yet arbitrary - statistical cut-offs and bias against the reporting of negative results, his claim that most published research findings are false is somewhat paradoxical. Ironically, the truer his premise is, the less likely his conclusions are. He, after all, relies heavily upon other studies to support his premise, so if most (i.e. \u003e50%) of his cited studies are themselves false (including the 8/37 that are to his own work), then his argument is automatically on shaky grounds. As mentioned in the editorial[2], scientific studies don\u0027t offer truth, per se. Even when studies appear in the best journals, they offer probabilistic assertions. Ioannidis\u0027 statement that \u0027the probability that a research finding is indeed true depends on the prior probability of it being true\u0027 is really begging the question; this, after all, is the problem. We cannot know such probabilities a priori, and guessing at such probabilities and/or parameters (as he does in his SNP association example) surely could not be less biased than any statistical test of significance. The key problem in Ioannidis\u0027 PPV formula to calculate the post-study probability that a relationship is true (PPV \u003d (1 - ??)R/(R - ??R + ??), where R is the ratio of true to non-relationships), is that one can postulate a near-infinite number of non-relationships. Just extending his SNP example, why assume each SNP acts independently? So rather than 99,990 SNPs not being associated with schizophrenia, we have potentially 99,990n not associated, where n is the number of potentially interacting SNPs. As n grows, R becomes very small very quickly and PPV effectively zero. Taken to the extreme, this would imply that all empirical studies are fruitless. One of the most important factors in moving towards the truth that was not discussed is fitting discoveries into a framework. Optimally, if a relationship is true, it should have more than one implication, permitting validation from multiple angles. For example, a SNP causally associated with schizophrenia must affect something on the molecular level, whether genomic, transcriptional, post-transcriptional, translational or post-translational. In turn, these molecules should interact differently with each other and/or with other molecules within the cell and/or within a tissue and/or system as a whole. If Norman, the android from Star Trek mentioned in the beginning quote, had been equipped with the capacity to evaluate statements within a framework, he never would have short-circuited as a result of Kirk\u0027s paradox. He could have entertained the possibility that either Kirk was lying about Harry or that Harry\u0027s statement was incomplete (i.e. lying about what?) Similarly, repeatedly examining and re-examining any particular study to resolve the true/not-true paradox via statistical arguments alone can short-circuit our patience. We should simultaneously seek to identify the framework by which implications can be tested and I would argue that the more important the finding, the more testable implications it has.\n\nReferences\n[1] Ioannidis JP (2005). Why Most Published Research Findings Are False. PLoS Med. Aug 9;2(8):e124.\n[2] PLoS Medicine Editors (2005. Minimizing mistakes and embracing uncertainty. PLoS Med 2(8):e272.",
    "truncatedBody": "\u003cp\u003eAuthor: Jonathan Wren\u003cbr/\u003eInstitution: Dept. of Botany \u0026amp; Microbiology University of Oklahoma\u003cbr/\u003eE-mail: jonathan.wren@ou.edu\u003cbr/\u003eSubmitted Date: September 12, 2005\u003cbr/\u003ePublished Date: September 12, 2005\u003cbr/\u003eThis comment was originally posted as a...\u003c/p\u003e",
    "bodyWithUrlLinkingNoPTags": "Author: Jonathan Wren\u003cbr/\u003eInstitution: Dept. of Botany \u0026amp; Microbiology University of Oklahoma\u003cbr/\u003eE-mail: jonathan.wren@ou.edu\u003cbr/\u003eSubmitted Date: September 12, 2005\u003cbr/\u003ePublished Date: September 12, 2005\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eJames T. Kirk: Harry lied to you, Norman. Everything Harry says is a lie. Remember that, Norman: Everything he says is a lie.\u003cbr/\u003e\u003cbr/\u003eHarry Mudd: Now I want you to listen to me very carefully, Norman: I... am... lying.\u003cbr/\u003e\u003cbr/\u003e-From Star Trek, the episode \u0026quot;I, Mudd\u0026quot;\u003cbr/\u003e\u003cbr/\u003eAlthough John Ioannidis[1] brings up several good points about over-reliance upon formal - yet arbitrary - statistical cut-offs and bias against the reporting of negative results, his claim that most published research findings are false is somewhat paradoxical. Ironically, the truer his premise is, the less likely his conclusions are. He, after all, relies heavily upon other studies to support his premise, so if most (i.e. \u0026gt;50%) of his cited studies are themselves false (including the 8/37 that are to his own work), then his argument is automatically on shaky grounds. As mentioned in the editorial[2], scientific studies don\u0027t offer truth, per se. Even when studies appear in the best journals, they offer probabilistic assertions. Ioannidis\u0027 statement that \u0027the probability that a research finding is indeed true depends on the prior probability of it being true\u0027 is really begging the question; this, after all, is the problem. We cannot know such probabilities a priori, and guessing at such probabilities and/or parameters (as he does in his SNP association example) surely could not be less biased than any statistical test of significance. The key problem in Ioannidis\u0027 PPV formula to calculate the post-study probability that a relationship is true (PPV \u003d (1 - ??)R/(R - ??R + ??), where R is the ratio of true to non-relationships), is that one can postulate a near-infinite number of non-relationships. Just extending his SNP example, why assume each SNP acts independently? So rather than 99,990 SNPs not being associated with schizophrenia, we have potentially 99,990n not associated, where n is the number of potentially interacting SNPs. As n grows, R becomes very small very quickly and PPV effectively zero. Taken to the extreme, this would imply that all empirical studies are fruitless. One of the most important factors in moving towards the truth that was not discussed is fitting discoveries into a framework. Optimally, if a relationship is true, it should have more than one implication, permitting validation from multiple angles. For example, a SNP causally associated with schizophrenia must affect something on the molecular level, whether genomic, transcriptional, post-transcriptional, translational or post-translational. In turn, these molecules should interact differently with each other and/or with other molecules within the cell and/or within a tissue and/or system as a whole. If Norman, the android from Star Trek mentioned in the beginning quote, had been equipped with the capacity to evaluate statements within a framework, he never would have short-circuited as a result of Kirk\u0027s paradox. He could have entertained the possibility that either Kirk was lying about Harry or that Harry\u0027s statement was incomplete (i.e. lying about what?) Similarly, repeatedly examining and re-examining any particular study to resolve the true/not-true paradox via statistical arguments alone can short-circuit our patience. We should simultaneously seek to identify the framework by which implications can be tested and I would argue that the more important the finding, the more testable implications it has.\u003cbr/\u003e\u003cbr/\u003eReferences\u003cbr/\u003e[1] Ioannidis JP (2005). Why Most Published Research Findings Are False. PLoS Med. Aug 9;2(8):e124.\u003cbr/\u003e[2] PLoS Medicine Editors (2005. Minimizing mistakes and embracing uncertainty. PLoS Med 2(8):e272.",
    "truncatedBodyWithUrlLinkingNoPTags": "Author: Jonathan Wren\u003cbr/\u003eInstitution: Dept. of Botany \u0026amp; Microbiology University of Oklahoma\u003cbr/\u003eE-mail: jonathan.wren@ou.edu\u003cbr/\u003eSubmitted Date: September 12, 2005\u003cbr/\u003ePublished Date: September 12, 2005\u003cbr/\u003eThis comment was originally posted as a...",
    "bodyWithHighlightedText": "\u003cp\u003eAuthor: Jonathan Wren\u003cbr/\u003eInstitution: Dept. of Botany \u0026amp; Microbiology University of Oklahoma\u003cbr/\u003eE-mail: jonathan.wren@ou.edu\u003cbr/\u003eSubmitted Date: September 12, 2005\u003cbr/\u003ePublished Date: September 12, 2005\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eJames T. Kirk: Harry lied to you, Norman. Everything Harry says is a lie. Remember that, Norman: Everything he says is a lie.\u003cbr/\u003e\u003cbr/\u003eHarry Mudd: Now I want you to listen to me very carefully, Norman: I... am... lying.\u003cbr/\u003e\u003cbr/\u003e-From Star Trek, the episode \u0026quot;I, Mudd\u0026quot;\u003cbr/\u003e\u003cbr/\u003eAlthough John Ioannidis[1] brings up several good points about over-reliance upon formal - yet arbitrary - statistical cut-offs and bias against the reporting of negative results, his claim that most published research findings are false is somewhat paradoxical. Ironically, the truer his premise is, the less likely his conclusions are. He, after all, relies heavily upon other studies to support his premise, so if most (i.e. \u0026gt;50%) of his cited studies are themselves false (including the 8/37 that are to his own work), then his argument is automatically on shaky grounds. As mentioned in the editorial[2], scientific studies don\u0027t offer truth, per se. Even when studies appear in the best journals, they offer probabilistic assertions. Ioannidis\u0027 statement that \u0027the probability that a research finding is indeed true depends on the prior probability of it being true\u0027 is really begging the question; this, after all, is the problem. We cannot know such probabilities a priori, and guessing at such probabilities and/or parameters (as he does in his SNP association example) surely could not be less biased than any statistical test of significance. The key problem in Ioannidis\u0027 PPV formula to calculate the post-study probability that a relationship is true (PPV \u003d (1 - ??)R/(R - ??R + ??), where R is the ratio of true to non-relationships), is that one can postulate a near-infinite number of non-relationships. Just extending his SNP example, why assume each SNP acts independently? So rather than 99,990 SNPs not being associated with schizophrenia, we have potentially 99,990n not associated, where n is the number of potentially interacting SNPs. As n grows, R becomes very small very quickly and PPV effectively zero. Taken to the extreme, this would imply that all empirical studies are fruitless. One of the most important factors in moving towards the truth that was not discussed is fitting discoveries into a framework. Optimally, if a relationship is true, it should have more than one implication, permitting validation from multiple angles. For example, a SNP causally associated with schizophrenia must affect something on the molecular level, whether genomic, transcriptional, post-transcriptional, translational or post-translational. In turn, these molecules should interact differently with each other and/or with other molecules within the cell and/or within a tissue and/or system as a whole. If Norman, the android from Star Trek mentioned in the beginning quote, had been equipped with the capacity to evaluate statements within a framework, he never would have short-circuited as a result of Kirk\u0027s paradox. He could have entertained the possibility that either Kirk was lying about Harry or that Harry\u0027s statement was incomplete (i.e. lying about what?) Similarly, repeatedly examining and re-examining any particular study to resolve the true/not-true paradox via statistical arguments alone can short-circuit our patience. We should simultaneously seek to identify the framework by which implications can be tested and I would argue that the more important the finding, the more testable implications it has.\u003cbr/\u003e\u003cbr/\u003eReferences\u003cbr/\u003e[1] Ioannidis JP (2005). Why Most Published Research Findings Are False. PLoS Med. Aug 9;2(8):e124.\u003cbr/\u003e[2] PLoS Medicine Editors (2005. Minimizing mistakes and embracing uncertainty. PLoS Med 2(8):e272.\u003c/p\u003e",
    "competingInterestStatement": "",
    "truncatedCompetingInterestStatement": "",
    "annotationUri": "10.1371/annotation/4f45dcac-a2cb-46cc-83ec-63dfdc86f1ef",
    "creatorID": 172479,
    "creatorDisplayName": "plosmedicine",
    "creatorFormattedName": "PLoS Medicine Staff",
    "articleID": 16292,
    "articleDoi": "info:doi/10.1371/journal.pmed.0020124",
    "articleTitle": "Why Most Published Research Findings Are False",
    "created": "2009-03-30T23:45:50Z",
    "createdFormatted": "2009-03-30T23:45:50Z",
    "type": "COMMENT",
    "replies": [],
    "lastReplyDate": "2009-03-30T23:45:50Z",
    "totalNumReplies": 0
  },
  {
    "originalTitle": "Power, Reliability and Heterogeneous Results",
    "title": "Power, Reliability and Heterogeneous Results",
    "body": "\u003cp\u003eAuthor: Ian Shrier\u003cbr/\u003ePosition: physician, researcher\u003cbr/\u003eInstitution: McGill University\u003cbr/\u003eE-mail: ian.shrier@mcgill.ca\u003cbr/\u003eSubmitted Date: September 14, 2005\u003cbr/\u003ePublished Date: September 15, 2005\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eI want to congratulate Dr. Ioannidis on his thought provoking article. I have two comments.\u003cbr/\u003e\u003cbr/\u003eIn Corollary 1, he suggests that small sample sizes mean smaller power and implies that larger studies with thousands of subjects are more likely to be true. I think it is important to stress that if the effect size is large (e.g. very small variance that is seen in physiological studies), then adequate power is obtained with small numbers. Further, some would argue that exposing subjects to research risks unnecessarily (e.g. when fewer subjects would yield sufficient power) is unethical. Since the analysis is based on power, we should remember that larger is not always better.\u003cbr/\u003e\u003cbr/\u003eIn corollary 4, Dr. Ioannidis argues that greater flexibility in designs, definitions, etc means the results are less likely to be true. I agree that replication of all aspects of the study is more likely to yield consistent results, but this does not necessarily mean true results. Since we don\u0027t know a priori which methodological details are most appropriate (e.g. dose, timing, etc), heterogenous results from different designs is an important source of information and can lead to a new and more in-depth understanding of the subject - and sometimes even paradigm shifts. I agree with the accompanying editorial to the article that we need to distinguish between the validity of the data and the validity of the authors\u0027 conclusions.\u003c/p\u003e",
    "originalBody": "Author: Ian Shrier\nPosition: physician, researcher\nInstitution: McGill University\nE-mail: ian.shrier@mcgill.ca\nSubmitted Date: September 14, 2005\nPublished Date: September 15, 2005\nThis comment was originally posted as a “Reader Response” on the publication date indicated above. All Reader Responses are now available as comments.\n\nI want to congratulate Dr. Ioannidis on his thought provoking article. I have two comments.\n\nIn Corollary 1, he suggests that small sample sizes mean smaller power and implies that larger studies with thousands of subjects are more likely to be true. I think it is important to stress that if the effect size is large (e.g. very small variance that is seen in physiological studies), then adequate power is obtained with small numbers. Further, some would argue that exposing subjects to research risks unnecessarily (e.g. when fewer subjects would yield sufficient power) is unethical. Since the analysis is based on power, we should remember that larger is not always better.\n\nIn corollary 4, Dr. Ioannidis argues that greater flexibility in designs, definitions, etc means the results are less likely to be true. I agree that replication of all aspects of the study is more likely to yield consistent results, but this does not necessarily mean true results. Since we don\u0027t know a priori which methodological details are most appropriate (e.g. dose, timing, etc), heterogenous results from different designs is an important source of information and can lead to a new and more in-depth understanding of the subject - and sometimes even paradigm shifts. I agree with the accompanying editorial to the article that we need to distinguish between the validity of the data and the validity of the authors\u0027 conclusions.",
    "truncatedBody": "\u003cp\u003eAuthor: Ian Shrier\u003cbr/\u003ePosition: physician, researcher\u003cbr/\u003eInstitution: McGill University\u003cbr/\u003eE-mail: ian.shrier@mcgill.ca\u003cbr/\u003eSubmitted Date: September 14, 2005\u003cbr/\u003ePublished Date: September 15, 2005\u003cbr/\u003eThis comment was originally posted as a...\u003c/p\u003e",
    "bodyWithUrlLinkingNoPTags": "Author: Ian Shrier\u003cbr/\u003ePosition: physician, researcher\u003cbr/\u003eInstitution: McGill University\u003cbr/\u003eE-mail: ian.shrier@mcgill.ca\u003cbr/\u003eSubmitted Date: September 14, 2005\u003cbr/\u003ePublished Date: September 15, 2005\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eI want to congratulate Dr. Ioannidis on his thought provoking article. I have two comments.\u003cbr/\u003e\u003cbr/\u003eIn Corollary 1, he suggests that small sample sizes mean smaller power and implies that larger studies with thousands of subjects are more likely to be true. I think it is important to stress that if the effect size is large (e.g. very small variance that is seen in physiological studies), then adequate power is obtained with small numbers. Further, some would argue that exposing subjects to research risks unnecessarily (e.g. when fewer subjects would yield sufficient power) is unethical. Since the analysis is based on power, we should remember that larger is not always better.\u003cbr/\u003e\u003cbr/\u003eIn corollary 4, Dr. Ioannidis argues that greater flexibility in designs, definitions, etc means the results are less likely to be true. I agree that replication of all aspects of the study is more likely to yield consistent results, but this does not necessarily mean true results. Since we don\u0027t know a priori which methodological details are most appropriate (e.g. dose, timing, etc), heterogenous results from different designs is an important source of information and can lead to a new and more in-depth understanding of the subject - and sometimes even paradigm shifts. I agree with the accompanying editorial to the article that we need to distinguish between the validity of the data and the validity of the authors\u0027 conclusions.",
    "truncatedBodyWithUrlLinkingNoPTags": "Author: Ian Shrier\u003cbr/\u003ePosition: physician, researcher\u003cbr/\u003eInstitution: McGill University\u003cbr/\u003eE-mail: ian.shrier@mcgill.ca\u003cbr/\u003eSubmitted Date: September 14, 2005\u003cbr/\u003ePublished Date: September 15, 2005\u003cbr/\u003eThis comment was originally posted as a...",
    "bodyWithHighlightedText": "\u003cp\u003eAuthor: Ian Shrier\u003cbr/\u003ePosition: physician, researcher\u003cbr/\u003eInstitution: McGill University\u003cbr/\u003eE-mail: ian.shrier@mcgill.ca\u003cbr/\u003eSubmitted Date: September 14, 2005\u003cbr/\u003ePublished Date: September 15, 2005\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eI want to congratulate Dr. Ioannidis on his thought provoking article. I have two comments.\u003cbr/\u003e\u003cbr/\u003eIn Corollary 1, he suggests that small sample sizes mean smaller power and implies that larger studies with thousands of subjects are more likely to be true. I think it is important to stress that if the effect size is large (e.g. very small variance that is seen in physiological studies), then adequate power is obtained with small numbers. Further, some would argue that exposing subjects to research risks unnecessarily (e.g. when fewer subjects would yield sufficient power) is unethical. Since the analysis is based on power, we should remember that larger is not always better.\u003cbr/\u003e\u003cbr/\u003eIn corollary 4, Dr. Ioannidis argues that greater flexibility in designs, definitions, etc means the results are less likely to be true. I agree that replication of all aspects of the study is more likely to yield consistent results, but this does not necessarily mean true results. Since we don\u0027t know a priori which methodological details are most appropriate (e.g. dose, timing, etc), heterogenous results from different designs is an important source of information and can lead to a new and more in-depth understanding of the subject - and sometimes even paradigm shifts. I agree with the accompanying editorial to the article that we need to distinguish between the validity of the data and the validity of the authors\u0027 conclusions.\u003c/p\u003e",
    "competingInterestStatement": "None",
    "truncatedCompetingInterestStatement": "None",
    "annotationUri": "10.1371/annotation/54a4ac80-b03e-41c8-9080-7dcde4356803",
    "creatorID": 172479,
    "creatorDisplayName": "plosmedicine",
    "creatorFormattedName": "PLoS Medicine Staff",
    "articleID": 16292,
    "articleDoi": "info:doi/10.1371/journal.pmed.0020124",
    "articleTitle": "Why Most Published Research Findings Are False",
    "created": "2009-03-30T23:46:14Z",
    "createdFormatted": "2009-03-30T23:46:14Z",
    "type": "COMMENT",
    "replies": [],
    "lastReplyDate": "2009-03-30T23:46:14Z",
    "totalNumReplies": 0
  },
  {
    "originalTitle": "My Opinion",
    "title": "My Opinion",
    "body": "\u003cp\u003eAuthor: Daniel Hulme\u003cbr/\u003ePosition: dental student\u003cbr/\u003eInstitution: UCSF\u003cbr/\u003eE-mail: hulme.dan@gmail.com\u003cbr/\u003eSubmitted Date: October 04, 2007\u003cbr/\u003ePublished Date: October 8, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eI agree that much of the data gathered through scientic research is biased and scewed to some extent. I believe that sometimes researchers have alterior motives, whatever they may be, leading to incomplete conclusions and false data that is not always consistantly obtainable by multiple groups.\u003c/p\u003e",
    "originalBody": "Author: Daniel Hulme\nPosition: dental student\nInstitution: UCSF\nE-mail: hulme.dan@gmail.com\nSubmitted Date: October 04, 2007\nPublished Date: October 8, 2007\nThis comment was originally posted as a “Reader Response” on the publication date indicated above. All Reader Responses are now available as comments.\n\nI agree that much of the data gathered through scientic research is biased and scewed to some extent. I believe that sometimes researchers have alterior motives, whatever they may be, leading to incomplete conclusions and false data that is not always consistantly obtainable by multiple groups.",
    "truncatedBody": "\u003cp\u003eAuthor: Daniel Hulme\u003cbr/\u003ePosition: dental student\u003cbr/\u003eInstitution: UCSF\u003cbr/\u003eE-mail: hulme.dan@gmail.com\u003cbr/\u003eSubmitted Date: October 04, 2007\u003cbr/\u003ePublished Date: October 8, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo;...\u003c/p\u003e",
    "bodyWithUrlLinkingNoPTags": "Author: Daniel Hulme\u003cbr/\u003ePosition: dental student\u003cbr/\u003eInstitution: UCSF\u003cbr/\u003eE-mail: hulme.dan@gmail.com\u003cbr/\u003eSubmitted Date: October 04, 2007\u003cbr/\u003ePublished Date: October 8, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eI agree that much of the data gathered through scientic research is biased and scewed to some extent. I believe that sometimes researchers have alterior motives, whatever they may be, leading to incomplete conclusions and false data that is not always consistantly obtainable by multiple groups.",
    "truncatedBodyWithUrlLinkingNoPTags": "Author: Daniel Hulme\u003cbr/\u003ePosition: dental student\u003cbr/\u003eInstitution: UCSF\u003cbr/\u003eE-mail: hulme.dan@gmail.com\u003cbr/\u003eSubmitted Date: October 04, 2007\u003cbr/\u003ePublished Date: October 8, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo;...",
    "bodyWithHighlightedText": "\u003cp\u003eAuthor: Daniel Hulme\u003cbr/\u003ePosition: dental student\u003cbr/\u003eInstitution: UCSF\u003cbr/\u003eE-mail: hulme.dan@gmail.com\u003cbr/\u003eSubmitted Date: October 04, 2007\u003cbr/\u003ePublished Date: October 8, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eI agree that much of the data gathered through scientic research is biased and scewed to some extent. I believe that sometimes researchers have alterior motives, whatever they may be, leading to incomplete conclusions and false data that is not always consistantly obtainable by multiple groups.\u003c/p\u003e",
    "competingInterestStatement": "",
    "truncatedCompetingInterestStatement": "",
    "annotationUri": "10.1371/annotation/5ce31bbc-9888-4d27-8c76-9246b01199f5",
    "creatorID": 172479,
    "creatorDisplayName": "plosmedicine",
    "creatorFormattedName": "PLoS Medicine Staff",
    "articleID": 16292,
    "articleDoi": "info:doi/10.1371/journal.pmed.0020124",
    "articleTitle": "Why Most Published Research Findings Are False",
    "created": "2009-03-31T00:14:50Z",
    "createdFormatted": "2009-03-31T00:14:50Z",
    "type": "COMMENT",
    "replies": [],
    "lastReplyDate": "2009-03-31T00:14:50Z",
    "totalNumReplies": 0
  },
  {
    "originalTitle": "Why most published research findings are true but so many are useless",
    "title": "Why most published research findings are true but so many are useless",
    "body": "\u003cp\u003eAuthor: Yonatan Loewenstein\u003cbr/\u003ePosition: No occupation was given\u003cbr/\u003eInstitution: Howard Hughes Medical Institute and Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, Cambridge MA 02139\u003cbr/\u003eE-mail: yonatanl@mit.edu\u003cbr/\u003eSubmitted Date: March 01, 2006\u003cbr/\u003ePublished Date: March 7, 2006\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eJohn P. A. Ioannidis makes an interesting claim that most research findings are false\u0026lt;sup\u0026gt;1\u0026lt;/sup\u0026gt;. Much of his argument is based on the assumption that the pre-study probability that a single hypothesis is correct is small and that the failure to incorporate this prior information in the estimation of the post-priori likelihood of the hypothesis to be correct leads to erroneous conclusions in the vast majority of studies. I argue that the prior probabilities for many of the hypotheses tested are large and therefore only a minority of the published findings is false. However, these published hypotheses are, in many of these cases, of little scientific or clinical interest.\u003cbr/\u003e\u003cbr/\u003eConsider a hypothetical epidemiological study of exploratory nature that attempts to find correlations between external factors and a specific medical condition. A positive, publishable, result in such a study is the finding of correlation between a specific external factor and a change in the medical condition: for example an improvement, suggesting a causal relation between the two. To support this hypothesis, a statistical test is presented to show that the likelihood that the external factor is uncorrelated with the improvement in the medical condition is low. What is the a-priori probability that the hypothesis is correct? Biological systems, such as biochemical or genetic networks, are characterized by a high degree of interconnectivity. Thus perturbations in one node of the network tend to propagate throughout the entire network.\u003cbr/\u003e\u003cbr/\u003eSince the subject is a biological system, any external factor is almost sure to have some effect on any medical condition. It is the direction of the effect, whether it improves the condition or worsens it, and its magnitude that are the unknown variables. In the absence of any mechanistic model, the parsimonious assumption is that both of the alternatives, improvement and worsening, are equally likely and therefore the prior of the hypothesis that the external factor is correlated with an improvement of the condition is 0.5.\u003cbr/\u003e\u003cbr/\u003eThus, when any supporting experimental evidence is incorporated with the prior, the post-priory probability that the hypothesis is true is larger than 0.5, Therefore, most published research hypotheses are correct, even if the stated p-value in these studies is overconfident, as a result of the biases described by Ioannidis.\u003cbr/\u003e\u003cbr/\u003eWhat can be learned from a study that presents correlations between the external factor and the change in the medical condition with a high level of confidence p? Not much. The level of confidence depends as much on the level of true correlations as it depends on the size of the dataset. The null hypothesis of no-correlation can be ruled out with the same level of confidence if the correlations are strong and the dataset small or if the correlations are weak and the dataset is large. What is of clinical or scientific importance are the latter type of correlations, that is, finding factors that change the medical condition by \u0027a lot\u0027. The corresponding statistical question is: How likely is it that the external factor changes the medical condition by at least x%, a question too seldom explicitly addressed in the medical and biological scientific literature.\u003cbr/\u003e\u003cbr/\u003eReferences\u003cbr/\u003e\u003cbr/\u003e1. Ioannidis JPA (2005) Why most research findings are false. PLoS Med : e124.\u003c/p\u003e",
    "originalBody": "Author: Yonatan Loewenstein\nPosition: No occupation was given\nInstitution: Howard Hughes Medical Institute and Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, Cambridge MA 02139\nE-mail: yonatanl@mit.edu\nSubmitted Date: March 01, 2006\nPublished Date: March 7, 2006\nThis comment was originally posted as a “Reader Response” on the publication date indicated above. All Reader Responses are now available as comments.\n\nJohn P. A. Ioannidis makes an interesting claim that most research findings are false\u003csup\u003e1\u003c/sup\u003e. Much of his argument is based on the assumption that the pre-study probability that a single hypothesis is correct is small and that the failure to incorporate this prior information in the estimation of the post-priori likelihood of the hypothesis to be correct leads to erroneous conclusions in the vast majority of studies. I argue that the prior probabilities for many of the hypotheses tested are large and therefore only a minority of the published findings is false. However, these published hypotheses are, in many of these cases, of little scientific or clinical interest.\n\nConsider a hypothetical epidemiological study of exploratory nature that attempts to find correlations between external factors and a specific medical condition. A positive, publishable, result in such a study is the finding of correlation between a specific external factor and a change in the medical condition: for example an improvement, suggesting a causal relation between the two. To support this hypothesis, a statistical test is presented to show that the likelihood that the external factor is uncorrelated with the improvement in the medical condition is low. What is the a-priori probability that the hypothesis is correct? Biological systems, such as biochemical or genetic networks, are characterized by a high degree of interconnectivity. Thus perturbations in one node of the network tend to propagate throughout the entire network.\n\nSince the subject is a biological system, any external factor is almost sure to have some effect on any medical condition. It is the direction of the effect, whether it improves the condition or worsens it, and its magnitude that are the unknown variables. In the absence of any mechanistic model, the parsimonious assumption is that both of the alternatives, improvement and worsening, are equally likely and therefore the prior of the hypothesis that the external factor is correlated with an improvement of the condition is 0.5.\n\nThus, when any supporting experimental evidence is incorporated with the prior, the post-priory probability that the hypothesis is true is larger than 0.5, Therefore, most published research hypotheses are correct, even if the stated p-value in these studies is overconfident, as a result of the biases described by Ioannidis.\n\nWhat can be learned from a study that presents correlations between the external factor and the change in the medical condition with a high level of confidence p? Not much. The level of confidence depends as much on the level of true correlations as it depends on the size of the dataset. The null hypothesis of no-correlation can be ruled out with the same level of confidence if the correlations are strong and the dataset small or if the correlations are weak and the dataset is large. What is of clinical or scientific importance are the latter type of correlations, that is, finding factors that change the medical condition by \u0027a lot\u0027. The corresponding statistical question is: How likely is it that the external factor changes the medical condition by at least x%, a question too seldom explicitly addressed in the medical and biological scientific literature.\n\nReferences\n\n1. Ioannidis JPA (2005) Why most research findings are false. PLoS Med : e124.",
    "truncatedBody": "\u003cp\u003eAuthor: Yonatan Loewenstein\u003cbr/\u003ePosition: No occupation was given\u003cbr/\u003eInstitution: Howard Hughes Medical Institute and Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, Cambridge MA 02139\u003cbr/\u003eE-mail:...\u003c/p\u003e",
    "bodyWithUrlLinkingNoPTags": "Author: Yonatan Loewenstein\u003cbr/\u003ePosition: No occupation was given\u003cbr/\u003eInstitution: Howard Hughes Medical Institute and Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, Cambridge MA 02139\u003cbr/\u003eE-mail: yonatanl@mit.edu\u003cbr/\u003eSubmitted Date: March 01, 2006\u003cbr/\u003ePublished Date: March 7, 2006\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eJohn P. A. Ioannidis makes an interesting claim that most research findings are false\u0026lt;sup\u0026gt;1\u0026lt;/sup\u0026gt;. Much of his argument is based on the assumption that the pre-study probability that a single hypothesis is correct is small and that the failure to incorporate this prior information in the estimation of the post-priori likelihood of the hypothesis to be correct leads to erroneous conclusions in the vast majority of studies. I argue that the prior probabilities for many of the hypotheses tested are large and therefore only a minority of the published findings is false. However, these published hypotheses are, in many of these cases, of little scientific or clinical interest.\u003cbr/\u003e\u003cbr/\u003eConsider a hypothetical epidemiological study of exploratory nature that attempts to find correlations between external factors and a specific medical condition. A positive, publishable, result in such a study is the finding of correlation between a specific external factor and a change in the medical condition: for example an improvement, suggesting a causal relation between the two. To support this hypothesis, a statistical test is presented to show that the likelihood that the external factor is uncorrelated with the improvement in the medical condition is low. What is the a-priori probability that the hypothesis is correct? Biological systems, such as biochemical or genetic networks, are characterized by a high degree of interconnectivity. Thus perturbations in one node of the network tend to propagate throughout the entire network.\u003cbr/\u003e\u003cbr/\u003eSince the subject is a biological system, any external factor is almost sure to have some effect on any medical condition. It is the direction of the effect, whether it improves the condition or worsens it, and its magnitude that are the unknown variables. In the absence of any mechanistic model, the parsimonious assumption is that both of the alternatives, improvement and worsening, are equally likely and therefore the prior of the hypothesis that the external factor is correlated with an improvement of the condition is 0.5.\u003cbr/\u003e\u003cbr/\u003eThus, when any supporting experimental evidence is incorporated with the prior, the post-priory probability that the hypothesis is true is larger than 0.5, Therefore, most published research hypotheses are correct, even if the stated p-value in these studies is overconfident, as a result of the biases described by Ioannidis.\u003cbr/\u003e\u003cbr/\u003eWhat can be learned from a study that presents correlations between the external factor and the change in the medical condition with a high level of confidence p? Not much. The level of confidence depends as much on the level of true correlations as it depends on the size of the dataset. The null hypothesis of no-correlation can be ruled out with the same level of confidence if the correlations are strong and the dataset small or if the correlations are weak and the dataset is large. What is of clinical or scientific importance are the latter type of correlations, that is, finding factors that change the medical condition by \u0027a lot\u0027. The corresponding statistical question is: How likely is it that the external factor changes the medical condition by at least x%, a question too seldom explicitly addressed in the medical and biological scientific literature.\u003cbr/\u003e\u003cbr/\u003eReferences\u003cbr/\u003e\u003cbr/\u003e1. Ioannidis JPA (2005) Why most research findings are false. PLoS Med : e124.",
    "truncatedBodyWithUrlLinkingNoPTags": "Author: Yonatan Loewenstein\u003cbr/\u003ePosition: No occupation was given\u003cbr/\u003eInstitution: Howard Hughes Medical Institute and Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, Cambridge MA 02139\u003cbr/\u003eE-mail:...",
    "bodyWithHighlightedText": "\u003cp\u003eAuthor: Yonatan Loewenstein\u003cbr/\u003ePosition: No occupation was given\u003cbr/\u003eInstitution: Howard Hughes Medical Institute and Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, Cambridge MA 02139\u003cbr/\u003eE-mail: yonatanl@mit.edu\u003cbr/\u003eSubmitted Date: March 01, 2006\u003cbr/\u003ePublished Date: March 7, 2006\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eJohn P. A. Ioannidis makes an interesting claim that most research findings are false\u0026lt;sup\u0026gt;1\u0026lt;/sup\u0026gt;. Much of his argument is based on the assumption that the pre-study probability that a single hypothesis is correct is small and that the failure to incorporate this prior information in the estimation of the post-priori likelihood of the hypothesis to be correct leads to erroneous conclusions in the vast majority of studies. I argue that the prior probabilities for many of the hypotheses tested are large and therefore only a minority of the published findings is false. However, these published hypotheses are, in many of these cases, of little scientific or clinical interest.\u003cbr/\u003e\u003cbr/\u003eConsider a hypothetical epidemiological study of exploratory nature that attempts to find correlations between external factors and a specific medical condition. A positive, publishable, result in such a study is the finding of correlation between a specific external factor and a change in the medical condition: for example an improvement, suggesting a causal relation between the two. To support this hypothesis, a statistical test is presented to show that the likelihood that the external factor is uncorrelated with the improvement in the medical condition is low. What is the a-priori probability that the hypothesis is correct? Biological systems, such as biochemical or genetic networks, are characterized by a high degree of interconnectivity. Thus perturbations in one node of the network tend to propagate throughout the entire network.\u003cbr/\u003e\u003cbr/\u003eSince the subject is a biological system, any external factor is almost sure to have some effect on any medical condition. It is the direction of the effect, whether it improves the condition or worsens it, and its magnitude that are the unknown variables. In the absence of any mechanistic model, the parsimonious assumption is that both of the alternatives, improvement and worsening, are equally likely and therefore the prior of the hypothesis that the external factor is correlated with an improvement of the condition is 0.5.\u003cbr/\u003e\u003cbr/\u003eThus, when any supporting experimental evidence is incorporated with the prior, the post-priory probability that the hypothesis is true is larger than 0.5, Therefore, most published research hypotheses are correct, even if the stated p-value in these studies is overconfident, as a result of the biases described by Ioannidis.\u003cbr/\u003e\u003cbr/\u003eWhat can be learned from a study that presents correlations between the external factor and the change in the medical condition with a high level of confidence p? Not much. The level of confidence depends as much on the level of true correlations as it depends on the size of the dataset. The null hypothesis of no-correlation can be ruled out with the same level of confidence if the correlations are strong and the dataset small or if the correlations are weak and the dataset is large. What is of clinical or scientific importance are the latter type of correlations, that is, finding factors that change the medical condition by \u0027a lot\u0027. The corresponding statistical question is: How likely is it that the external factor changes the medical condition by at least x%, a question too seldom explicitly addressed in the medical and biological scientific literature.\u003cbr/\u003e\u003cbr/\u003eReferences\u003cbr/\u003e\u003cbr/\u003e1. Ioannidis JPA (2005) Why most research findings are false. PLoS Med : e124.\u003c/p\u003e",
    "competingInterestStatement": "",
    "truncatedCompetingInterestStatement": "",
    "annotationUri": "10.1371/annotation/6a180926-4193-4b50-b424-7b2d87efd0f2",
    "creatorID": 172479,
    "creatorDisplayName": "plosmedicine",
    "creatorFormattedName": "PLoS Medicine Staff",
    "articleID": 16292,
    "articleDoi": "info:doi/10.1371/journal.pmed.0020124",
    "articleTitle": "Why Most Published Research Findings Are False",
    "created": "2009-03-30T23:51:13Z",
    "createdFormatted": "2009-03-30T23:51:13Z",
    "type": "COMMENT",
    "replies": [
      {
        "originalTitle": "RE: Why most published research findings are true but so many are useless",
        "title": "RE: Why most published research findings are true but so many are useless",
        "body": "\u003cp\u003eA priori, before flipping a fair coin, we already know that one side is heads, one is tails and therefore the result of a fair flip is .5 either way. One could express the effect of external factors (coin manipulations, etc.) in the manner described.\u003cbr/\u003e\u003cbr/\u003eBut life is not a nickle or a quarter. In many situations it is not known a priori if the situation, the \u0026quot;coin\u0026quot;, is fair or how much it might be weighted. This is why people gather knowlege, apply it to their experience and make judgements. Whether a situation is weighted or not, or by how much, cannot be assumed. The a priori probabilities of outcomes in an unfamiliar situation are unknown. Always.\u003cbr/\u003e\u003cbr/\u003eA priori, one cannot assume, in the absence of knowlege or experience, that the probabilities of improvement or worsening from some particular external factor are equal (.5). Instead, they are unknown.\u003c/p\u003e",
        "originalBody": "A priori, before flipping a fair coin, we already know that one side is heads, one is tails and therefore the result of a fair flip is .5 either way. One could express the effect of external factors (coin manipulations, etc.) in the manner described.\r\n\r\nBut life is not a nickle or a quarter. In many situations it is not known a priori if the situation, the \"coin\", is fair or how much it might be weighted. This is why people gather knowlege, apply it to their experience and make judgements. Whether a situation is weighted or not, or by how much, cannot be assumed. The a priori probabilities of outcomes in an unfamiliar situation are unknown. Always.\r\n\r\nA priori, one cannot assume, in the absence of knowlege or experience, that the probabilities of improvement or worsening from some particular external factor are equal (.5). Instead, they are unknown.",
        "truncatedBody": "\u003cp\u003eA priori, before flipping a fair coin, we already know that one side is heads, one is tails and therefore the result of a fair flip is .5 either way. One could express the effect of external factors (coin manipulations, etc.) in the manner...\u003c/p\u003e",
        "bodyWithUrlLinkingNoPTags": "A priori, before flipping a fair coin, we already know that one side is heads, one is tails and therefore the result of a fair flip is .5 either way. One could express the effect of external factors (coin manipulations, etc.) in the manner described.\u003cbr/\u003e\u003cbr/\u003eBut life is not a nickle or a quarter. In many situations it is not known a priori if the situation, the \u0026quot;coin\u0026quot;, is fair or how much it might be weighted. This is why people gather knowlege, apply it to their experience and make judgements. Whether a situation is weighted or not, or by how much, cannot be assumed. The a priori probabilities of outcomes in an unfamiliar situation are unknown. Always.\u003cbr/\u003e\u003cbr/\u003eA priori, one cannot assume, in the absence of knowlege or experience, that the probabilities of improvement or worsening from some particular external factor are equal (.5). Instead, they are unknown.",
        "truncatedBodyWithUrlLinkingNoPTags": "A priori, before flipping a fair coin, we already know that one side is heads, one is tails and therefore the result of a fair flip is .5 either way. One could express the effect of external factors (coin manipulations, etc.) in the manner...",
        "bodyWithHighlightedText": "\u003cp\u003eA priori, before flipping a fair coin, we already know that one side is heads, one is tails and therefore the result of a fair flip is .5 either way. One could express the effect of external factors (coin manipulations, etc.) in the manner described.\u003cbr/\u003e\u003cbr/\u003eBut life is not a nickle or a quarter. In many situations it is not known a priori if the situation, the \u0026quot;coin\u0026quot;, is fair or how much it might be weighted. This is why people gather knowlege, apply it to their experience and make judgements. Whether a situation is weighted or not, or by how much, cannot be assumed. The a priori probabilities of outcomes in an unfamiliar situation are unknown. Always.\u003cbr/\u003e\u003cbr/\u003eA priori, one cannot assume, in the absence of knowlege or experience, that the probabilities of improvement or worsening from some particular external factor are equal (.5). Instead, they are unknown.\u003c/p\u003e",
        "competingInterestStatement": "",
        "truncatedCompetingInterestStatement": "",
        "annotationUri": "10.1371/reply/540fe180-6470-4e52-a2c8-836ae2d09fc2",
        "creatorID": 3003,
        "creatorDisplayName": "Clif_Carl",
        "creatorFormattedName": "Clifton Carl",
        "articleID": 16292,
        "parentID": 9785,
        "articleDoi": "info:doi/10.1371/journal.pmed.0020124",
        "articleTitle": "Why Most Published Research Findings Are False",
        "created": "2010-01-17T08:14:03Z",
        "createdFormatted": "2010-01-17T08:14:03Z",
        "type": "REPLY",
        "replies": [
          {
            "originalTitle": "RE: RE: Why most published research findings are true but so many are useless",
            "title": "RE: RE: Why most published research findings are true but so many are useless",
            "body": "\u003cp\u003eThank you Clif Carl for this sage caveat. It\u0027s obvious but often seems to be forgotten.\u003cbr/\u003e\u003cbr/\u003eAnother issue around correlation of \u0027external factors\u0027 and \u0027medical conditions\u0027 is the thorny problem of direction of causation, which I believe plosmedicine left out of his analysis. This is a key problem in all correlations found, especially in medical research. The tendency to incorrectly affirm the consequent appears very high, especially in research claiming psychogenic aetiology for somatic illness.\u003c/p\u003e",
            "originalBody": "Thank you Clif Carl for this sage caveat. It\u0027s obvious but often seems to be forgotten.\r\n\r\nAnother issue around correlation of \u0027external factors\u0027 and \u0027medical conditions\u0027 is the thorny problem of direction of causation, which I believe plosmedicine left out of his analysis. This is a key problem in all correlations found, especially in medical research. The tendency to incorrectly affirm the consequent appears very high, especially in research claiming psychogenic aetiology for somatic illness.",
            "truncatedBody": "\u003cp\u003eThank you Clif Carl for this sage caveat. It\u0027s obvious but often seems to be forgotten.\u003cbr/\u003e\u003cbr/\u003eAnother issue around correlation of \u0027external factors\u0027 and \u0027medical conditions\u0027 is the thorny problem of direction of causation, which I believe...\u003c/p\u003e",
            "bodyWithUrlLinkingNoPTags": "Thank you Clif Carl for this sage caveat. It\u0027s obvious but often seems to be forgotten.\u003cbr/\u003e\u003cbr/\u003eAnother issue around correlation of \u0027external factors\u0027 and \u0027medical conditions\u0027 is the thorny problem of direction of causation, which I believe plosmedicine left out of his analysis. This is a key problem in all correlations found, especially in medical research. The tendency to incorrectly affirm the consequent appears very high, especially in research claiming psychogenic aetiology for somatic illness.",
            "truncatedBodyWithUrlLinkingNoPTags": "Thank you Clif Carl for this sage caveat. It\u0027s obvious but often seems to be forgotten.\u003cbr/\u003e\u003cbr/\u003eAnother issue around correlation of \u0027external factors\u0027 and \u0027medical conditions\u0027 is the thorny problem of direction of causation, which I believe...",
            "bodyWithHighlightedText": "\u003cp\u003eThank you Clif Carl for this sage caveat. It\u0027s obvious but often seems to be forgotten.\u003cbr/\u003e\u003cbr/\u003eAnother issue around correlation of \u0027external factors\u0027 and \u0027medical conditions\u0027 is the thorny problem of direction of causation, which I believe plosmedicine left out of his analysis. This is a key problem in all correlations found, especially in medical research. The tendency to incorrectly affirm the consequent appears very high, especially in research claiming psychogenic aetiology for somatic illness.\u003c/p\u003e",
            "competingInterestStatement": "",
            "truncatedCompetingInterestStatement": "",
            "annotationUri": "10.1371/reply/992f052a-701f-48e0-b628-bcaad51fc5da",
            "creatorID": 168533,
            "creatorDisplayName": "AngelaKennedy",
            "creatorFormattedName": "Angela Kennedy",
            "articleID": 16292,
            "parentID": 37741,
            "articleDoi": "info:doi/10.1371/journal.pmed.0020124",
            "articleTitle": "Why Most Published Research Findings Are False",
            "created": "2011-03-09T15:57:00Z",
            "createdFormatted": "2011-03-09T15:57:00Z",
            "type": "REPLY",
            "replies": [],
            "lastReplyDate": "2011-03-09T15:57:00Z",
            "totalNumReplies": 0
          }
        ],
        "lastReplyDate": "2011-03-09T15:57:00Z",
        "totalNumReplies": 1
      },
      {
        "originalTitle": "R is the problem",
        "title": "R is the problem",
        "body": "\u003cp\u003e\u003cbr/\u003eThe claim that \u0027so many [findings] are useless\u0027 has the sympathy of my cynical side, but it a hasty conclusion. Merely finding a correlation, by itself, is of little use when attempting to quantitatively relate the efficacy of a drug to dosage (to chose the example given at the end). However, it is a necessary first-step to a more detailed study of the system: i.e. \u0027essential\u0027 rather than \u0027useless\u0027.\u003cbr/\u003e\u003cbr/\u003eI would argue that the most cogent point this response makes is in relation to the true value of R in Ioannidis\u0027s analysis. The claim that we conduct research by blindly choosing variables then test correlations is preposterous, for, as the reply points out, biological systems are highly interconnected. Moreover, our knowledge of the correlations is cumulative and non-trivial to begin with, so any quantity relating to prior belief of a correlation should reflect this. The former fact means that a low value of R is an absurd assumption even in fields such as epidemiology, the latter suggests that the original article would be more useful if it modelled the effect of accumulating evidence.\u003cbr/\u003e\u003c/p\u003e",
        "originalBody": "\nThe claim that \u0027so many [findings] are useless\u0027 has the sympathy of my cynical side, but it a hasty conclusion. Merely finding a correlation, by itself, is of little use when attempting to quantitatively relate the efficacy of a drug to dosage (to chose the example given at the end). However, it is a necessary first-step to a more detailed study of the system: i.e. \u0027essential\u0027 rather than \u0027useless\u0027.\n\nI would argue that the most cogent point this response makes is in relation to the true value of R in Ioannidis\u0027s analysis. The claim that we conduct research by blindly choosing variables then test correlations is preposterous, for, as the reply points out, biological systems are highly interconnected. Moreover, our knowledge of the correlations is cumulative and non-trivial to begin with, so any quantity relating to prior belief of a correlation should reflect this. The former fact means that a low value of R is an absurd assumption even in fields such as epidemiology, the latter suggests that the original article would be more useful if it modelled the effect of accumulating evidence.\n",
        "truncatedBody": "\u003cp\u003e\u003cbr/\u003eThe claim that \u0027so many [findings] are useless\u0027 has the sympathy of my cynical side, but it a hasty conclusion. Merely finding a correlation, by itself, is of little use when attempting to quantitatively relate the efficacy of a drug to dosage (to...\u003c/p\u003e",
        "bodyWithUrlLinkingNoPTags": "\u003cbr/\u003eThe claim that \u0027so many [findings] are useless\u0027 has the sympathy of my cynical side, but it a hasty conclusion. Merely finding a correlation, by itself, is of little use when attempting to quantitatively relate the efficacy of a drug to dosage (to chose the example given at the end). However, it is a necessary first-step to a more detailed study of the system: i.e. \u0027essential\u0027 rather than \u0027useless\u0027.\u003cbr/\u003e\u003cbr/\u003eI would argue that the most cogent point this response makes is in relation to the true value of R in Ioannidis\u0027s analysis. The claim that we conduct research by blindly choosing variables then test correlations is preposterous, for, as the reply points out, biological systems are highly interconnected. Moreover, our knowledge of the correlations is cumulative and non-trivial to begin with, so any quantity relating to prior belief of a correlation should reflect this. The former fact means that a low value of R is an absurd assumption even in fields such as epidemiology, the latter suggests that the original article would be more useful if it modelled the effect of accumulating evidence.\u003cbr/\u003e",
        "truncatedBodyWithUrlLinkingNoPTags": "\u003cbr/\u003eThe claim that \u0027so many [findings] are useless\u0027 has the sympathy of my cynical side, but it a hasty conclusion. Merely finding a correlation, by itself, is of little use when attempting to quantitatively relate the efficacy of a drug to dosage (to...",
        "bodyWithHighlightedText": "\u003cp\u003e\u003cbr/\u003eThe claim that \u0027so many [findings] are useless\u0027 has the sympathy of my cynical side, but it a hasty conclusion. Merely finding a correlation, by itself, is of little use when attempting to quantitatively relate the efficacy of a drug to dosage (to chose the example given at the end). However, it is a necessary first-step to a more detailed study of the system: i.e. \u0027essential\u0027 rather than \u0027useless\u0027.\u003cbr/\u003e\u003cbr/\u003eI would argue that the most cogent point this response makes is in relation to the true value of R in Ioannidis\u0027s analysis. The claim that we conduct research by blindly choosing variables then test correlations is preposterous, for, as the reply points out, biological systems are highly interconnected. Moreover, our knowledge of the correlations is cumulative and non-trivial to begin with, so any quantity relating to prior belief of a correlation should reflect this. The former fact means that a low value of R is an absurd assumption even in fields such as epidemiology, the latter suggests that the original article would be more useful if it modelled the effect of accumulating evidence.\u003cbr/\u003e\u003c/p\u003e",
        "competingInterestStatement": "",
        "truncatedCompetingInterestStatement": "",
        "annotationUri": "10.1371/reply/4b5598e8-c450-4e5d-b6e1-ebc526af37ef",
        "creatorID": 85303,
        "creatorDisplayName": "oleary_t",
        "creatorFormattedName": "Tim O\u0027Leary",
        "articleID": 16292,
        "parentID": 9785,
        "articleDoi": "info:doi/10.1371/journal.pmed.0020124",
        "articleTitle": "Why Most Published Research Findings Are False",
        "created": "2010-07-04T14:44:29Z",
        "createdFormatted": "2010-07-04T14:44:29Z",
        "type": "REPLY",
        "replies": [
          {
            "originalTitle": "RE: R is the problem",
            "title": "RE: R is the problem",
            "body": "\u003cp\u003eoleary_t, I think your point has some validity, but there are some big issues with it. \u003cbr/\u003e\u003cbr/\u003eFor one thing, there is a serious problem in the typical methodology used to interpret results and/or reject null hypotheses. I refer you to two excellent summaries of the issue, Cohen\u0027s \u0026quot;The Earth is Round p\u0026lt;0.5\u0026quot; and Hubbard and Bayarri\u0027s \u0026quot;P Values are not Error Probabilities.\u0026quot; Both are easily available online. plosmedicine\u0027s makes a comment that unfortunately shows this same misunderstanding of statistical methodology that most researchers also exhibit: \u0026quot;To support this hypothesis, a statistical test is presented to show that the likelihood that the external factor is uncorrelated with the improvement in the medical condition is low.\u0026quot;\u003cbr/\u003e\u003cbr/\u003eSecond, there is something of a \u0026quot;house of cards\u0026quot; nature to conclusions drawn in a field. Attempts to replicate research results are no longer common. Researchers often take previous conclusions at face value and use them to formulate their own theories, when in fact the evidence of the previous conclusions may be very shaky.\u003cbr/\u003e\u003cbr/\u003eThird, approaches that are close to being scattershot may not be as uncommon as you think. Biau et al., in \u0026quot;P Value and the Theory of Hypothesis Testing,\u0026quot; note that by 1981 there were 246 factors reported as potentially predictive of cardiovascular disease, including slow beard growth, fingerprint patterns, and others. It\u0027s taken \u0026gt;25 years and a lot of effort to weed out the noise and get to the 9 or so factors that have finally been proven as clinically relevant to risk.\u003cbr/\u003e\u003cbr/\u003eThis leads to my fourth point, which is consistent with plosmedicine\u0027s final paragraph (though I don\u0027t agree with his/her analysis in general), which is that effect size is a very important factor to include in the design of a study. In other words, you can refine your test via sample size to prove that noise (random variation that can look like a signal if you put too fine a focus on it) is significant, but it is not really important in the proper perspective of treatment effects that consistently make a difference greater than random variation and experimental error. Statistical power analysis is needed to get a handle on this and it\u0027s not usually done.\u003cbr/\u003e\u003cbr/\u003eFinally - and I don\u0027t mean to pick on plosmedicine, but the above comment illustrates my point well - too often researchers forget that correlation is not causation, to wit: \u0026quot;A positive, publishable, result in such a study is the finding of correlation between a specific external factor and a change in the medical condition: for example an improvement, suggesting a causal relation between the two.\u0026quot; A positive result of correlation should be only HALF of a published full-length study. The other half should be a high-quality analysis of the factors involved in interpreting whether the correlation might represent causation, and how, or might represent a parallel effect of a deeper cause instead. Without this, the knowledge on which to form further hypotheses is incomplete.\u003cbr/\u003e\u003cbr/\u003eAnd we get a bonus point in this quote too...the bias that arises because lack of support for a finding of correlation is not usually published/publishable. So people don\u0027t get to see the 15 studies that find no correlation, only the 1 or 2 studies that do. \u003c/p\u003e",
            "originalBody": "oleary_t, I think your point has some validity, but there are some big issues with it. \n\nFor one thing, there is a serious problem in the typical methodology used to interpret results and/or reject null hypotheses. I refer you to two excellent summaries of the issue, Cohen\u0027s \"The Earth is Round p\u003c0.5\" and Hubbard and Bayarri\u0027s \"P Values are not Error Probabilities.\" Both are easily available online. plosmedicine\u0027s makes a comment that unfortunately shows this same misunderstanding of statistical methodology that most researchers also exhibit: \"To support this hypothesis, a statistical test is presented to show that the likelihood that the external factor is uncorrelated with the improvement in the medical condition is low.\"\n\nSecond, there is something of a \"house of cards\" nature to conclusions drawn in a field. Attempts to replicate research results are no longer common. Researchers often take previous conclusions at face value and use them to formulate their own theories, when in fact the evidence of the previous conclusions may be very shaky.\n\nThird, approaches that are close to being scattershot may not be as uncommon as you think. Biau et al., in \"P Value and the Theory of Hypothesis Testing,\" note that by 1981 there were 246 factors reported as potentially predictive of cardiovascular disease, including slow beard growth, fingerprint patterns, and others. It\u0027s taken \u003e25 years and a lot of effort to weed out the noise and get to the 9 or so factors that have finally been proven as clinically relevant to risk.\n\nThis leads to my fourth point, which is consistent with plosmedicine\u0027s final paragraph (though I don\u0027t agree with his/her analysis in general), which is that effect size is a very important factor to include in the design of a study. In other words, you can refine your test via sample size to prove that noise (random variation that can look like a signal if you put too fine a focus on it) is significant, but it is not really important in the proper perspective of treatment effects that consistently make a difference greater than random variation and experimental error. Statistical power analysis is needed to get a handle on this and it\u0027s not usually done.\n\nFinally - and I don\u0027t mean to pick on plosmedicine, but the above comment illustrates my point well - too often researchers forget that correlation is not causation, to wit: \"A positive, publishable, result in such a study is the finding of correlation between a specific external factor and a change in the medical condition: for example an improvement, suggesting a causal relation between the two.\" A positive result of correlation should be only HALF of a published full-length study. The other half should be a high-quality analysis of the factors involved in interpreting whether the correlation might represent causation, and how, or might represent a parallel effect of a deeper cause instead. Without this, the knowledge on which to form further hypotheses is incomplete.\n\nAnd we get a bonus point in this quote too...the bias that arises because lack of support for a finding of correlation is not usually published/publishable. So people don\u0027t get to see the 15 studies that find no correlation, only the 1 or 2 studies that do. ",
            "truncatedBody": "\u003cp\u003eoleary_t, I think your point has some validity, but there are some big issues with it. \u003cbr/\u003e\u003cbr/\u003eFor one thing, there is a serious problem in the typical methodology used to interpret results and/or reject null hypotheses. I refer you to two excellent...\u003c/p\u003e",
            "bodyWithUrlLinkingNoPTags": "oleary_t, I think your point has some validity, but there are some big issues with it. \u003cbr/\u003e\u003cbr/\u003eFor one thing, there is a serious problem in the typical methodology used to interpret results and/or reject null hypotheses. I refer you to two excellent summaries of the issue, Cohen\u0027s \u0026quot;The Earth is Round p\u0026lt;0.5\u0026quot; and Hubbard and Bayarri\u0027s \u0026quot;P Values are not Error Probabilities.\u0026quot; Both are easily available online. plosmedicine\u0027s makes a comment that unfortunately shows this same misunderstanding of statistical methodology that most researchers also exhibit: \u0026quot;To support this hypothesis, a statistical test is presented to show that the likelihood that the external factor is uncorrelated with the improvement in the medical condition is low.\u0026quot;\u003cbr/\u003e\u003cbr/\u003eSecond, there is something of a \u0026quot;house of cards\u0026quot; nature to conclusions drawn in a field. Attempts to replicate research results are no longer common. Researchers often take previous conclusions at face value and use them to formulate their own theories, when in fact the evidence of the previous conclusions may be very shaky.\u003cbr/\u003e\u003cbr/\u003eThird, approaches that are close to being scattershot may not be as uncommon as you think. Biau et al., in \u0026quot;P Value and the Theory of Hypothesis Testing,\u0026quot; note that by 1981 there were 246 factors reported as potentially predictive of cardiovascular disease, including slow beard growth, fingerprint patterns, and others. It\u0027s taken \u0026gt;25 years and a lot of effort to weed out the noise and get to the 9 or so factors that have finally been proven as clinically relevant to risk.\u003cbr/\u003e\u003cbr/\u003eThis leads to my fourth point, which is consistent with plosmedicine\u0027s final paragraph (though I don\u0027t agree with his/her analysis in general), which is that effect size is a very important factor to include in the design of a study. In other words, you can refine your test via sample size to prove that noise (random variation that can look like a signal if you put too fine a focus on it) is significant, but it is not really important in the proper perspective of treatment effects that consistently make a difference greater than random variation and experimental error. Statistical power analysis is needed to get a handle on this and it\u0027s not usually done.\u003cbr/\u003e\u003cbr/\u003eFinally - and I don\u0027t mean to pick on plosmedicine, but the above comment illustrates my point well - too often researchers forget that correlation is not causation, to wit: \u0026quot;A positive, publishable, result in such a study is the finding of correlation between a specific external factor and a change in the medical condition: for example an improvement, suggesting a causal relation between the two.\u0026quot; A positive result of correlation should be only HALF of a published full-length study. The other half should be a high-quality analysis of the factors involved in interpreting whether the correlation might represent causation, and how, or might represent a parallel effect of a deeper cause instead. Without this, the knowledge on which to form further hypotheses is incomplete.\u003cbr/\u003e\u003cbr/\u003eAnd we get a bonus point in this quote too...the bias that arises because lack of support for a finding of correlation is not usually published/publishable. So people don\u0027t get to see the 15 studies that find no correlation, only the 1 or 2 studies that do. ",
            "truncatedBodyWithUrlLinkingNoPTags": "oleary_t, I think your point has some validity, but there are some big issues with it. \u003cbr/\u003e\u003cbr/\u003eFor one thing, there is a serious problem in the typical methodology used to interpret results and/or reject null hypotheses. I refer you to two excellent...",
            "bodyWithHighlightedText": "\u003cp\u003eoleary_t, I think your point has some validity, but there are some big issues with it. \u003cbr/\u003e\u003cbr/\u003eFor one thing, there is a serious problem in the typical methodology used to interpret results and/or reject null hypotheses. I refer you to two excellent summaries of the issue, Cohen\u0027s \u0026quot;The Earth is Round p\u0026lt;0.5\u0026quot; and Hubbard and Bayarri\u0027s \u0026quot;P Values are not Error Probabilities.\u0026quot; Both are easily available online. plosmedicine\u0027s makes a comment that unfortunately shows this same misunderstanding of statistical methodology that most researchers also exhibit: \u0026quot;To support this hypothesis, a statistical test is presented to show that the likelihood that the external factor is uncorrelated with the improvement in the medical condition is low.\u0026quot;\u003cbr/\u003e\u003cbr/\u003eSecond, there is something of a \u0026quot;house of cards\u0026quot; nature to conclusions drawn in a field. Attempts to replicate research results are no longer common. Researchers often take previous conclusions at face value and use them to formulate their own theories, when in fact the evidence of the previous conclusions may be very shaky.\u003cbr/\u003e\u003cbr/\u003eThird, approaches that are close to being scattershot may not be as uncommon as you think. Biau et al., in \u0026quot;P Value and the Theory of Hypothesis Testing,\u0026quot; note that by 1981 there were 246 factors reported as potentially predictive of cardiovascular disease, including slow beard growth, fingerprint patterns, and others. It\u0027s taken \u0026gt;25 years and a lot of effort to weed out the noise and get to the 9 or so factors that have finally been proven as clinically relevant to risk.\u003cbr/\u003e\u003cbr/\u003eThis leads to my fourth point, which is consistent with plosmedicine\u0027s final paragraph (though I don\u0027t agree with his/her analysis in general), which is that effect size is a very important factor to include in the design of a study. In other words, you can refine your test via sample size to prove that noise (random variation that can look like a signal if you put too fine a focus on it) is significant, but it is not really important in the proper perspective of treatment effects that consistently make a difference greater than random variation and experimental error. Statistical power analysis is needed to get a handle on this and it\u0027s not usually done.\u003cbr/\u003e\u003cbr/\u003eFinally - and I don\u0027t mean to pick on plosmedicine, but the above comment illustrates my point well - too often researchers forget that correlation is not causation, to wit: \u0026quot;A positive, publishable, result in such a study is the finding of correlation between a specific external factor and a change in the medical condition: for example an improvement, suggesting a causal relation between the two.\u0026quot; A positive result of correlation should be only HALF of a published full-length study. The other half should be a high-quality analysis of the factors involved in interpreting whether the correlation might represent causation, and how, or might represent a parallel effect of a deeper cause instead. Without this, the knowledge on which to form further hypotheses is incomplete.\u003cbr/\u003e\u003cbr/\u003eAnd we get a bonus point in this quote too...the bias that arises because lack of support for a finding of correlation is not usually published/publishable. So people don\u0027t get to see the 15 studies that find no correlation, only the 1 or 2 studies that do. \u003c/p\u003e",
            "competingInterestStatement": "",
            "truncatedCompetingInterestStatement": "",
            "annotationUri": "10.1371/reply/e936362c-bfda-41ef-acb5-2a059b8b59e1",
            "creatorID": 312111,
            "creatorDisplayName": "rasraster",
            "creatorFormattedName": "Richard Shandross",
            "articleID": 16292,
            "parentID": 34337,
            "articleDoi": "info:doi/10.1371/journal.pmed.0020124",
            "articleTitle": "Why Most Published Research Findings Are False",
            "created": "2013-02-18T17:07:47Z",
            "createdFormatted": "2013-02-18T17:07:47Z",
            "type": "REPLY",
            "replies": [],
            "lastReplyDate": "2013-02-18T17:07:47Z",
            "totalNumReplies": 0
          }
        ],
        "lastReplyDate": "2013-02-18T17:07:47Z",
        "totalNumReplies": 1
      },
      {
        "originalTitle": "RE: Why most published research findings are true but so many are useless",
        "title": "RE: Why most published research findings are true but so many are useless",
        "body": "\u003cp\u003eIt is an excellent point. However, in practice, a priori, most researchers predict a minimum level of improvement, enough to be of some biological significance. Ex: taking fish oils improves IQ by at least 2% (otherwise the study would not be conducted.). Under these conditions, the events improvement vs. no improvement or worsening are not equally likely. Although researchers test the probability of the improvement being greater than zero, they will not publish unless the improvement is clinically valuable (of some substance). Under those conditions, the a prior probability was very small, and likely remains very small after the study. We can rephrase by saying that most published research findings are false when we consider the substance or magniture of the improvement\u003c/p\u003e",
        "originalBody": "It is an excellent point. However, in practice, a priori, most researchers predict a minimum level of improvement, enough to be of some biological significance. Ex: taking fish oils improves IQ by at least 2% (otherwise the study would not be conducted.). Under these conditions, the events improvement vs. no improvement or worsening are not equally likely. Although researchers test the probability of the improvement being greater than zero, they will not publish unless the improvement is clinically valuable (of some substance). Under those conditions, the a prior probability was very small, and likely remains very small after the study. We can rephrase by saying that most published research findings are false when we consider the substance or magniture of the improvement",
        "truncatedBody": "\u003cp\u003eIt is an excellent point. However, in practice, a priori, most researchers predict a minimum level of improvement, enough to be of some biological significance. Ex: taking fish oils improves IQ by at least 2% (otherwise the study would not be...\u003c/p\u003e",
        "bodyWithUrlLinkingNoPTags": "It is an excellent point. However, in practice, a priori, most researchers predict a minimum level of improvement, enough to be of some biological significance. Ex: taking fish oils improves IQ by at least 2% (otherwise the study would not be conducted.). Under these conditions, the events improvement vs. no improvement or worsening are not equally likely. Although researchers test the probability of the improvement being greater than zero, they will not publish unless the improvement is clinically valuable (of some substance). Under those conditions, the a prior probability was very small, and likely remains very small after the study. We can rephrase by saying that most published research findings are false when we consider the substance or magniture of the improvement",
        "truncatedBodyWithUrlLinkingNoPTags": "It is an excellent point. However, in practice, a priori, most researchers predict a minimum level of improvement, enough to be of some biological significance. Ex: taking fish oils improves IQ by at least 2% (otherwise the study would not be...",
        "bodyWithHighlightedText": "\u003cp\u003eIt is an excellent point. However, in practice, a priori, most researchers predict a minimum level of improvement, enough to be of some biological significance. Ex: taking fish oils improves IQ by at least 2% (otherwise the study would not be conducted.). Under these conditions, the events improvement vs. no improvement or worsening are not equally likely. Although researchers test the probability of the improvement being greater than zero, they will not publish unless the improvement is clinically valuable (of some substance). Under those conditions, the a prior probability was very small, and likely remains very small after the study. We can rephrase by saying that most published research findings are false when we consider the substance or magniture of the improvement\u003c/p\u003e",
        "competingInterestStatement": "",
        "truncatedCompetingInterestStatement": "",
        "annotationUri": "10.1371/reply/f9e6432b-bebd-4eee-a5d7-b5c064c8c2d0",
        "creatorID": 109521,
        "creatorDisplayName": "optimalpolicies",
        "creatorFormattedName": "Eduardo Siguel",
        "articleID": 16292,
        "parentID": 9785,
        "articleDoi": "info:doi/10.1371/journal.pmed.0020124",
        "articleTitle": "Why Most Published Research Findings Are False",
        "created": "2011-04-17T23:30:32Z",
        "createdFormatted": "2011-04-17T23:30:32Z",
        "type": "REPLY",
        "replies": [],
        "lastReplyDate": "2011-04-17T23:30:32Z",
        "totalNumReplies": 0
      }
    ],
    "lastReplyDate": "2011-04-17T23:30:32Z",
    "totalNumReplies": 5
  },
  {
    "originalTitle": "true existing relationships to true non-existent relationships",
    "title": "true existing relationships to true non-existent relationships",
    "body": "\u003cp\u003eThis is a minor point to quibble, except that further definitions rely upon clear understanding here:  R, as the author has defined it, is the ratio of the # of \u0026quot;true existing relationships\u0026quot; to the # of \u0026quot;true non-existent relationships\u0026quot;.  Both the numerator \u0026amp; the denominator are \u0026quot;true, but unknowable\u0026quot;.\u003c/p\u003e",
    "originalBody": "This is a minor point to quibble, except that further definitions rely upon clear understanding here:  R, as the author has defined it, is the ratio of the # of \"true existing relationships\" to the # of \"true non-existent relationships\".  Both the numerator \u0026 the denominator are \"true, but unknowable\".",
    "truncatedBody": "\u003cp\u003eThis is a minor point to quibble, except that further definitions rely upon clear understanding here:  R, as the author has defined it, is the ratio of the # of \u0026quot;true existing relationships\u0026quot; to the # of \u0026quot;true non-existent...\u003c/p\u003e",
    "bodyWithUrlLinkingNoPTags": "This is a minor point to quibble, except that further definitions rely upon clear understanding here:  R, as the author has defined it, is the ratio of the # of \u0026quot;true existing relationships\u0026quot; to the # of \u0026quot;true non-existent relationships\u0026quot;.  Both the numerator \u0026amp; the denominator are \u0026quot;true, but unknowable\u0026quot;.",
    "truncatedBodyWithUrlLinkingNoPTags": "This is a minor point to quibble, except that further definitions rely upon clear understanding here:  R, as the author has defined it, is the ratio of the # of \u0026quot;true existing relationships\u0026quot; to the # of \u0026quot;true non-existent...",
    "bodyWithHighlightedText": "\u003cp\u003e\u003cem\u003eLet R be the ratio of the number of \u0026ldquo;true relationships\u0026rdquo; to \u0026ldquo;no relationships\u0026rdquo; among those tested in the field.\u003c/em\u003e\u003cbr/\u003e\u003ca href\u003d\"http://plosmedicine.org/article/info:doi/10.1371/journal.pmed.0020124#article1.body1.sec2.p3\"\u003ehttp://plosmedicine.org/article/info:doi/10.1371/journal.pmed.0020124#article1.body1.sec2.p3\u003c/a\u003e\u003cbr/\u003e\u003cbr/\u003eThis is a minor point to quibble, except that further definitions rely upon clear understanding here:  R, as the author has defined it, is the ratio of the # of \u0026quot;true existing relationships\u0026quot; to the # of \u0026quot;true non-existent relationships\u0026quot;.  Both the numerator \u0026amp; the denominator are \u0026quot;true, but unknowable\u0026quot;.\u003c/p\u003e",
    "competingInterestStatement": "",
    "truncatedCompetingInterestStatement": "",
    "annotationUri": "10.1371/annotation/6c384f5a-b2c0-4e31-b683-167e793fa1da",
    "creatorID": 153531,
    "creatorDisplayName": "lazarillo",
    "creatorFormattedName": "Mike Williamson",
    "articleID": 16292,
    "articleDoi": "info:doi/10.1371/journal.pmed.0020124",
    "articleTitle": "Why Most Published Research Findings Are False",
    "created": "2011-06-27T22:52:52Z",
    "createdFormatted": "2011-06-27T22:52:52Z",
    "type": "COMMENT",
    "replies": [],
    "lastReplyDate": "2011-06-27T22:52:52Z",
    "totalNumReplies": 0
  },
  {
    "originalTitle": "The Clinical Interpretation of Research",
    "title": "The Clinical Interpretation of Research",
    "body": "\u003cp\u003eAuthor: Stephen Pauker, MD\u003cbr/\u003ePosition: Professor of Medicine\u003cbr/\u003eInstitution: Tufts-New England Medical Center\u003cbr/\u003eE-mail: spauker@tufts-nemc.org\u003cbr/\u003eSubmitted Date: September 11, 2005\u003cbr/\u003ePublished Date: September 12, 2005\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eJohn Ioannidis emphasizes the central role of prior probabilities [1]. His conclusion rests on the presumed low probability that a  hypothesis was true before the study.\u003cbr/\u003e\u003cbr/\u003eUnfortunately, his formulation relates the post-study probability that the study\u0027s conclusion is true to the pre-study odds. The results might have been clearer had he also plotted the relation of odds to probability, a curvilinear relationship, assuming the study carried no information. Further, the various graphs are right-truncated at pre-study odds, R, of 1.0 (a probability of 0.5), although his examples go as high as pre-study odds of 2.0. A positive study must, by definition, increase the likelihood that the hypothesis is true. It might have been clearer had Ioannidis chosen to relate odds to odds or probability to probability; in both cases a neutral study would produce at straight line along a 45-degree diagonal.\u003cbr/\u003e\u003cbr/\u003eThe pre-study to post-study relation can more simply be expressed using the odds-likelihood form of Bayes rule - i.e., the post-study odds equal the pre-study odds times the likelihood ratio (LR) of the study. Then the equations for positive predictive value become the simple product R x LR. For a single unbiased study, the LR equals (1-beta)/alpha. When incorporating study bias u, as defined by Ioannidis, LR equals  [1-beta*(1-u)]/[alpha*(1-u)+u]. For a typical study with alpha equal to 0.05 and beta equal to 0.2 (ie, with power equal to 0.8), the LR equals 16. When the pre-study odds are less than 1:16 (a probability of 0.0588), the post-study odds will be less that 1.0 - i.e., the study\u0027s hypothesis will be more likely false than true.\u003cbr/\u003e\u003cbr/\u003eFor non-Bayesians, statistical significance testing presumes a uninformative prior probability - i.e., pre-study odds (R) of 1. Then LR would merely need to exceed 1 for the study\u0027s conclusions to be more likely true than false. At the common significance levels (alpha) of 0.05 and 0.01, the requisite study powers would merely need to exceed 0.05 and 0.01 respectively, corresponding to maximum type II error rates (beta) of 0.95 and 0.99. Such lax requirements would almost always be met for a published study. Hence, the common belief that the vast majority of studies have valid conclusions would be correct, if we can assume that the pre-study odds are truly uninformative. However, as Ioannidis suggests, this is unlikely to be the case.\u003cbr/\u003e\u003cbr/\u003eTwo more corollaries might be added. The higher the pre-study odds that the study\u0027s hypothesis is true, the lower the requisite power (study size and effect size) required to make the study\u0027s findings more likely true than false. When studies are published, the investigator should estimate the pre-study odds and report the LR implied by the observed effect.\u003cbr/\u003e\u003cbr/\u003eFrom the perspective of an epidemiologist or a statistician, the relevant question is whether the study\u0027s hypothesis is true - i.e., is the probability of H1 greater than 0.5. For clinicians and their patients, the relevant question is whether a particular strategy should be followed in an individual patient or a subset of similar patients. That decision (or recommendation to the patient) will depend on the pre-study likelihood of benefit in that patient and on the relative magnitude of benefits and risks of that strategy, if the diagnosis in that patient is uncertain. For many such decisions, the \u0026quot;more likely true than false\u0026quot; criterion may not be the best decision rule. For serious diseases and treatments of only modest risk, post-study probabilities of considerably less than 0.5 may be sufficient to justify treatment [2].\u003cbr/\u003e\u003cbr/\u003eIoannidis\u0027s provocative essay is a timely call for careful consideration of published studies. The odds-likelihood formulation suggested herein may be helpful in providing a more intuitive model. Clinicians now need to take it to the next step.\u003cbr/\u003e\u003cbr/\u003eReferences\u003cbr/\u003e1. Ioannidis JPA (2005). PLoS Med 2(8):e124.\u003cbr/\u003e2. Pauker SG, Kassirer JP (1975). N Eng J Med 293:229.\u003cbr/\u003e\u003cbr/\u003e\u003c/p\u003e",
    "originalBody": "Author: Stephen Pauker, MD\nPosition: Professor of Medicine\nInstitution: Tufts-New England Medical Center\nE-mail: spauker@tufts-nemc.org\nSubmitted Date: September 11, 2005\nPublished Date: September 12, 2005\nThis comment was originally posted as a “Reader Response” on the publication date indicated above. All Reader Responses are now available as comments.\n\nJohn Ioannidis emphasizes the central role of prior probabilities [1]. His conclusion rests on the presumed low probability that a  hypothesis was true before the study.\n\nUnfortunately, his formulation relates the post-study probability that the study\u0027s conclusion is true to the pre-study odds. The results might have been clearer had he also plotted the relation of odds to probability, a curvilinear relationship, assuming the study carried no information. Further, the various graphs are right-truncated at pre-study odds, R, of 1.0 (a probability of 0.5), although his examples go as high as pre-study odds of 2.0. A positive study must, by definition, increase the likelihood that the hypothesis is true. It might have been clearer had Ioannidis chosen to relate odds to odds or probability to probability; in both cases a neutral study would produce at straight line along a 45-degree diagonal.\n\nThe pre-study to post-study relation can more simply be expressed using the odds-likelihood form of Bayes rule - i.e., the post-study odds equal the pre-study odds times the likelihood ratio (LR) of the study. Then the equations for positive predictive value become the simple product R x LR. For a single unbiased study, the LR equals (1-beta)/alpha. When incorporating study bias u, as defined by Ioannidis, LR equals  [1-beta*(1-u)]/[alpha*(1-u)+u]. For a typical study with alpha equal to 0.05 and beta equal to 0.2 (ie, with power equal to 0.8), the LR equals 16. When the pre-study odds are less than 1:16 (a probability of 0.0588), the post-study odds will be less that 1.0 - i.e., the study\u0027s hypothesis will be more likely false than true.\n\nFor non-Bayesians, statistical significance testing presumes a uninformative prior probability - i.e., pre-study odds (R) of 1. Then LR would merely need to exceed 1 for the study\u0027s conclusions to be more likely true than false. At the common significance levels (alpha) of 0.05 and 0.01, the requisite study powers would merely need to exceed 0.05 and 0.01 respectively, corresponding to maximum type II error rates (beta) of 0.95 and 0.99. Such lax requirements would almost always be met for a published study. Hence, the common belief that the vast majority of studies have valid conclusions would be correct, if we can assume that the pre-study odds are truly uninformative. However, as Ioannidis suggests, this is unlikely to be the case.\n\nTwo more corollaries might be added. The higher the pre-study odds that the study\u0027s hypothesis is true, the lower the requisite power (study size and effect size) required to make the study\u0027s findings more likely true than false. When studies are published, the investigator should estimate the pre-study odds and report the LR implied by the observed effect.\n\nFrom the perspective of an epidemiologist or a statistician, the relevant question is whether the study\u0027s hypothesis is true - i.e., is the probability of H1 greater than 0.5. For clinicians and their patients, the relevant question is whether a particular strategy should be followed in an individual patient or a subset of similar patients. That decision (or recommendation to the patient) will depend on the pre-study likelihood of benefit in that patient and on the relative magnitude of benefits and risks of that strategy, if the diagnosis in that patient is uncertain. For many such decisions, the \"more likely true than false\" criterion may not be the best decision rule. For serious diseases and treatments of only modest risk, post-study probabilities of considerably less than 0.5 may be sufficient to justify treatment [2].\n\nIoannidis\u0027s provocative essay is a timely call for careful consideration of published studies. The odds-likelihood formulation suggested herein may be helpful in providing a more intuitive model. Clinicians now need to take it to the next step.\n\nReferences\n1. Ioannidis JPA (2005). PLoS Med 2(8):e124.\n2. Pauker SG, Kassirer JP (1975). N Eng J Med 293:229.\n\n",
    "truncatedBody": "\u003cp\u003eAuthor: Stephen Pauker, MD\u003cbr/\u003ePosition: Professor of Medicine\u003cbr/\u003eInstitution: Tufts-New England Medical Center\u003cbr/\u003eE-mail: spauker@tufts-nemc.org\u003cbr/\u003eSubmitted Date: September 11, 2005\u003cbr/\u003ePublished Date: September 12, 2005\u003cbr/\u003eThis comment was...\u003c/p\u003e",
    "bodyWithUrlLinkingNoPTags": "Author: Stephen Pauker, MD\u003cbr/\u003ePosition: Professor of Medicine\u003cbr/\u003eInstitution: Tufts-New England Medical Center\u003cbr/\u003eE-mail: spauker@tufts-nemc.org\u003cbr/\u003eSubmitted Date: September 11, 2005\u003cbr/\u003ePublished Date: September 12, 2005\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eJohn Ioannidis emphasizes the central role of prior probabilities [1]. His conclusion rests on the presumed low probability that a  hypothesis was true before the study.\u003cbr/\u003e\u003cbr/\u003eUnfortunately, his formulation relates the post-study probability that the study\u0027s conclusion is true to the pre-study odds. The results might have been clearer had he also plotted the relation of odds to probability, a curvilinear relationship, assuming the study carried no information. Further, the various graphs are right-truncated at pre-study odds, R, of 1.0 (a probability of 0.5), although his examples go as high as pre-study odds of 2.0. A positive study must, by definition, increase the likelihood that the hypothesis is true. It might have been clearer had Ioannidis chosen to relate odds to odds or probability to probability; in both cases a neutral study would produce at straight line along a 45-degree diagonal.\u003cbr/\u003e\u003cbr/\u003eThe pre-study to post-study relation can more simply be expressed using the odds-likelihood form of Bayes rule - i.e., the post-study odds equal the pre-study odds times the likelihood ratio (LR) of the study. Then the equations for positive predictive value become the simple product R x LR. For a single unbiased study, the LR equals (1-beta)/alpha. When incorporating study bias u, as defined by Ioannidis, LR equals  [1-beta*(1-u)]/[alpha*(1-u)+u]. For a typical study with alpha equal to 0.05 and beta equal to 0.2 (ie, with power equal to 0.8), the LR equals 16. When the pre-study odds are less than 1:16 (a probability of 0.0588), the post-study odds will be less that 1.0 - i.e., the study\u0027s hypothesis will be more likely false than true.\u003cbr/\u003e\u003cbr/\u003eFor non-Bayesians, statistical significance testing presumes a uninformative prior probability - i.e., pre-study odds (R) of 1. Then LR would merely need to exceed 1 for the study\u0027s conclusions to be more likely true than false. At the common significance levels (alpha) of 0.05 and 0.01, the requisite study powers would merely need to exceed 0.05 and 0.01 respectively, corresponding to maximum type II error rates (beta) of 0.95 and 0.99. Such lax requirements would almost always be met for a published study. Hence, the common belief that the vast majority of studies have valid conclusions would be correct, if we can assume that the pre-study odds are truly uninformative. However, as Ioannidis suggests, this is unlikely to be the case.\u003cbr/\u003e\u003cbr/\u003eTwo more corollaries might be added. The higher the pre-study odds that the study\u0027s hypothesis is true, the lower the requisite power (study size and effect size) required to make the study\u0027s findings more likely true than false. When studies are published, the investigator should estimate the pre-study odds and report the LR implied by the observed effect.\u003cbr/\u003e\u003cbr/\u003eFrom the perspective of an epidemiologist or a statistician, the relevant question is whether the study\u0027s hypothesis is true - i.e., is the probability of H1 greater than 0.5. For clinicians and their patients, the relevant question is whether a particular strategy should be followed in an individual patient or a subset of similar patients. That decision (or recommendation to the patient) will depend on the pre-study likelihood of benefit in that patient and on the relative magnitude of benefits and risks of that strategy, if the diagnosis in that patient is uncertain. For many such decisions, the \u0026quot;more likely true than false\u0026quot; criterion may not be the best decision rule. For serious diseases and treatments of only modest risk, post-study probabilities of considerably less than 0.5 may be sufficient to justify treatment [2].\u003cbr/\u003e\u003cbr/\u003eIoannidis\u0027s provocative essay is a timely call for careful consideration of published studies. The odds-likelihood formulation suggested herein may be helpful in providing a more intuitive model. Clinicians now need to take it to the next step.\u003cbr/\u003e\u003cbr/\u003eReferences\u003cbr/\u003e1. Ioannidis JPA (2005). PLoS Med 2(8):e124.\u003cbr/\u003e2. Pauker SG, Kassirer JP (1975). N Eng J Med 293:229.\u003cbr/\u003e\u003cbr/\u003e",
    "truncatedBodyWithUrlLinkingNoPTags": "Author: Stephen Pauker, MD\u003cbr/\u003ePosition: Professor of Medicine\u003cbr/\u003eInstitution: Tufts-New England Medical Center\u003cbr/\u003eE-mail: spauker@tufts-nemc.org\u003cbr/\u003eSubmitted Date: September 11, 2005\u003cbr/\u003ePublished Date: September 12, 2005\u003cbr/\u003eThis comment was...",
    "bodyWithHighlightedText": "\u003cp\u003eAuthor: Stephen Pauker, MD\u003cbr/\u003ePosition: Professor of Medicine\u003cbr/\u003eInstitution: Tufts-New England Medical Center\u003cbr/\u003eE-mail: spauker@tufts-nemc.org\u003cbr/\u003eSubmitted Date: September 11, 2005\u003cbr/\u003ePublished Date: September 12, 2005\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eJohn Ioannidis emphasizes the central role of prior probabilities [1]. His conclusion rests on the presumed low probability that a  hypothesis was true before the study.\u003cbr/\u003e\u003cbr/\u003eUnfortunately, his formulation relates the post-study probability that the study\u0027s conclusion is true to the pre-study odds. The results might have been clearer had he also plotted the relation of odds to probability, a curvilinear relationship, assuming the study carried no information. Further, the various graphs are right-truncated at pre-study odds, R, of 1.0 (a probability of 0.5), although his examples go as high as pre-study odds of 2.0. A positive study must, by definition, increase the likelihood that the hypothesis is true. It might have been clearer had Ioannidis chosen to relate odds to odds or probability to probability; in both cases a neutral study would produce at straight line along a 45-degree diagonal.\u003cbr/\u003e\u003cbr/\u003eThe pre-study to post-study relation can more simply be expressed using the odds-likelihood form of Bayes rule - i.e., the post-study odds equal the pre-study odds times the likelihood ratio (LR) of the study. Then the equations for positive predictive value become the simple product R x LR. For a single unbiased study, the LR equals (1-beta)/alpha. When incorporating study bias u, as defined by Ioannidis, LR equals  [1-beta*(1-u)]/[alpha*(1-u)+u]. For a typical study with alpha equal to 0.05 and beta equal to 0.2 (ie, with power equal to 0.8), the LR equals 16. When the pre-study odds are less than 1:16 (a probability of 0.0588), the post-study odds will be less that 1.0 - i.e., the study\u0027s hypothesis will be more likely false than true.\u003cbr/\u003e\u003cbr/\u003eFor non-Bayesians, statistical significance testing presumes a uninformative prior probability - i.e., pre-study odds (R) of 1. Then LR would merely need to exceed 1 for the study\u0027s conclusions to be more likely true than false. At the common significance levels (alpha) of 0.05 and 0.01, the requisite study powers would merely need to exceed 0.05 and 0.01 respectively, corresponding to maximum type II error rates (beta) of 0.95 and 0.99. Such lax requirements would almost always be met for a published study. Hence, the common belief that the vast majority of studies have valid conclusions would be correct, if we can assume that the pre-study odds are truly uninformative. However, as Ioannidis suggests, this is unlikely to be the case.\u003cbr/\u003e\u003cbr/\u003eTwo more corollaries might be added. The higher the pre-study odds that the study\u0027s hypothesis is true, the lower the requisite power (study size and effect size) required to make the study\u0027s findings more likely true than false. When studies are published, the investigator should estimate the pre-study odds and report the LR implied by the observed effect.\u003cbr/\u003e\u003cbr/\u003eFrom the perspective of an epidemiologist or a statistician, the relevant question is whether the study\u0027s hypothesis is true - i.e., is the probability of H1 greater than 0.5. For clinicians and their patients, the relevant question is whether a particular strategy should be followed in an individual patient or a subset of similar patients. That decision (or recommendation to the patient) will depend on the pre-study likelihood of benefit in that patient and on the relative magnitude of benefits and risks of that strategy, if the diagnosis in that patient is uncertain. For many such decisions, the \u0026quot;more likely true than false\u0026quot; criterion may not be the best decision rule. For serious diseases and treatments of only modest risk, post-study probabilities of considerably less than 0.5 may be sufficient to justify treatment [2].\u003cbr/\u003e\u003cbr/\u003eIoannidis\u0027s provocative essay is a timely call for careful consideration of published studies. The odds-likelihood formulation suggested herein may be helpful in providing a more intuitive model. Clinicians now need to take it to the next step.\u003cbr/\u003e\u003cbr/\u003eReferences\u003cbr/\u003e1. Ioannidis JPA (2005). PLoS Med 2(8):e124.\u003cbr/\u003e2. Pauker SG, Kassirer JP (1975). N Eng J Med 293:229.\u003cbr/\u003e\u003cbr/\u003e\u003c/p\u003e",
    "competingInterestStatement": "",
    "truncatedCompetingInterestStatement": "",
    "annotationUri": "10.1371/annotation/6c65e813-e021-4335-8750-3173ec96cd88",
    "creatorID": 172479,
    "creatorDisplayName": "plosmedicine",
    "creatorFormattedName": "PLoS Medicine Staff",
    "articleID": 16292,
    "articleDoi": "info:doi/10.1371/journal.pmed.0020124",
    "articleTitle": "Why Most Published Research Findings Are False",
    "created": "2009-03-30T23:45:46Z",
    "createdFormatted": "2009-03-30T23:45:46Z",
    "type": "COMMENT",
    "replies": [],
    "lastReplyDate": "2009-03-30T23:45:46Z",
    "totalNumReplies": 0
  },
  {
    "originalTitle": "Problems in the underlying analysis",
    "title": "Problems in the underlying analysis",
    "body": "\u003cp\u003eAuthor: Steven Goodman\u003cbr/\u003ePosition: Associate Professor\u003cbr/\u003eInstitution: Johns Hopkins University\u003cbr/\u003eE-mail: sgoodman@jhmi.edu\u003cbr/\u003eAdditional Authors: Sander Greenland\u003cbr/\u003eSubmitted Date: February 28, 2007\u003cbr/\u003ePublished Date: March 1, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eThe article published in PLoS Medicine by Ioannidis makes the dramatic claim in the title that \u0026ldquo;most published research claims are false,\u0026rdquo; and has received extensive attention as a result. The article does provide a useful reminder that the probability of hypotheses depends on much more than just the P-value, a point that has been made in the medical literature for at least four decades, and in the statistical literature for decades previous. This topic has renewed importance with advent of the massive multiple testing often seen in genomics studies.\u003cbr/\u003e\u003cbr/\u003eUnfortunately, while we agree that there are more false claims than many would suspect \u0026ndash; based both on poor study design, misinterpretation of P-values, and perhaps analytic manipulation - the mathematical argument in the PLoS Medicine paper underlying the \u0026ldquo;proof\u0026rdquo; underlying the title\u0026rsquo;s claim has a degree of circularity. As we show in detail in a separately published paper (\u003ca href\u003d\"http://www.bepress.com/jhubiostat/paper135\" title\u003d\"http://www.bepress.com/jhubiostat/paper135\"\u003ewww.bepress.com/jhubiosta...\u003c/a\u003e), Dr. Ioannidis utilizes a mathematical model that severely diminishes the evidential value of studies - even meta-analyses \u0026ndash; such that none can produce more than modest evidence against the null hypothesis, and most are far weaker. This is why, in the offered \u0026ldquo;proof\u0026rdquo;, the only study types that achieve a posterior probability of 50% or more (large RCTs and meta-analysis of RCTs) are those to which are assigned a prior probability of 50% or more. So the model employed cannot be considered a proof that most published claims are untrue, but is rather a claim that no study or combination of studies can ever provide convincing evidence.\u003cbr/\u003e\u003cbr/\u003eThe two assumptions that produce the above effect are:\u003cbr/\u003e 1) Calculating the evidential effect only of verdicts of \u0026ldquo;significance\u0026rdquo;, i.e. P\u003d0.05, instead of the actual P-value observed in a study, e.g. P\u003d0.001.\u003cbr/\u003e\u003cbr/\u003e 2) Introducing a new \u0026ldquo;bias\u0026rdquo; term into the Bayesian calculations, which even at a described \u0026ldquo;minimal\u0026rdquo; level (of 10%) has the effect of very dramatically diminishing a study\u0026rsquo;s evidential impact.\u003cbr/\u003e\u003cbr/\u003e In addition to the above problems, the paper claims to have proven something it describes as paradoxical; that the \u0026ldquo;hotter\u0026rdquo; an area is (i.e. the more studies published), the more likely studies in that area are to make false claims. We have shown this claim to be erroneous (\u003ca href\u003d\"http://www.bepress.com/jhubiostat/paper135\" title\u003d\"http://www.bepress.com/jhubiostat/paper135\"\u003ewww.bepress.com/jhubiosta...\u003c/a\u003e). The mathematical proof offered for this in the PLoS paper shows merely that the more studies published on any subject, the higher the absolute number of false positive (and false negative) studies. It does not show what the papers\u0026rsquo; graphs and text claim, viz, that the number of false claims will be a higher proportion of the total number of studies published(i.e. that the positive predictive value of each study decreases with increasing number of studies).\u003cbr/\u003e\u003cbr/\u003eThe paper offers useful guidance in a number of areas, calling attention to the importance of avoiding all forms of bias, of obtaining more empirical research on the prevalence of various forms of bias and on the determinants of prior odds of hypotheses. But the claims that the model employed in this paper constitutes a \u0026ldquo;proof\u0026rdquo; that most published medical research claims are false, and that research in \u0026ldquo;hot\u0026rdquo; areas is most likely to be false, are unfounded.\u003c/p\u003e",
    "originalBody": "Author: Steven Goodman\nPosition: Associate Professor\nInstitution: Johns Hopkins University\nE-mail: sgoodman@jhmi.edu\nAdditional Authors: Sander Greenland\nSubmitted Date: February 28, 2007\nPublished Date: March 1, 2007\nThis comment was originally posted as a “Reader Response” on the publication date indicated above. All Reader Responses are now available as comments.\n\nThe article published in PLoS Medicine by Ioannidis makes the dramatic claim in the title that “most published research claims are false,” and has received extensive attention as a result. The article does provide a useful reminder that the probability of hypotheses depends on much more than just the P-value, a point that has been made in the medical literature for at least four decades, and in the statistical literature for decades previous. This topic has renewed importance with advent of the massive multiple testing often seen in genomics studies.\n\nUnfortunately, while we agree that there are more false claims than many would suspect – based both on poor study design, misinterpretation of P-values, and perhaps analytic manipulation - the mathematical argument in the PLoS Medicine paper underlying the “proof” underlying the title’s claim has a degree of circularity. As we show in detail in a separately published paper (www.bepress.com/jhubiostat/paper135), Dr. Ioannidis utilizes a mathematical model that severely diminishes the evidential value of studies - even meta-analyses – such that none can produce more than modest evidence against the null hypothesis, and most are far weaker. This is why, in the offered “proof”, the only study types that achieve a posterior probability of 50% or more (large RCTs and meta-analysis of RCTs) are those to which are assigned a prior probability of 50% or more. So the model employed cannot be considered a proof that most published claims are untrue, but is rather a claim that no study or combination of studies can ever provide convincing evidence.\n\nThe two assumptions that produce the above effect are:\n 1) Calculating the evidential effect only of verdicts of “significance”, i.e. P\u003d0.05, instead of the actual P-value observed in a study, e.g. P\u003d0.001.\n\n 2) Introducing a new “bias” term into the Bayesian calculations, which even at a described “minimal” level (of 10%) has the effect of very dramatically diminishing a study’s evidential impact.\n\n In addition to the above problems, the paper claims to have proven something it describes as paradoxical; that the “hotter” an area is (i.e. the more studies published), the more likely studies in that area are to make false claims. We have shown this claim to be erroneous (www.bepress.com/jhubiostat/paper135). The mathematical proof offered for this in the PLoS paper shows merely that the more studies published on any subject, the higher the absolute number of false positive (and false negative) studies. It does not show what the papers’ graphs and text claim, viz, that the number of false claims will be a higher proportion of the total number of studies published(i.e. that the positive predictive value of each study decreases with increasing number of studies).\n\nThe paper offers useful guidance in a number of areas, calling attention to the importance of avoiding all forms of bias, of obtaining more empirical research on the prevalence of various forms of bias and on the determinants of prior odds of hypotheses. But the claims that the model employed in this paper constitutes a “proof” that most published medical research claims are false, and that research in “hot” areas is most likely to be false, are unfounded.",
    "truncatedBody": "\u003cp\u003eAuthor: Steven Goodman\u003cbr/\u003ePosition: Associate Professor\u003cbr/\u003eInstitution: Johns Hopkins University\u003cbr/\u003eE-mail: sgoodman@jhmi.edu\u003cbr/\u003eAdditional Authors: Sander Greenland\u003cbr/\u003eSubmitted Date: February 28, 2007\u003cbr/\u003ePublished Date: March 1, 2007\u003cbr/\u003eThis...\u003c/p\u003e",
    "bodyWithUrlLinkingNoPTags": "Author: Steven Goodman\u003cbr/\u003ePosition: Associate Professor\u003cbr/\u003eInstitution: Johns Hopkins University\u003cbr/\u003eE-mail: sgoodman@jhmi.edu\u003cbr/\u003eAdditional Authors: Sander Greenland\u003cbr/\u003eSubmitted Date: February 28, 2007\u003cbr/\u003ePublished Date: March 1, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eThe article published in PLoS Medicine by Ioannidis makes the dramatic claim in the title that \u0026ldquo;most published research claims are false,\u0026rdquo; and has received extensive attention as a result. The article does provide a useful reminder that the probability of hypotheses depends on much more than just the P-value, a point that has been made in the medical literature for at least four decades, and in the statistical literature for decades previous. This topic has renewed importance with advent of the massive multiple testing often seen in genomics studies.\u003cbr/\u003e\u003cbr/\u003eUnfortunately, while we agree that there are more false claims than many would suspect \u0026ndash; based both on poor study design, misinterpretation of P-values, and perhaps analytic manipulation - the mathematical argument in the PLoS Medicine paper underlying the \u0026ldquo;proof\u0026rdquo; underlying the title\u0026rsquo;s claim has a degree of circularity. As we show in detail in a separately published paper (\u003ca href\u003d\"http://www.bepress.com/jhubiostat/paper135\" title\u003d\"http://www.bepress.com/jhubiostat/paper135\"\u003ewww.bepress.com/jhubiosta...\u003c/a\u003e), Dr. Ioannidis utilizes a mathematical model that severely diminishes the evidential value of studies - even meta-analyses \u0026ndash; such that none can produce more than modest evidence against the null hypothesis, and most are far weaker. This is why, in the offered \u0026ldquo;proof\u0026rdquo;, the only study types that achieve a posterior probability of 50% or more (large RCTs and meta-analysis of RCTs) are those to which are assigned a prior probability of 50% or more. So the model employed cannot be considered a proof that most published claims are untrue, but is rather a claim that no study or combination of studies can ever provide convincing evidence.\u003cbr/\u003e\u003cbr/\u003eThe two assumptions that produce the above effect are:\u003cbr/\u003e 1) Calculating the evidential effect only of verdicts of \u0026ldquo;significance\u0026rdquo;, i.e. P\u003d0.05, instead of the actual P-value observed in a study, e.g. P\u003d0.001.\u003cbr/\u003e\u003cbr/\u003e 2) Introducing a new \u0026ldquo;bias\u0026rdquo; term into the Bayesian calculations, which even at a described \u0026ldquo;minimal\u0026rdquo; level (of 10%) has the effect of very dramatically diminishing a study\u0026rsquo;s evidential impact.\u003cbr/\u003e\u003cbr/\u003e In addition to the above problems, the paper claims to have proven something it describes as paradoxical; that the \u0026ldquo;hotter\u0026rdquo; an area is (i.e. the more studies published), the more likely studies in that area are to make false claims. We have shown this claim to be erroneous (\u003ca href\u003d\"http://www.bepress.com/jhubiostat/paper135\" title\u003d\"http://www.bepress.com/jhubiostat/paper135\"\u003ewww.bepress.com/jhubiosta...\u003c/a\u003e). The mathematical proof offered for this in the PLoS paper shows merely that the more studies published on any subject, the higher the absolute number of false positive (and false negative) studies. It does not show what the papers\u0026rsquo; graphs and text claim, viz, that the number of false claims will be a higher proportion of the total number of studies published(i.e. that the positive predictive value of each study decreases with increasing number of studies).\u003cbr/\u003e\u003cbr/\u003eThe paper offers useful guidance in a number of areas, calling attention to the importance of avoiding all forms of bias, of obtaining more empirical research on the prevalence of various forms of bias and on the determinants of prior odds of hypotheses. But the claims that the model employed in this paper constitutes a \u0026ldquo;proof\u0026rdquo; that most published medical research claims are false, and that research in \u0026ldquo;hot\u0026rdquo; areas is most likely to be false, are unfounded.",
    "truncatedBodyWithUrlLinkingNoPTags": "Author: Steven Goodman\u003cbr/\u003ePosition: Associate Professor\u003cbr/\u003eInstitution: Johns Hopkins University\u003cbr/\u003eE-mail: sgoodman@jhmi.edu\u003cbr/\u003eAdditional Authors: Sander Greenland\u003cbr/\u003eSubmitted Date: February 28, 2007\u003cbr/\u003ePublished Date: March 1, 2007\u003cbr/\u003eThis...",
    "bodyWithHighlightedText": "\u003cp\u003eAuthor: Steven Goodman\u003cbr/\u003ePosition: Associate Professor\u003cbr/\u003eInstitution: Johns Hopkins University\u003cbr/\u003eE-mail: sgoodman@jhmi.edu\u003cbr/\u003eAdditional Authors: Sander Greenland\u003cbr/\u003eSubmitted Date: February 28, 2007\u003cbr/\u003ePublished Date: March 1, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eThe article published in PLoS Medicine by Ioannidis makes the dramatic claim in the title that \u0026ldquo;most published research claims are false,\u0026rdquo; and has received extensive attention as a result. The article does provide a useful reminder that the probability of hypotheses depends on much more than just the P-value, a point that has been made in the medical literature for at least four decades, and in the statistical literature for decades previous. This topic has renewed importance with advent of the massive multiple testing often seen in genomics studies.\u003cbr/\u003e\u003cbr/\u003eUnfortunately, while we agree that there are more false claims than many would suspect \u0026ndash; based both on poor study design, misinterpretation of P-values, and perhaps analytic manipulation - the mathematical argument in the PLoS Medicine paper underlying the \u0026ldquo;proof\u0026rdquo; underlying the title\u0026rsquo;s claim has a degree of circularity. As we show in detail in a separately published paper (\u003ca href\u003d\"http://www.bepress.com/jhubiostat/paper135\" title\u003d\"http://www.bepress.com/jhubiostat/paper135\"\u003ewww.bepress.com/jhubiosta...\u003c/a\u003e), Dr. Ioannidis utilizes a mathematical model that severely diminishes the evidential value of studies - even meta-analyses \u0026ndash; such that none can produce more than modest evidence against the null hypothesis, and most are far weaker. This is why, in the offered \u0026ldquo;proof\u0026rdquo;, the only study types that achieve a posterior probability of 50% or more (large RCTs and meta-analysis of RCTs) are those to which are assigned a prior probability of 50% or more. So the model employed cannot be considered a proof that most published claims are untrue, but is rather a claim that no study or combination of studies can ever provide convincing evidence.\u003cbr/\u003e\u003cbr/\u003eThe two assumptions that produce the above effect are:\u003cbr/\u003e 1) Calculating the evidential effect only of verdicts of \u0026ldquo;significance\u0026rdquo;, i.e. P\u003d0.05, instead of the actual P-value observed in a study, e.g. P\u003d0.001.\u003cbr/\u003e\u003cbr/\u003e 2) Introducing a new \u0026ldquo;bias\u0026rdquo; term into the Bayesian calculations, which even at a described \u0026ldquo;minimal\u0026rdquo; level (of 10%) has the effect of very dramatically diminishing a study\u0026rsquo;s evidential impact.\u003cbr/\u003e\u003cbr/\u003e In addition to the above problems, the paper claims to have proven something it describes as paradoxical; that the \u0026ldquo;hotter\u0026rdquo; an area is (i.e. the more studies published), the more likely studies in that area are to make false claims. We have shown this claim to be erroneous (\u003ca href\u003d\"http://www.bepress.com/jhubiostat/paper135\" title\u003d\"http://www.bepress.com/jhubiostat/paper135\"\u003ewww.bepress.com/jhubiosta...\u003c/a\u003e). The mathematical proof offered for this in the PLoS paper shows merely that the more studies published on any subject, the higher the absolute number of false positive (and false negative) studies. It does not show what the papers\u0026rsquo; graphs and text claim, viz, that the number of false claims will be a higher proportion of the total number of studies published(i.e. that the positive predictive value of each study decreases with increasing number of studies).\u003cbr/\u003e\u003cbr/\u003eThe paper offers useful guidance in a number of areas, calling attention to the importance of avoiding all forms of bias, of obtaining more empirical research on the prevalence of various forms of bias and on the determinants of prior odds of hypotheses. But the claims that the model employed in this paper constitutes a \u0026ldquo;proof\u0026rdquo; that most published medical research claims are false, and that research in \u0026ldquo;hot\u0026rdquo; areas is most likely to be false, are unfounded.\u003c/p\u003e",
    "competingInterestStatement": "",
    "truncatedCompetingInterestStatement": "",
    "annotationUri": "10.1371/annotation/71c1f105-d443-4ac7-a6c0-1933ee5d6ae4",
    "creatorID": 172479,
    "creatorDisplayName": "plosmedicine",
    "creatorFormattedName": "PLoS Medicine Staff",
    "articleID": 16292,
    "articleDoi": "info:doi/10.1371/journal.pmed.0020124",
    "articleTitle": "Why Most Published Research Findings Are False",
    "created": "2009-03-31T00:06:32Z",
    "createdFormatted": "2009-03-31T00:06:32Z",
    "type": "COMMENT",
    "replies": [],
    "lastReplyDate": "2009-03-31T00:06:32Z",
    "totalNumReplies": 0
  },
  {
    "originalTitle": "Why Most Published Research Findings Are False: How about Functional Brain Mapping?",
    "title": "Why Most Published Research Findings Are False: How about Functional Brain Mapping?",
    "body": "\u003cp\u003eAuthor: Donald F. Smith\u003cbr/\u003ePosition: Center for Psychiatric Research\u003cbr/\u003eInstitution: Psychiatric Hospital of Aarhus University\u003cbr/\u003eE-mail: dfsmith@inet.uni2.dk\u003cbr/\u003eSubmitted Date: June 29, 2007\u003cbr/\u003ePublished Date: June 29, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eThe article published in PLoS Medicine by Ioannidis [1] is a masterpiece in drawing attention to errors that can arise from massive multiple testing during phases of rapid growth in research publications. The focus of that article was on genomics, while I believe that the lessons to be learned apply equally well to functional brain mapping because that topic has several features that may also be highly conducive to publishing false findings.\u003cbr/\u003e\u003cbr/\u003eAs noted by Ioannidis [1], the probability of obtaining a false finding is determined by the p-value, the statistical power (SP), and the pre-study probability (R) that a finding is true. Thus, a research finding is more likely false than true if (SP)(R) lesser than p-value [1]. For a functional brain mapping study with a statistical power of 90% and the p-value of 5%, a finding is more likely false than true if the pre-study probability (R) that the finding is true is less than 5.6% (i.e. R lesser than (0.05)/(0.9). What, then, is the pre-study probability that a change in signal strength will occur in a particular site of the brain in response to a particular mental task? Although that question may often be difficult to answer, I believe that the pre-study probability in most studies of functional brain mapping is well below 1%. If that guesstimate is correct, then most published research findings in functional brain mapping may be false.\u003cbr/\u003e\u003cbr/\u003eUnsubstantiated assumptions weaken research findings. One unsubstantiated assumption behind much research in functional brain mapping is that the brain consists of many separate parts, and that each part has a specific mental function. That notion has been criticized strongly [2], but it still persists despite recent interest in functional connectivity [3]. It is surprising that so many studies in functional brain mapping have assumed that complex mental functions are localized at specific sites in the brain, despite strong evidence to the contrary [4;5].\u003cbr/\u003e\u003cbr/\u003eAnother unsubstantiated assumption beneath much research in functional brain mapping is that cognitive processing in the brain is linear, such that new cognitive components can be inserted sequentially without being affected by previous ones [6]. Friston and coworkers have given a detailed account of errors that can result from assuming that brain function is linear during cognitive tasks [2].\u003cbr/\u003e\u003cbr/\u003eA third unsubstantiated assumption of much research in functional brain mapping is that the blood oxygen-level dependent signal (BOLD) measured by magnetic resonance provides a valid measure of neuronal activity at sites in the brain [7;8]. BOLD arises from susceptibility effects of deoxyhemoglobin in venous blood and reflects, thereby, the blood oxygen level [8]. Research has shown, however, that BOLD may reflect neuronal activity only under certain, as yet undefined, conditions [9;10].\u003cbr/\u003e\u003cbr/\u003eThe probability of detecting a true effect, known as statistical power, is low in experiments with small sample size [11;12]. 16 \u0026ndash; 17 subjects are often required to obtain reliable results in studies of functional brain mapping [12-14], whereas the number of subjects used in 75 studies of functional brain mapping 10 years ago ranged from 4 to 34 with a median of 9 [12]. In preparation for this article, I found that the number of subjects ranged from 3 to 38 with a median of 10 in 90 recent functional brain mapping reports, and that such studies continue to use predominantly right-handed, college-age males [12].\u003cbr/\u003e\u003cbr/\u003eData processing and statistical analysis in functional brain mapping are complicated and involve massive multiple testing [15-19].  Decisions made by a researcher during data processing and analysis may reflect confirmatory bias [20;21]. Confirmatory bias usually involves unwitting selectivity in experimental design, data analysis, and reporting of evidence, and is not necessarily synonymous with fraud or conscious data manipulation [20;22;23]. For example, confirmatory bias may come into play when negative (i.e. non-statistically significant) findings are initially obtained. Under such circumstances, one may be tempted to devise secondary research questions and to do further data analyses in search of hypothesis-supporting evidence, a behavior commonly known as data-dredging [22;24;25].\u003cbr/\u003e\u003cbr/\u003eReducing the likelihood of publishing false findings in functional brain mapping may require a major effort. Perhaps particular attention should be given to publishing of negative findings and of results from experiments carried out specifically to replicate someone else\u0026rsquo;s findings. Failure to reduce false findings in functional brain mapping may eventually undermine the field.\u003cbr/\u003e\u003cbr/\u003eReferences available on request.\u003cbr/\u003e\u003cbr/\u003eword count: 711 in main text\u003cbr/\u003e\u003cbr/\u003eReferences\u003cbr/\u003e\u003cbr/\u003e1. \tIoannidis JP (2005): Why most published research findings are false. PLoS.Med 2:e124.\u003cbr/\u003e2. \tFriston KJ, Price CJ, Fletcher P, Moore C, Frackowiak RS, Dolan RJ (1996): The trouble with cognitive subtraction. NeuroImage 4:97-104.\u003cbr/\u003e3. \tRamnani N, Behrens TE, Penny W, Matthews PM (2004): New approaches for exploring anatomical and functional connectivity in the human brain. Biol.Psychiatry 56:613-619.\u003cbr/\u003e4. \tCabeza R, Nyberg L (2000): Imaging cognition II: An empirical review of 275 PET and fMRI studies. J.Cogn.Neurosci. 12:1-47.\u003cbr/\u003e5. \tVigneau M, Beaucousin V, Herve PY, Duffau H, Crivello F, Houde O, Mazoyer B, Tzourio-Mazoyer N (2006): Meta-analyzing left hemisphere language areas: phonology, semantics, and sentence processing. NeuroImage 30:1414-1432.\u003cbr/\u003e6. \tLee A, Kannan V, Hillis AE (2006): The contribution of neuroimaging to the study of language and aphasia. Neuropsychol.Rev. 16:171-183.\u003cbr/\u003e7. \tMatthews PM, Jezzard P (2004): Functional magnetic resonance imaging. J Neurol.Neurosurg.Psychiatry 75:6-12.\u003cbr/\u003e8. \tLogothetis NK (2003): The underpinnings of the BOLD functional magnetic resonance imaging signal. J Neurosci. 23:3963-3971.\u003cbr/\u003e9. \tLauritzen M (2001): Relationship of spikes, synaptic activity, and local changes of cerebral blood flow. J Cereb.Blood Flow Metab 21:1367-1383.\u003cbr/\u003e10. \tThomsen K, Offenhauser N, Lauritzen M (2004): Principal neuron spiking: neither necessary nor sufficient for cerebral blood flow in rat cerebellum. J Physiol 560:181-189.\u003cbr/\u003e11. \tGuilford JP (1965): Fundamental statistics in psychology and education. McGraw-Hill Book Company, New York.\u003cbr/\u003e12. \tAndreasen NC, Arndt S, Cizadlo T, O\u0027Leary DS, Watkins GL, Ponto LL, Hichwa RD (1996): Sample size and statistical power in [15O]H2O studies of human cognition. J Cereb Blood Flow Metab 16:804-816.\u003cbr/\u003e13. \tJernigan TL, Gamst AC, Fennema-Notestine C, Ostergaard AL (2003): More \u0026quot;mapping\u0026quot; in brain mapping: statistical comparison of effects. Hum.Brain Mapp. 19:90-95.\u003cbr/\u003e14. \tGold S, Arndt S, Johnson D, O\u0027Leary DS, Andreasen NC (1997): Factors that influence effect size of 15O PET studies: a meta-analytic review. NeuroImage 5:280-291.\u003cbr/\u003e15. \tWorsley K, Marrett S, Neelin P, Vandal AC, Friston KJ, Evans AC (1996): A unified statistical approach for determining significant signals in images of cerebral activation. Hum.Brain Mapp. 4:58-73.\u003cbr/\u003e16. \tWorsley KJ (2005): An improved theoretical P value for SPMs based on discrete local maxima. NeuroImage 28:1056-1062.\u003cbr/\u003e17. \tFriston K (2002): Beyond phrenology: what can neuroimaging tell us about distributed circuitry? Annu.Rev.Neurosci. 25:221-250.\u003cbr/\u003e18. \tFriston KJ, Holmes AP, Worsley KJ, Poline JB, Frith CD, Frackowiak RSJ (1995): Statistical parametric maps in functional imaging: a general linear approach. Human Brain Mapping 2:189-210.\u003cbr/\u003e19. \tFriston KJ, Rotshtein P, Geng JJ, Sterzer P, Henson RN (2006): A critique of functional localisers. NeuroImage 30:1077-1087.\u003cbr/\u003e20. \tNickerson RS (1998): Confirmation Bias: a ubiquitous phenomenon in many guises. Review of General Psychology 2:175-220.\u003cbr/\u003e21. \tWacholder S, Chanock S, Garcia-Closas M, El Ghormli L, Rothman N (2004): Assessing the probability that a positive report is false: an approach for molecular epidemiology studies. J Natl.Cancer Inst. 96:434-442.\u003cbr/\u003e22. \tLindner MD, Frydel BR, Francis JM, Cain CK (2003): Analgesic effects of adrenal chromaffin allografts: contingent on special procedures or due to experimenter bias? J Pain 4:64-73.\u003cbr/\u003e23. \tMele AR (1997): Real self-deception. Behav.Brain Sci. 20:91-102.\u003cbr/\u003e24. \tLord SJ, Gebski VJ, Keech AC (2004): Multiple analyses in clinical trials: sound science or data dredging? Med J Aust. 181:452-454.\u003cbr/\u003e25. \tLee WC, Huang HY (2005): Data-dredging gene-dose analyses in association studies: biases and their corrections. Cancer Epidemiol.Biomarkers Prev. 14:3004-3006.\u003c/p\u003e",
    "originalBody": "Author: Donald F. Smith\nPosition: Center for Psychiatric Research\nInstitution: Psychiatric Hospital of Aarhus University\nE-mail: dfsmith@inet.uni2.dk\nSubmitted Date: June 29, 2007\nPublished Date: June 29, 2007\nThis comment was originally posted as a “Reader Response” on the publication date indicated above. All Reader Responses are now available as comments.\n\nThe article published in PLoS Medicine by Ioannidis [1] is a masterpiece in drawing attention to errors that can arise from massive multiple testing during phases of rapid growth in research publications. The focus of that article was on genomics, while I believe that the lessons to be learned apply equally well to functional brain mapping because that topic has several features that may also be highly conducive to publishing false findings.\n\nAs noted by Ioannidis [1], the probability of obtaining a false finding is determined by the p-value, the statistical power (SP), and the pre-study probability (R) that a finding is true. Thus, a research finding is more likely false than true if (SP)(R) lesser than p-value [1]. For a functional brain mapping study with a statistical power of 90% and the p-value of 5%, a finding is more likely false than true if the pre-study probability (R) that the finding is true is less than 5.6% (i.e. R lesser than (0.05)/(0.9). What, then, is the pre-study probability that a change in signal strength will occur in a particular site of the brain in response to a particular mental task? Although that question may often be difficult to answer, I believe that the pre-study probability in most studies of functional brain mapping is well below 1%. If that guesstimate is correct, then most published research findings in functional brain mapping may be false.\n\nUnsubstantiated assumptions weaken research findings. One unsubstantiated assumption behind much research in functional brain mapping is that the brain consists of many separate parts, and that each part has a specific mental function. That notion has been criticized strongly [2], but it still persists despite recent interest in functional connectivity [3]. It is surprising that so many studies in functional brain mapping have assumed that complex mental functions are localized at specific sites in the brain, despite strong evidence to the contrary [4;5].\n\nAnother unsubstantiated assumption beneath much research in functional brain mapping is that cognitive processing in the brain is linear, such that new cognitive components can be inserted sequentially without being affected by previous ones [6]. Friston and coworkers have given a detailed account of errors that can result from assuming that brain function is linear during cognitive tasks [2].\n\nA third unsubstantiated assumption of much research in functional brain mapping is that the blood oxygen-level dependent signal (BOLD) measured by magnetic resonance provides a valid measure of neuronal activity at sites in the brain [7;8]. BOLD arises from susceptibility effects of deoxyhemoglobin in venous blood and reflects, thereby, the blood oxygen level [8]. Research has shown, however, that BOLD may reflect neuronal activity only under certain, as yet undefined, conditions [9;10].\n\nThe probability of detecting a true effect, known as statistical power, is low in experiments with small sample size [11;12]. 16 – 17 subjects are often required to obtain reliable results in studies of functional brain mapping [12-14], whereas the number of subjects used in 75 studies of functional brain mapping 10 years ago ranged from 4 to 34 with a median of 9 [12]. In preparation for this article, I found that the number of subjects ranged from 3 to 38 with a median of 10 in 90 recent functional brain mapping reports, and that such studies continue to use predominantly right-handed, college-age males [12].\n\nData processing and statistical analysis in functional brain mapping are complicated and involve massive multiple testing [15-19].  Decisions made by a researcher during data processing and analysis may reflect confirmatory bias [20;21]. Confirmatory bias usually involves unwitting selectivity in experimental design, data analysis, and reporting of evidence, and is not necessarily synonymous with fraud or conscious data manipulation [20;22;23]. For example, confirmatory bias may come into play when negative (i.e. non-statistically significant) findings are initially obtained. Under such circumstances, one may be tempted to devise secondary research questions and to do further data analyses in search of hypothesis-supporting evidence, a behavior commonly known as data-dredging [22;24;25].\n\nReducing the likelihood of publishing false findings in functional brain mapping may require a major effort. Perhaps particular attention should be given to publishing of negative findings and of results from experiments carried out specifically to replicate someone else’s findings. Failure to reduce false findings in functional brain mapping may eventually undermine the field.\n\nReferences available on request.\n\nword count: 711 in main text\n\nReferences\n\n1. \tIoannidis JP (2005): Why most published research findings are false. PLoS.Med 2:e124.\n2. \tFriston KJ, Price CJ, Fletcher P, Moore C, Frackowiak RS, Dolan RJ (1996): The trouble with cognitive subtraction. NeuroImage 4:97-104.\n3. \tRamnani N, Behrens TE, Penny W, Matthews PM (2004): New approaches for exploring anatomical and functional connectivity in the human brain. Biol.Psychiatry 56:613-619.\n4. \tCabeza R, Nyberg L (2000): Imaging cognition II: An empirical review of 275 PET and fMRI studies. J.Cogn.Neurosci. 12:1-47.\n5. \tVigneau M, Beaucousin V, Herve PY, Duffau H, Crivello F, Houde O, Mazoyer B, Tzourio-Mazoyer N (2006): Meta-analyzing left hemisphere language areas: phonology, semantics, and sentence processing. NeuroImage 30:1414-1432.\n6. \tLee A, Kannan V, Hillis AE (2006): The contribution of neuroimaging to the study of language and aphasia. Neuropsychol.Rev. 16:171-183.\n7. \tMatthews PM, Jezzard P (2004): Functional magnetic resonance imaging. J Neurol.Neurosurg.Psychiatry 75:6-12.\n8. \tLogothetis NK (2003): The underpinnings of the BOLD functional magnetic resonance imaging signal. J Neurosci. 23:3963-3971.\n9. \tLauritzen M (2001): Relationship of spikes, synaptic activity, and local changes of cerebral blood flow. J Cereb.Blood Flow Metab 21:1367-1383.\n10. \tThomsen K, Offenhauser N, Lauritzen M (2004): Principal neuron spiking: neither necessary nor sufficient for cerebral blood flow in rat cerebellum. J Physiol 560:181-189.\n11. \tGuilford JP (1965): Fundamental statistics in psychology and education. McGraw-Hill Book Company, New York.\n12. \tAndreasen NC, Arndt S, Cizadlo T, O\u0027Leary DS, Watkins GL, Ponto LL, Hichwa RD (1996): Sample size and statistical power in [15O]H2O studies of human cognition. J Cereb Blood Flow Metab 16:804-816.\n13. \tJernigan TL, Gamst AC, Fennema-Notestine C, Ostergaard AL (2003): More \"mapping\" in brain mapping: statistical comparison of effects. Hum.Brain Mapp. 19:90-95.\n14. \tGold S, Arndt S, Johnson D, O\u0027Leary DS, Andreasen NC (1997): Factors that influence effect size of 15O PET studies: a meta-analytic review. NeuroImage 5:280-291.\n15. \tWorsley K, Marrett S, Neelin P, Vandal AC, Friston KJ, Evans AC (1996): A unified statistical approach for determining significant signals in images of cerebral activation. Hum.Brain Mapp. 4:58-73.\n16. \tWorsley KJ (2005): An improved theoretical P value for SPMs based on discrete local maxima. NeuroImage 28:1056-1062.\n17. \tFriston K (2002): Beyond phrenology: what can neuroimaging tell us about distributed circuitry? Annu.Rev.Neurosci. 25:221-250.\n18. \tFriston KJ, Holmes AP, Worsley KJ, Poline JB, Frith CD, Frackowiak RSJ (1995): Statistical parametric maps in functional imaging: a general linear approach. Human Brain Mapping 2:189-210.\n19. \tFriston KJ, Rotshtein P, Geng JJ, Sterzer P, Henson RN (2006): A critique of functional localisers. NeuroImage 30:1077-1087.\n20. \tNickerson RS (1998): Confirmation Bias: a ubiquitous phenomenon in many guises. Review of General Psychology 2:175-220.\n21. \tWacholder S, Chanock S, Garcia-Closas M, El Ghormli L, Rothman N (2004): Assessing the probability that a positive report is false: an approach for molecular epidemiology studies. J Natl.Cancer Inst. 96:434-442.\n22. \tLindner MD, Frydel BR, Francis JM, Cain CK (2003): Analgesic effects of adrenal chromaffin allografts: contingent on special procedures or due to experimenter bias? J Pain 4:64-73.\n23. \tMele AR (1997): Real self-deception. Behav.Brain Sci. 20:91-102.\n24. \tLord SJ, Gebski VJ, Keech AC (2004): Multiple analyses in clinical trials: sound science or data dredging? Med J Aust. 181:452-454.\n25. \tLee WC, Huang HY (2005): Data-dredging gene-dose analyses in association studies: biases and their corrections. Cancer Epidemiol.Biomarkers Prev. 14:3004-3006.",
    "truncatedBody": "\u003cp\u003eAuthor: Donald F. Smith\u003cbr/\u003ePosition: Center for Psychiatric Research\u003cbr/\u003eInstitution: Psychiatric Hospital of Aarhus University\u003cbr/\u003eE-mail: dfsmith@inet.uni2.dk\u003cbr/\u003eSubmitted Date: June 29, 2007\u003cbr/\u003ePublished Date: June 29, 2007\u003cbr/\u003eThis comment was...\u003c/p\u003e",
    "bodyWithUrlLinkingNoPTags": "Author: Donald F. Smith\u003cbr/\u003ePosition: Center for Psychiatric Research\u003cbr/\u003eInstitution: Psychiatric Hospital of Aarhus University\u003cbr/\u003eE-mail: dfsmith@inet.uni2.dk\u003cbr/\u003eSubmitted Date: June 29, 2007\u003cbr/\u003ePublished Date: June 29, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eThe article published in PLoS Medicine by Ioannidis [1] is a masterpiece in drawing attention to errors that can arise from massive multiple testing during phases of rapid growth in research publications. The focus of that article was on genomics, while I believe that the lessons to be learned apply equally well to functional brain mapping because that topic has several features that may also be highly conducive to publishing false findings.\u003cbr/\u003e\u003cbr/\u003eAs noted by Ioannidis [1], the probability of obtaining a false finding is determined by the p-value, the statistical power (SP), and the pre-study probability (R) that a finding is true. Thus, a research finding is more likely false than true if (SP)(R) lesser than p-value [1]. For a functional brain mapping study with a statistical power of 90% and the p-value of 5%, a finding is more likely false than true if the pre-study probability (R) that the finding is true is less than 5.6% (i.e. R lesser than (0.05)/(0.9). What, then, is the pre-study probability that a change in signal strength will occur in a particular site of the brain in response to a particular mental task? Although that question may often be difficult to answer, I believe that the pre-study probability in most studies of functional brain mapping is well below 1%. If that guesstimate is correct, then most published research findings in functional brain mapping may be false.\u003cbr/\u003e\u003cbr/\u003eUnsubstantiated assumptions weaken research findings. One unsubstantiated assumption behind much research in functional brain mapping is that the brain consists of many separate parts, and that each part has a specific mental function. That notion has been criticized strongly [2], but it still persists despite recent interest in functional connectivity [3]. It is surprising that so many studies in functional brain mapping have assumed that complex mental functions are localized at specific sites in the brain, despite strong evidence to the contrary [4;5].\u003cbr/\u003e\u003cbr/\u003eAnother unsubstantiated assumption beneath much research in functional brain mapping is that cognitive processing in the brain is linear, such that new cognitive components can be inserted sequentially without being affected by previous ones [6]. Friston and coworkers have given a detailed account of errors that can result from assuming that brain function is linear during cognitive tasks [2].\u003cbr/\u003e\u003cbr/\u003eA third unsubstantiated assumption of much research in functional brain mapping is that the blood oxygen-level dependent signal (BOLD) measured by magnetic resonance provides a valid measure of neuronal activity at sites in the brain [7;8]. BOLD arises from susceptibility effects of deoxyhemoglobin in venous blood and reflects, thereby, the blood oxygen level [8]. Research has shown, however, that BOLD may reflect neuronal activity only under certain, as yet undefined, conditions [9;10].\u003cbr/\u003e\u003cbr/\u003eThe probability of detecting a true effect, known as statistical power, is low in experiments with small sample size [11;12]. 16 \u0026ndash; 17 subjects are often required to obtain reliable results in studies of functional brain mapping [12-14], whereas the number of subjects used in 75 studies of functional brain mapping 10 years ago ranged from 4 to 34 with a median of 9 [12]. In preparation for this article, I found that the number of subjects ranged from 3 to 38 with a median of 10 in 90 recent functional brain mapping reports, and that such studies continue to use predominantly right-handed, college-age males [12].\u003cbr/\u003e\u003cbr/\u003eData processing and statistical analysis in functional brain mapping are complicated and involve massive multiple testing [15-19].  Decisions made by a researcher during data processing and analysis may reflect confirmatory bias [20;21]. Confirmatory bias usually involves unwitting selectivity in experimental design, data analysis, and reporting of evidence, and is not necessarily synonymous with fraud or conscious data manipulation [20;22;23]. For example, confirmatory bias may come into play when negative (i.e. non-statistically significant) findings are initially obtained. Under such circumstances, one may be tempted to devise secondary research questions and to do further data analyses in search of hypothesis-supporting evidence, a behavior commonly known as data-dredging [22;24;25].\u003cbr/\u003e\u003cbr/\u003eReducing the likelihood of publishing false findings in functional brain mapping may require a major effort. Perhaps particular attention should be given to publishing of negative findings and of results from experiments carried out specifically to replicate someone else\u0026rsquo;s findings. Failure to reduce false findings in functional brain mapping may eventually undermine the field.\u003cbr/\u003e\u003cbr/\u003eReferences available on request.\u003cbr/\u003e\u003cbr/\u003eword count: 711 in main text\u003cbr/\u003e\u003cbr/\u003eReferences\u003cbr/\u003e\u003cbr/\u003e1. \tIoannidis JP (2005): Why most published research findings are false. PLoS.Med 2:e124.\u003cbr/\u003e2. \tFriston KJ, Price CJ, Fletcher P, Moore C, Frackowiak RS, Dolan RJ (1996): The trouble with cognitive subtraction. NeuroImage 4:97-104.\u003cbr/\u003e3. \tRamnani N, Behrens TE, Penny W, Matthews PM (2004): New approaches for exploring anatomical and functional connectivity in the human brain. Biol.Psychiatry 56:613-619.\u003cbr/\u003e4. \tCabeza R, Nyberg L (2000): Imaging cognition II: An empirical review of 275 PET and fMRI studies. J.Cogn.Neurosci. 12:1-47.\u003cbr/\u003e5. \tVigneau M, Beaucousin V, Herve PY, Duffau H, Crivello F, Houde O, Mazoyer B, Tzourio-Mazoyer N (2006): Meta-analyzing left hemisphere language areas: phonology, semantics, and sentence processing. NeuroImage 30:1414-1432.\u003cbr/\u003e6. \tLee A, Kannan V, Hillis AE (2006): The contribution of neuroimaging to the study of language and aphasia. Neuropsychol.Rev. 16:171-183.\u003cbr/\u003e7. \tMatthews PM, Jezzard P (2004): Functional magnetic resonance imaging. J Neurol.Neurosurg.Psychiatry 75:6-12.\u003cbr/\u003e8. \tLogothetis NK (2003): The underpinnings of the BOLD functional magnetic resonance imaging signal. J Neurosci. 23:3963-3971.\u003cbr/\u003e9. \tLauritzen M (2001): Relationship of spikes, synaptic activity, and local changes of cerebral blood flow. J Cereb.Blood Flow Metab 21:1367-1383.\u003cbr/\u003e10. \tThomsen K, Offenhauser N, Lauritzen M (2004): Principal neuron spiking: neither necessary nor sufficient for cerebral blood flow in rat cerebellum. J Physiol 560:181-189.\u003cbr/\u003e11. \tGuilford JP (1965): Fundamental statistics in psychology and education. McGraw-Hill Book Company, New York.\u003cbr/\u003e12. \tAndreasen NC, Arndt S, Cizadlo T, O\u0027Leary DS, Watkins GL, Ponto LL, Hichwa RD (1996): Sample size and statistical power in [15O]H2O studies of human cognition. J Cereb Blood Flow Metab 16:804-816.\u003cbr/\u003e13. \tJernigan TL, Gamst AC, Fennema-Notestine C, Ostergaard AL (2003): More \u0026quot;mapping\u0026quot; in brain mapping: statistical comparison of effects. Hum.Brain Mapp. 19:90-95.\u003cbr/\u003e14. \tGold S, Arndt S, Johnson D, O\u0027Leary DS, Andreasen NC (1997): Factors that influence effect size of 15O PET studies: a meta-analytic review. NeuroImage 5:280-291.\u003cbr/\u003e15. \tWorsley K, Marrett S, Neelin P, Vandal AC, Friston KJ, Evans AC (1996): A unified statistical approach for determining significant signals in images of cerebral activation. Hum.Brain Mapp. 4:58-73.\u003cbr/\u003e16. \tWorsley KJ (2005): An improved theoretical P value for SPMs based on discrete local maxima. NeuroImage 28:1056-1062.\u003cbr/\u003e17. \tFriston K (2002): Beyond phrenology: what can neuroimaging tell us about distributed circuitry? Annu.Rev.Neurosci. 25:221-250.\u003cbr/\u003e18. \tFriston KJ, Holmes AP, Worsley KJ, Poline JB, Frith CD, Frackowiak RSJ (1995): Statistical parametric maps in functional imaging: a general linear approach. Human Brain Mapping 2:189-210.\u003cbr/\u003e19. \tFriston KJ, Rotshtein P, Geng JJ, Sterzer P, Henson RN (2006): A critique of functional localisers. NeuroImage 30:1077-1087.\u003cbr/\u003e20. \tNickerson RS (1998): Confirmation Bias: a ubiquitous phenomenon in many guises. Review of General Psychology 2:175-220.\u003cbr/\u003e21. \tWacholder S, Chanock S, Garcia-Closas M, El Ghormli L, Rothman N (2004): Assessing the probability that a positive report is false: an approach for molecular epidemiology studies. J Natl.Cancer Inst. 96:434-442.\u003cbr/\u003e22. \tLindner MD, Frydel BR, Francis JM, Cain CK (2003): Analgesic effects of adrenal chromaffin allografts: contingent on special procedures or due to experimenter bias? J Pain 4:64-73.\u003cbr/\u003e23. \tMele AR (1997): Real self-deception. Behav.Brain Sci. 20:91-102.\u003cbr/\u003e24. \tLord SJ, Gebski VJ, Keech AC (2004): Multiple analyses in clinical trials: sound science or data dredging? Med J Aust. 181:452-454.\u003cbr/\u003e25. \tLee WC, Huang HY (2005): Data-dredging gene-dose analyses in association studies: biases and their corrections. Cancer Epidemiol.Biomarkers Prev. 14:3004-3006.",
    "truncatedBodyWithUrlLinkingNoPTags": "Author: Donald F. Smith\u003cbr/\u003ePosition: Center for Psychiatric Research\u003cbr/\u003eInstitution: Psychiatric Hospital of Aarhus University\u003cbr/\u003eE-mail: dfsmith@inet.uni2.dk\u003cbr/\u003eSubmitted Date: June 29, 2007\u003cbr/\u003ePublished Date: June 29, 2007\u003cbr/\u003eThis comment was...",
    "bodyWithHighlightedText": "\u003cp\u003eAuthor: Donald F. Smith\u003cbr/\u003ePosition: Center for Psychiatric Research\u003cbr/\u003eInstitution: Psychiatric Hospital of Aarhus University\u003cbr/\u003eE-mail: dfsmith@inet.uni2.dk\u003cbr/\u003eSubmitted Date: June 29, 2007\u003cbr/\u003ePublished Date: June 29, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eThe article published in PLoS Medicine by Ioannidis [1] is a masterpiece in drawing attention to errors that can arise from massive multiple testing during phases of rapid growth in research publications. The focus of that article was on genomics, while I believe that the lessons to be learned apply equally well to functional brain mapping because that topic has several features that may also be highly conducive to publishing false findings.\u003cbr/\u003e\u003cbr/\u003eAs noted by Ioannidis [1], the probability of obtaining a false finding is determined by the p-value, the statistical power (SP), and the pre-study probability (R) that a finding is true. Thus, a research finding is more likely false than true if (SP)(R) lesser than p-value [1]. For a functional brain mapping study with a statistical power of 90% and the p-value of 5%, a finding is more likely false than true if the pre-study probability (R) that the finding is true is less than 5.6% (i.e. R lesser than (0.05)/(0.9). What, then, is the pre-study probability that a change in signal strength will occur in a particular site of the brain in response to a particular mental task? Although that question may often be difficult to answer, I believe that the pre-study probability in most studies of functional brain mapping is well below 1%. If that guesstimate is correct, then most published research findings in functional brain mapping may be false.\u003cbr/\u003e\u003cbr/\u003eUnsubstantiated assumptions weaken research findings. One unsubstantiated assumption behind much research in functional brain mapping is that the brain consists of many separate parts, and that each part has a specific mental function. That notion has been criticized strongly [2], but it still persists despite recent interest in functional connectivity [3]. It is surprising that so many studies in functional brain mapping have assumed that complex mental functions are localized at specific sites in the brain, despite strong evidence to the contrary [4;5].\u003cbr/\u003e\u003cbr/\u003eAnother unsubstantiated assumption beneath much research in functional brain mapping is that cognitive processing in the brain is linear, such that new cognitive components can be inserted sequentially without being affected by previous ones [6]. Friston and coworkers have given a detailed account of errors that can result from assuming that brain function is linear during cognitive tasks [2].\u003cbr/\u003e\u003cbr/\u003eA third unsubstantiated assumption of much research in functional brain mapping is that the blood oxygen-level dependent signal (BOLD) measured by magnetic resonance provides a valid measure of neuronal activity at sites in the brain [7;8]. BOLD arises from susceptibility effects of deoxyhemoglobin in venous blood and reflects, thereby, the blood oxygen level [8]. Research has shown, however, that BOLD may reflect neuronal activity only under certain, as yet undefined, conditions [9;10].\u003cbr/\u003e\u003cbr/\u003eThe probability of detecting a true effect, known as statistical power, is low in experiments with small sample size [11;12]. 16 \u0026ndash; 17 subjects are often required to obtain reliable results in studies of functional brain mapping [12-14], whereas the number of subjects used in 75 studies of functional brain mapping 10 years ago ranged from 4 to 34 with a median of 9 [12]. In preparation for this article, I found that the number of subjects ranged from 3 to 38 with a median of 10 in 90 recent functional brain mapping reports, and that such studies continue to use predominantly right-handed, college-age males [12].\u003cbr/\u003e\u003cbr/\u003eData processing and statistical analysis in functional brain mapping are complicated and involve massive multiple testing [15-19].  Decisions made by a researcher during data processing and analysis may reflect confirmatory bias [20;21]. Confirmatory bias usually involves unwitting selectivity in experimental design, data analysis, and reporting of evidence, and is not necessarily synonymous with fraud or conscious data manipulation [20;22;23]. For example, confirmatory bias may come into play when negative (i.e. non-statistically significant) findings are initially obtained. Under such circumstances, one may be tempted to devise secondary research questions and to do further data analyses in search of hypothesis-supporting evidence, a behavior commonly known as data-dredging [22;24;25].\u003cbr/\u003e\u003cbr/\u003eReducing the likelihood of publishing false findings in functional brain mapping may require a major effort. Perhaps particular attention should be given to publishing of negative findings and of results from experiments carried out specifically to replicate someone else\u0026rsquo;s findings. Failure to reduce false findings in functional brain mapping may eventually undermine the field.\u003cbr/\u003e\u003cbr/\u003eReferences available on request.\u003cbr/\u003e\u003cbr/\u003eword count: 711 in main text\u003cbr/\u003e\u003cbr/\u003eReferences\u003cbr/\u003e\u003cbr/\u003e1. \tIoannidis JP (2005): Why most published research findings are false. PLoS.Med 2:e124.\u003cbr/\u003e2. \tFriston KJ, Price CJ, Fletcher P, Moore C, Frackowiak RS, Dolan RJ (1996): The trouble with cognitive subtraction. NeuroImage 4:97-104.\u003cbr/\u003e3. \tRamnani N, Behrens TE, Penny W, Matthews PM (2004): New approaches for exploring anatomical and functional connectivity in the human brain. Biol.Psychiatry 56:613-619.\u003cbr/\u003e4. \tCabeza R, Nyberg L (2000): Imaging cognition II: An empirical review of 275 PET and fMRI studies. J.Cogn.Neurosci. 12:1-47.\u003cbr/\u003e5. \tVigneau M, Beaucousin V, Herve PY, Duffau H, Crivello F, Houde O, Mazoyer B, Tzourio-Mazoyer N (2006): Meta-analyzing left hemisphere language areas: phonology, semantics, and sentence processing. NeuroImage 30:1414-1432.\u003cbr/\u003e6. \tLee A, Kannan V, Hillis AE (2006): The contribution of neuroimaging to the study of language and aphasia. Neuropsychol.Rev. 16:171-183.\u003cbr/\u003e7. \tMatthews PM, Jezzard P (2004): Functional magnetic resonance imaging. J Neurol.Neurosurg.Psychiatry 75:6-12.\u003cbr/\u003e8. \tLogothetis NK (2003): The underpinnings of the BOLD functional magnetic resonance imaging signal. J Neurosci. 23:3963-3971.\u003cbr/\u003e9. \tLauritzen M (2001): Relationship of spikes, synaptic activity, and local changes of cerebral blood flow. J Cereb.Blood Flow Metab 21:1367-1383.\u003cbr/\u003e10. \tThomsen K, Offenhauser N, Lauritzen M (2004): Principal neuron spiking: neither necessary nor sufficient for cerebral blood flow in rat cerebellum. J Physiol 560:181-189.\u003cbr/\u003e11. \tGuilford JP (1965): Fundamental statistics in psychology and education. McGraw-Hill Book Company, New York.\u003cbr/\u003e12. \tAndreasen NC, Arndt S, Cizadlo T, O\u0027Leary DS, Watkins GL, Ponto LL, Hichwa RD (1996): Sample size and statistical power in [15O]H2O studies of human cognition. J Cereb Blood Flow Metab 16:804-816.\u003cbr/\u003e13. \tJernigan TL, Gamst AC, Fennema-Notestine C, Ostergaard AL (2003): More \u0026quot;mapping\u0026quot; in brain mapping: statistical comparison of effects. Hum.Brain Mapp. 19:90-95.\u003cbr/\u003e14. \tGold S, Arndt S, Johnson D, O\u0027Leary DS, Andreasen NC (1997): Factors that influence effect size of 15O PET studies: a meta-analytic review. NeuroImage 5:280-291.\u003cbr/\u003e15. \tWorsley K, Marrett S, Neelin P, Vandal AC, Friston KJ, Evans AC (1996): A unified statistical approach for determining significant signals in images of cerebral activation. Hum.Brain Mapp. 4:58-73.\u003cbr/\u003e16. \tWorsley KJ (2005): An improved theoretical P value for SPMs based on discrete local maxima. NeuroImage 28:1056-1062.\u003cbr/\u003e17. \tFriston K (2002): Beyond phrenology: what can neuroimaging tell us about distributed circuitry? Annu.Rev.Neurosci. 25:221-250.\u003cbr/\u003e18. \tFriston KJ, Holmes AP, Worsley KJ, Poline JB, Frith CD, Frackowiak RSJ (1995): Statistical parametric maps in functional imaging: a general linear approach. Human Brain Mapping 2:189-210.\u003cbr/\u003e19. \tFriston KJ, Rotshtein P, Geng JJ, Sterzer P, Henson RN (2006): A critique of functional localisers. NeuroImage 30:1077-1087.\u003cbr/\u003e20. \tNickerson RS (1998): Confirmation Bias: a ubiquitous phenomenon in many guises. Review of General Psychology 2:175-220.\u003cbr/\u003e21. \tWacholder S, Chanock S, Garcia-Closas M, El Ghormli L, Rothman N (2004): Assessing the probability that a positive report is false: an approach for molecular epidemiology studies. J Natl.Cancer Inst. 96:434-442.\u003cbr/\u003e22. \tLindner MD, Frydel BR, Francis JM, Cain CK (2003): Analgesic effects of adrenal chromaffin allografts: contingent on special procedures or due to experimenter bias? J Pain 4:64-73.\u003cbr/\u003e23. \tMele AR (1997): Real self-deception. Behav.Brain Sci. 20:91-102.\u003cbr/\u003e24. \tLord SJ, Gebski VJ, Keech AC (2004): Multiple analyses in clinical trials: sound science or data dredging? Med J Aust. 181:452-454.\u003cbr/\u003e25. \tLee WC, Huang HY (2005): Data-dredging gene-dose analyses in association studies: biases and their corrections. Cancer Epidemiol.Biomarkers Prev. 14:3004-3006.\u003c/p\u003e",
    "competingInterestStatement": "",
    "truncatedCompetingInterestStatement": "",
    "annotationUri": "10.1371/annotation/72ee6b4a-178b-4e61-bcde-53e976bb3925",
    "creatorID": 172479,
    "creatorDisplayName": "plosmedicine",
    "creatorFormattedName": "PLoS Medicine Staff",
    "articleID": 16292,
    "articleDoi": "info:doi/10.1371/journal.pmed.0020124",
    "articleTitle": "Why Most Published Research Findings Are False",
    "created": "2009-03-31T00:10:31Z",
    "createdFormatted": "2009-03-31T00:10:31Z",
    "type": "COMMENT",
    "replies": [],
    "lastReplyDate": "2009-03-31T00:10:31Z",
    "totalNumReplies": 0
  },
  {
    "originalTitle": "Why most published research findings are false",
    "title": "Why most published research findings are false",
    "body": "\u003cp\u003eAuthor: Professor Brian Peskin\u003cbr/\u003ePosition: Research scientist\u003cbr/\u003eInstitution: Cambridge International Institute for Medical Science\u003cbr/\u003eE-mail: prof-nutrition@sbcglobal.net\u003cbr/\u003eSubmitted Date: October 27, 2007\u003cbr/\u003ePublished Date: October 29, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eDr. Ioannidis\u0027 article is exceptional. More researchers need to know that most of what is published in the medical journals is wrong. Statistical analysis is often wrong, too (Dr. Stanton Glatz).\u003cbr/\u003e\u003cbr/\u003eThank you for explaining why.\u003c/p\u003e",
    "originalBody": "Author: Professor Brian Peskin\nPosition: Research scientist\nInstitution: Cambridge International Institute for Medical Science\nE-mail: prof-nutrition@sbcglobal.net\nSubmitted Date: October 27, 2007\nPublished Date: October 29, 2007\nThis comment was originally posted as a “Reader Response” on the publication date indicated above. All Reader Responses are now available as comments.\n\nDr. Ioannidis\u0027 article is exceptional. More researchers need to know that most of what is published in the medical journals is wrong. Statistical analysis is often wrong, too (Dr. Stanton Glatz).\n\nThank you for explaining why.",
    "truncatedBody": "\u003cp\u003eAuthor: Professor Brian Peskin\u003cbr/\u003ePosition: Research scientist\u003cbr/\u003eInstitution: Cambridge International Institute for Medical Science\u003cbr/\u003eE-mail: prof-nutrition@sbcglobal.net\u003cbr/\u003eSubmitted Date: October 27, 2007\u003cbr/\u003ePublished Date: October 29,...\u003c/p\u003e",
    "bodyWithUrlLinkingNoPTags": "Author: Professor Brian Peskin\u003cbr/\u003ePosition: Research scientist\u003cbr/\u003eInstitution: Cambridge International Institute for Medical Science\u003cbr/\u003eE-mail: prof-nutrition@sbcglobal.net\u003cbr/\u003eSubmitted Date: October 27, 2007\u003cbr/\u003ePublished Date: October 29, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eDr. Ioannidis\u0027 article is exceptional. More researchers need to know that most of what is published in the medical journals is wrong. Statistical analysis is often wrong, too (Dr. Stanton Glatz).\u003cbr/\u003e\u003cbr/\u003eThank you for explaining why.",
    "truncatedBodyWithUrlLinkingNoPTags": "Author: Professor Brian Peskin\u003cbr/\u003ePosition: Research scientist\u003cbr/\u003eInstitution: Cambridge International Institute for Medical Science\u003cbr/\u003eE-mail: prof-nutrition@sbcglobal.net\u003cbr/\u003eSubmitted Date: October 27, 2007\u003cbr/\u003ePublished Date: October 29,...",
    "bodyWithHighlightedText": "\u003cp\u003eAuthor: Professor Brian Peskin\u003cbr/\u003ePosition: Research scientist\u003cbr/\u003eInstitution: Cambridge International Institute for Medical Science\u003cbr/\u003eE-mail: prof-nutrition@sbcglobal.net\u003cbr/\u003eSubmitted Date: October 27, 2007\u003cbr/\u003ePublished Date: October 29, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eDr. Ioannidis\u0027 article is exceptional. More researchers need to know that most of what is published in the medical journals is wrong. Statistical analysis is often wrong, too (Dr. Stanton Glatz).\u003cbr/\u003e\u003cbr/\u003eThank you for explaining why.\u003c/p\u003e",
    "competingInterestStatement": "",
    "truncatedCompetingInterestStatement": "",
    "annotationUri": "10.1371/annotation/75f84e17-046e-4f78-b60a-c56c2398bc64",
    "creatorID": 172479,
    "creatorDisplayName": "plosmedicine",
    "creatorFormattedName": "PLoS Medicine Staff",
    "articleID": 16292,
    "articleDoi": "info:doi/10.1371/journal.pmed.0020124",
    "articleTitle": "Why Most Published Research Findings Are False",
    "created": "2009-03-31T00:15:46Z",
    "createdFormatted": "2009-03-31T00:15:46Z",
    "type": "COMMENT",
    "replies": [],
    "lastReplyDate": "2009-03-31T00:15:46Z",
    "totalNumReplies": 0
  },
  {
    "originalTitle": "Error in \"Why most published research findings are false\"",
    "title": "Error in \u0026quot;Why most published research findings are false\u0026quot;",
    "body": "\u003cp\u003eAuthor: Art Owen\u003cbr/\u003ePosition: Professor of Statistics\u003cbr/\u003eInstitution: Stanford University\u003cbr/\u003eE-mail: owen@stat.stanford.edu\u003cbr/\u003eSubmitted Date: September 27, 2005\u003cbr/\u003ePublished Date: October 6, 2005\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eThere is an error in the Table 2 entry for the row with research finding \u003d yes and the column with true relationship \u003d no. Plugging in u\u003d0 should make Table 2 equal to Table 1 but it does not. There should be parentheses starting at c alpha and ending just before the division by R+1.\u003cbr/\u003e\u003cbr/\u003eThe PPV formula given for Table 2 is unaffected by this error.\u003cbr/\u003e\u003cbr/\u003eDespite the irony of there being an error in an article with such a title, I found the article useful.  It provides a good reminder of the need to think about issues of bias and power and the ratio R.\u003cbr/\u003e\u003c/p\u003e",
    "originalBody": "Author: Art Owen\nPosition: Professor of Statistics\nInstitution: Stanford University\nE-mail: owen@stat.stanford.edu\nSubmitted Date: September 27, 2005\nPublished Date: October 6, 2005\nThis comment was originally posted as a “Reader Response” on the publication date indicated above. All Reader Responses are now available as comments.\n\nThere is an error in the Table 2 entry for the row with research finding \u003d yes and the column with true relationship \u003d no. Plugging in u\u003d0 should make Table 2 equal to Table 1 but it does not. There should be parentheses starting at c alpha and ending just before the division by R+1.\n\nThe PPV formula given for Table 2 is unaffected by this error.\n\nDespite the irony of there being an error in an article with such a title, I found the article useful.  It provides a good reminder of the need to think about issues of bias and power and the ratio R.\n",
    "truncatedBody": "\u003cp\u003eAuthor: Art Owen\u003cbr/\u003ePosition: Professor of Statistics\u003cbr/\u003eInstitution: Stanford University\u003cbr/\u003eE-mail: owen@stat.stanford.edu\u003cbr/\u003eSubmitted Date: September 27, 2005\u003cbr/\u003ePublished Date: October 6, 2005\u003cbr/\u003eThis comment was originally posted as a...\u003c/p\u003e",
    "bodyWithUrlLinkingNoPTags": "Author: Art Owen\u003cbr/\u003ePosition: Professor of Statistics\u003cbr/\u003eInstitution: Stanford University\u003cbr/\u003eE-mail: owen@stat.stanford.edu\u003cbr/\u003eSubmitted Date: September 27, 2005\u003cbr/\u003ePublished Date: October 6, 2005\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eThere is an error in the Table 2 entry for the row with research finding \u003d yes and the column with true relationship \u003d no. Plugging in u\u003d0 should make Table 2 equal to Table 1 but it does not. There should be parentheses starting at c alpha and ending just before the division by R+1.\u003cbr/\u003e\u003cbr/\u003eThe PPV formula given for Table 2 is unaffected by this error.\u003cbr/\u003e\u003cbr/\u003eDespite the irony of there being an error in an article with such a title, I found the article useful.  It provides a good reminder of the need to think about issues of bias and power and the ratio R.\u003cbr/\u003e",
    "truncatedBodyWithUrlLinkingNoPTags": "Author: Art Owen\u003cbr/\u003ePosition: Professor of Statistics\u003cbr/\u003eInstitution: Stanford University\u003cbr/\u003eE-mail: owen@stat.stanford.edu\u003cbr/\u003eSubmitted Date: September 27, 2005\u003cbr/\u003ePublished Date: October 6, 2005\u003cbr/\u003eThis comment was originally posted as a...",
    "bodyWithHighlightedText": "\u003cp\u003eAuthor: Art Owen\u003cbr/\u003ePosition: Professor of Statistics\u003cbr/\u003eInstitution: Stanford University\u003cbr/\u003eE-mail: owen@stat.stanford.edu\u003cbr/\u003eSubmitted Date: September 27, 2005\u003cbr/\u003ePublished Date: October 6, 2005\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eThere is an error in the Table 2 entry for the row with research finding \u003d yes and the column with true relationship \u003d no. Plugging in u\u003d0 should make Table 2 equal to Table 1 but it does not. There should be parentheses starting at c alpha and ending just before the division by R+1.\u003cbr/\u003e\u003cbr/\u003eThe PPV formula given for Table 2 is unaffected by this error.\u003cbr/\u003e\u003cbr/\u003eDespite the irony of there being an error in an article with such a title, I found the article useful.  It provides a good reminder of the need to think about issues of bias and power and the ratio R.\u003cbr/\u003e\u003c/p\u003e",
    "competingInterestStatement": "I declare that I have no competing interests.\u003cbr/\u003e",
    "truncatedCompetingInterestStatement": "I declare that I have no competing interests.\u003cbr/\u003e",
    "annotationUri": "10.1371/annotation/762cbd6d-cff3-4188-b3db-0225188ac58f",
    "creatorID": 172479,
    "creatorDisplayName": "plosmedicine",
    "creatorFormattedName": "PLoS Medicine Staff",
    "articleID": 16292,
    "articleDoi": "info:doi/10.1371/journal.pmed.0020124",
    "articleTitle": "Why Most Published Research Findings Are False",
    "created": "2009-03-30T23:46:39Z",
    "createdFormatted": "2009-03-30T23:46:39Z",
    "type": "COMMENT",
    "replies": [],
    "lastReplyDate": "2009-03-30T23:46:39Z",
    "totalNumReplies": 0
  },
  {
    "originalTitle": "Let Truth Bare Sway",
    "title": "Let Truth Bare Sway",
    "body": "\u003cp\u003eAuthor: mark bellisario\u003cbr/\u003ePosition: student\u003cbr/\u003eInstitution: UCSF\u003cbr/\u003eE-mail: mrbellisario@yahoo.com\u003cbr/\u003eSubmitted Date: October 04, 2007\u003cbr/\u003ePublished Date: October 8, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eI agree that most research shows up as false positives.  I feel that the race for some research groups to publish findings on hot, new science topics as quickly as possible, or being payed by some companies to produce the results needed, has lead to a boom of lacking research.  I mean researchers that don\u0027t spend enough time preparing, and executing so that confounding variables can be ruled out, the number of subjects can be increased, more trials and studies with similar outcomes, etc.\u003cbr/\u003e\u003cbr/\u003eThe literature is full of these types of studies that rely on p-value much more than repetition.\u003c/p\u003e",
    "originalBody": "Author: mark bellisario\nPosition: student\nInstitution: UCSF\nE-mail: mrbellisario@yahoo.com\nSubmitted Date: October 04, 2007\nPublished Date: October 8, 2007\nThis comment was originally posted as a “Reader Response” on the publication date indicated above. All Reader Responses are now available as comments.\n\nI agree that most research shows up as false positives.  I feel that the race for some research groups to publish findings on hot, new science topics as quickly as possible, or being payed by some companies to produce the results needed, has lead to a boom of lacking research.  I mean researchers that don\u0027t spend enough time preparing, and executing so that confounding variables can be ruled out, the number of subjects can be increased, more trials and studies with similar outcomes, etc.\n\nThe literature is full of these types of studies that rely on p-value much more than repetition.",
    "truncatedBody": "\u003cp\u003eAuthor: mark bellisario\u003cbr/\u003ePosition: student\u003cbr/\u003eInstitution: UCSF\u003cbr/\u003eE-mail: mrbellisario@yahoo.com\u003cbr/\u003eSubmitted Date: October 04, 2007\u003cbr/\u003ePublished Date: October 8, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on...\u003c/p\u003e",
    "bodyWithUrlLinkingNoPTags": "Author: mark bellisario\u003cbr/\u003ePosition: student\u003cbr/\u003eInstitution: UCSF\u003cbr/\u003eE-mail: mrbellisario@yahoo.com\u003cbr/\u003eSubmitted Date: October 04, 2007\u003cbr/\u003ePublished Date: October 8, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eI agree that most research shows up as false positives.  I feel that the race for some research groups to publish findings on hot, new science topics as quickly as possible, or being payed by some companies to produce the results needed, has lead to a boom of lacking research.  I mean researchers that don\u0027t spend enough time preparing, and executing so that confounding variables can be ruled out, the number of subjects can be increased, more trials and studies with similar outcomes, etc.\u003cbr/\u003e\u003cbr/\u003eThe literature is full of these types of studies that rely on p-value much more than repetition.",
    "truncatedBodyWithUrlLinkingNoPTags": "Author: mark bellisario\u003cbr/\u003ePosition: student\u003cbr/\u003eInstitution: UCSF\u003cbr/\u003eE-mail: mrbellisario@yahoo.com\u003cbr/\u003eSubmitted Date: October 04, 2007\u003cbr/\u003ePublished Date: October 8, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on...",
    "bodyWithHighlightedText": "\u003cp\u003eAuthor: mark bellisario\u003cbr/\u003ePosition: student\u003cbr/\u003eInstitution: UCSF\u003cbr/\u003eE-mail: mrbellisario@yahoo.com\u003cbr/\u003eSubmitted Date: October 04, 2007\u003cbr/\u003ePublished Date: October 8, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eI agree that most research shows up as false positives.  I feel that the race for some research groups to publish findings on hot, new science topics as quickly as possible, or being payed by some companies to produce the results needed, has lead to a boom of lacking research.  I mean researchers that don\u0027t spend enough time preparing, and executing so that confounding variables can be ruled out, the number of subjects can be increased, more trials and studies with similar outcomes, etc.\u003cbr/\u003e\u003cbr/\u003eThe literature is full of these types of studies that rely on p-value much more than repetition.\u003c/p\u003e",
    "competingInterestStatement": "",
    "truncatedCompetingInterestStatement": "",
    "annotationUri": "10.1371/annotation/8181a7c2-8746-4f00-9cfc-1db2d28c89b7",
    "creatorID": 172479,
    "creatorDisplayName": "plosmedicine",
    "creatorFormattedName": "PLoS Medicine Staff",
    "articleID": 16292,
    "articleDoi": "info:doi/10.1371/journal.pmed.0020124",
    "articleTitle": "Why Most Published Research Findings Are False",
    "created": "2009-03-31T00:14:54Z",
    "createdFormatted": "2009-03-31T00:14:54Z",
    "type": "COMMENT",
    "replies": [],
    "lastReplyDate": "2009-03-31T00:14:54Z",
    "totalNumReplies": 0
  },
  {
    "originalTitle": "Responce",
    "title": "Responce",
    "body": "\u003cp\u003eAuthor: Scott Crane\u003cbr/\u003eInstitution: UCSF Student\u003cbr/\u003eE-mail: scottdcrane@hotmail.com\u003cbr/\u003eSubmitted Date: October 06, 2007\u003cbr/\u003ePublished Date: October 8, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eI don\u0027t know that we can totally say that most published research findings are false.  However, I do agree that bias and small effective sizes can create problems when trying to get the best possible results.  I also think it goes without saying that manipulating results will cause the study to be inaccurate.\u003c/p\u003e",
    "originalBody": "Author: Scott Crane\nInstitution: UCSF Student\nE-mail: scottdcrane@hotmail.com\nSubmitted Date: October 06, 2007\nPublished Date: October 8, 2007\nThis comment was originally posted as a “Reader Response” on the publication date indicated above. All Reader Responses are now available as comments.\n\nI don\u0027t know that we can totally say that most published research findings are false.  However, I do agree that bias and small effective sizes can create problems when trying to get the best possible results.  I also think it goes without saying that manipulating results will cause the study to be inaccurate.",
    "truncatedBody": "\u003cp\u003eAuthor: Scott Crane\u003cbr/\u003eInstitution: UCSF Student\u003cbr/\u003eE-mail: scottdcrane@hotmail.com\u003cbr/\u003eSubmitted Date: October 06, 2007\u003cbr/\u003ePublished Date: October 8, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication...\u003c/p\u003e",
    "bodyWithUrlLinkingNoPTags": "Author: Scott Crane\u003cbr/\u003eInstitution: UCSF Student\u003cbr/\u003eE-mail: scottdcrane@hotmail.com\u003cbr/\u003eSubmitted Date: October 06, 2007\u003cbr/\u003ePublished Date: October 8, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eI don\u0027t know that we can totally say that most published research findings are false.  However, I do agree that bias and small effective sizes can create problems when trying to get the best possible results.  I also think it goes without saying that manipulating results will cause the study to be inaccurate.",
    "truncatedBodyWithUrlLinkingNoPTags": "Author: Scott Crane\u003cbr/\u003eInstitution: UCSF Student\u003cbr/\u003eE-mail: scottdcrane@hotmail.com\u003cbr/\u003eSubmitted Date: October 06, 2007\u003cbr/\u003ePublished Date: October 8, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication...",
    "bodyWithHighlightedText": "\u003cp\u003eAuthor: Scott Crane\u003cbr/\u003eInstitution: UCSF Student\u003cbr/\u003eE-mail: scottdcrane@hotmail.com\u003cbr/\u003eSubmitted Date: October 06, 2007\u003cbr/\u003ePublished Date: October 8, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eI don\u0027t know that we can totally say that most published research findings are false.  However, I do agree that bias and small effective sizes can create problems when trying to get the best possible results.  I also think it goes without saying that manipulating results will cause the study to be inaccurate.\u003c/p\u003e",
    "competingInterestStatement": "",
    "truncatedCompetingInterestStatement": "",
    "annotationUri": "10.1371/annotation/8b68834c-35b3-47a9-8ee1-4d6d293ba1f8",
    "creatorID": 172479,
    "creatorDisplayName": "plosmedicine",
    "creatorFormattedName": "PLoS Medicine Staff",
    "articleID": 16292,
    "articleDoi": "info:doi/10.1371/journal.pmed.0020124",
    "articleTitle": "Why Most Published Research Findings Are False",
    "created": "2009-03-31T00:15:09Z",
    "createdFormatted": "2009-03-31T00:15:09Z",
    "type": "COMMENT",
    "replies": [],
    "lastReplyDate": "2009-03-31T00:15:09Z",
    "totalNumReplies": 0
  },
  {
    "originalTitle": "complex experiments require advanced statistics",
    "title": "complex experiments require advanced statistics",
    "body": "\u003cp\u003eInstead of relying upon p-values, or other single-test metrics, one should rely upon concepts such as \u0026quot;uniformly most powerful\u0026quot; (UMP) unbiased tests, or \u0026quot;false discovery rates\u0026quot; (FDR), which acknowledge \u0026amp; optimize for multiple tests upon the same set of observations.\u003cbr/\u003e\u003cbr/\u003eThe author is correct, insofar as many published papers rely solely \u0026amp; too heavily upon t-test p-values.\u003c/p\u003e",
    "originalBody": "Instead of relying upon p-values, or other single-test metrics, one should rely upon concepts such as \"uniformly most powerful\" (UMP) unbiased tests, or \"false discovery rates\" (FDR), which acknowledge \u0026 optimize for multiple tests upon the same set of observations.\n\nThe author is correct, insofar as many published papers rely solely \u0026 too heavily upon t-test p-values.",
    "truncatedBody": "\u003cp\u003eInstead of relying upon p-values, or other single-test metrics, one should rely upon concepts such as \u0026quot;uniformly most powerful\u0026quot; (UMP) unbiased tests, or \u0026quot;false discovery rates\u0026quot; (FDR), which acknowledge \u0026amp; optimize for multiple...\u003c/p\u003e",
    "bodyWithUrlLinkingNoPTags": "Instead of relying upon p-values, or other single-test metrics, one should rely upon concepts such as \u0026quot;uniformly most powerful\u0026quot; (UMP) unbiased tests, or \u0026quot;false discovery rates\u0026quot; (FDR), which acknowledge \u0026amp; optimize for multiple tests upon the same set of observations.\u003cbr/\u003e\u003cbr/\u003eThe author is correct, insofar as many published papers rely solely \u0026amp; too heavily upon t-test p-values.",
    "truncatedBodyWithUrlLinkingNoPTags": "Instead of relying upon p-values, or other single-test metrics, one should rely upon concepts such as \u0026quot;uniformly most powerful\u0026quot; (UMP) unbiased tests, or \u0026quot;false discovery rates\u0026quot; (FDR), which acknowledge \u0026amp; optimize for multiple...",
    "bodyWithHighlightedText": "\u003cp\u003e\u003cem\u003eMoreover, one should be cautious that extremely large studies may be more likely to find a formally statistical significant difference for a trivial effect that is not really meaningfully different from the null\u003c/em\u003e\u003cbr/\u003e\u003ca href\u003d\"http://plosmedicine.org/article/info:doi/10.1371/journal.pmed.0020124#article1.body1.sec8.p2\"\u003ehttp://plosmedicine.org/article/info:doi/10.1371/journal.pmed.0020124#article1.body1.sec8.p2\u003c/a\u003e\u003cbr/\u003e\u003cbr/\u003eInstead of relying upon p-values, or other single-test metrics, one should rely upon concepts such as \u0026quot;uniformly most powerful\u0026quot; (UMP) unbiased tests, or \u0026quot;false discovery rates\u0026quot; (FDR), which acknowledge \u0026amp; optimize for multiple tests upon the same set of observations.\u003cbr/\u003e\u003cbr/\u003eThe author is correct, insofar as many published papers rely solely \u0026amp; too heavily upon t-test p-values.\u003c/p\u003e",
    "competingInterestStatement": "",
    "truncatedCompetingInterestStatement": "",
    "annotationUri": "10.1371/annotation/91ef1a27-d1f1-45c8-b7d1-1bb7495ba390",
    "creatorID": 153531,
    "creatorDisplayName": "lazarillo",
    "creatorFormattedName": "Mike Williamson",
    "articleID": 16292,
    "articleDoi": "info:doi/10.1371/journal.pmed.0020124",
    "articleTitle": "Why Most Published Research Findings Are False",
    "created": "2011-06-27T23:08:57Z",
    "createdFormatted": "2011-06-27T23:08:57Z",
    "type": "COMMENT",
    "replies": [],
    "lastReplyDate": "2011-06-27T23:08:57Z",
    "totalNumReplies": 0
  },
  {
    "originalTitle": "Truth Is Hard to Find",
    "title": "Truth Is Hard to Find",
    "body": "\u003cp\u003eAuthor: Benjamin Ellsworth\u003cbr/\u003ePosition: Dental Student\u003cbr/\u003eInstitution: UCSF\u003cbr/\u003eE-mail: benworth@hotmail.com\u003cbr/\u003eSubmitted Date: October 04, 2007\u003cbr/\u003ePublished Date: October 8, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eReplicated findings from parallel studies combined with meta-analysis is truly the strongest source of confident conclusions.  I have found that looking at a small number of similarly performed studies sometimes can completely contradict each other.  Finding the truth is very difficult.\u003c/p\u003e",
    "originalBody": "Author: Benjamin Ellsworth\nPosition: Dental Student\nInstitution: UCSF\nE-mail: benworth@hotmail.com\nSubmitted Date: October 04, 2007\nPublished Date: October 8, 2007\nThis comment was originally posted as a “Reader Response” on the publication date indicated above. All Reader Responses are now available as comments.\n\nReplicated findings from parallel studies combined with meta-analysis is truly the strongest source of confident conclusions.  I have found that looking at a small number of similarly performed studies sometimes can completely contradict each other.  Finding the truth is very difficult.",
    "truncatedBody": "\u003cp\u003eAuthor: Benjamin Ellsworth\u003cbr/\u003ePosition: Dental Student\u003cbr/\u003eInstitution: UCSF\u003cbr/\u003eE-mail: benworth@hotmail.com\u003cbr/\u003eSubmitted Date: October 04, 2007\u003cbr/\u003ePublished Date: October 8, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader...\u003c/p\u003e",
    "bodyWithUrlLinkingNoPTags": "Author: Benjamin Ellsworth\u003cbr/\u003ePosition: Dental Student\u003cbr/\u003eInstitution: UCSF\u003cbr/\u003eE-mail: benworth@hotmail.com\u003cbr/\u003eSubmitted Date: October 04, 2007\u003cbr/\u003ePublished Date: October 8, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eReplicated findings from parallel studies combined with meta-analysis is truly the strongest source of confident conclusions.  I have found that looking at a small number of similarly performed studies sometimes can completely contradict each other.  Finding the truth is very difficult.",
    "truncatedBodyWithUrlLinkingNoPTags": "Author: Benjamin Ellsworth\u003cbr/\u003ePosition: Dental Student\u003cbr/\u003eInstitution: UCSF\u003cbr/\u003eE-mail: benworth@hotmail.com\u003cbr/\u003eSubmitted Date: October 04, 2007\u003cbr/\u003ePublished Date: October 8, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader...",
    "bodyWithHighlightedText": "\u003cp\u003eAuthor: Benjamin Ellsworth\u003cbr/\u003ePosition: Dental Student\u003cbr/\u003eInstitution: UCSF\u003cbr/\u003eE-mail: benworth@hotmail.com\u003cbr/\u003eSubmitted Date: October 04, 2007\u003cbr/\u003ePublished Date: October 8, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eReplicated findings from parallel studies combined with meta-analysis is truly the strongest source of confident conclusions.  I have found that looking at a small number of similarly performed studies sometimes can completely contradict each other.  Finding the truth is very difficult.\u003c/p\u003e",
    "competingInterestStatement": "",
    "truncatedCompetingInterestStatement": "",
    "annotationUri": "10.1371/annotation/b76357af-7a73-401f-8e97-648eb1ffe8d6",
    "creatorID": 172479,
    "creatorDisplayName": "plosmedicine",
    "creatorFormattedName": "PLoS Medicine Staff",
    "articleID": 16292,
    "articleDoi": "info:doi/10.1371/journal.pmed.0020124",
    "articleTitle": "Why Most Published Research Findings Are False",
    "created": "2009-03-31T00:15:02Z",
    "createdFormatted": "2009-03-31T00:15:02Z",
    "type": "COMMENT",
    "replies": [],
    "lastReplyDate": "2009-03-31T00:15:02Z",
    "totalNumReplies": 0
  },
  {
    "originalTitle": "Title of paper is erroneous: most published studies are not false",
    "title": "Title of paper is erroneous: most published studies are not false",
    "body": "\u003cp\u003eAuthor: Andrew Vickers\u003cbr/\u003ePosition: Biostatistician\u003cbr/\u003eInstitution: Memorial Sloan Kettering Cancer Center\u003cbr/\u003eE-mail: vickersa@mskcc.org\u003cbr/\u003eSubmitted Date: August 15, 2006\u003cbr/\u003ePublished Date: August 18, 2006\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eI agree entirely with Dr Ioannidis\u0027 general conceptual framework: we undoubtedly need to focus on the relationship between the amount of evidence provide by a study and the prior probability of a hypothesis rather than on p values in isolation. However, the claim that \u0026quot;Most Published Research Findings Are False\u0026quot; is based on a false premise - that medical research is only about hypothesis testing, rather than estimation - and an unsubstantiated generalization about the typical distribution of prior probabilities in medical research.\u003cbr/\u003e\u003cbr/\u003eI just looked up the five most recently published studies that I co-authored. Three were concerned with estimation, for example, the expected survival of patients with a particular type of cancer and the proportion of patients undergoing a surgical procedure that would be eligible for adjuvant chemotherapy. None of Dr Ioannidis\u0027 arguments have any bearing on the value of these studies. The fourth study examined whether FCGR2A polymorphism is associated with outcome of an antibody treatment that involving the FC receptor. This strikes me as having a high prior probability, and indeed, we did find that polymorphism predicted outcome. The final study examined  risk factors for teratoma found at lymph node dissection for testicular cancer. We concluded that response to initial chemotherapy and teratoma in the orchiectomy specimen were predictive. Again, one would hardly give this a low prior probability.\u003cbr/\u003e\u003cbr/\u003eIn sum, I agree with Dr Ioannidis\u0027 methods, but not his conclusions.\u003c/p\u003e",
    "originalBody": "Author: Andrew Vickers\nPosition: Biostatistician\nInstitution: Memorial Sloan Kettering Cancer Center\nE-mail: vickersa@mskcc.org\nSubmitted Date: August 15, 2006\nPublished Date: August 18, 2006\nThis comment was originally posted as a “Reader Response” on the publication date indicated above. All Reader Responses are now available as comments.\n\nI agree entirely with Dr Ioannidis\u0027 general conceptual framework: we undoubtedly need to focus on the relationship between the amount of evidence provide by a study and the prior probability of a hypothesis rather than on p values in isolation. However, the claim that \"Most Published Research Findings Are False\" is based on a false premise - that medical research is only about hypothesis testing, rather than estimation - and an unsubstantiated generalization about the typical distribution of prior probabilities in medical research.\n\nI just looked up the five most recently published studies that I co-authored. Three were concerned with estimation, for example, the expected survival of patients with a particular type of cancer and the proportion of patients undergoing a surgical procedure that would be eligible for adjuvant chemotherapy. None of Dr Ioannidis\u0027 arguments have any bearing on the value of these studies. The fourth study examined whether FCGR2A polymorphism is associated with outcome of an antibody treatment that involving the FC receptor. This strikes me as having a high prior probability, and indeed, we did find that polymorphism predicted outcome. The final study examined  risk factors for teratoma found at lymph node dissection for testicular cancer. We concluded that response to initial chemotherapy and teratoma in the orchiectomy specimen were predictive. Again, one would hardly give this a low prior probability.\n\nIn sum, I agree with Dr Ioannidis\u0027 methods, but not his conclusions.",
    "truncatedBody": "\u003cp\u003eAuthor: Andrew Vickers\u003cbr/\u003ePosition: Biostatistician\u003cbr/\u003eInstitution: Memorial Sloan Kettering Cancer Center\u003cbr/\u003eE-mail: vickersa@mskcc.org\u003cbr/\u003eSubmitted Date: August 15, 2006\u003cbr/\u003ePublished Date: August 18, 2006\u003cbr/\u003eThis comment was originally posted...\u003c/p\u003e",
    "bodyWithUrlLinkingNoPTags": "Author: Andrew Vickers\u003cbr/\u003ePosition: Biostatistician\u003cbr/\u003eInstitution: Memorial Sloan Kettering Cancer Center\u003cbr/\u003eE-mail: vickersa@mskcc.org\u003cbr/\u003eSubmitted Date: August 15, 2006\u003cbr/\u003ePublished Date: August 18, 2006\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eI agree entirely with Dr Ioannidis\u0027 general conceptual framework: we undoubtedly need to focus on the relationship between the amount of evidence provide by a study and the prior probability of a hypothesis rather than on p values in isolation. However, the claim that \u0026quot;Most Published Research Findings Are False\u0026quot; is based on a false premise - that medical research is only about hypothesis testing, rather than estimation - and an unsubstantiated generalization about the typical distribution of prior probabilities in medical research.\u003cbr/\u003e\u003cbr/\u003eI just looked up the five most recently published studies that I co-authored. Three were concerned with estimation, for example, the expected survival of patients with a particular type of cancer and the proportion of patients undergoing a surgical procedure that would be eligible for adjuvant chemotherapy. None of Dr Ioannidis\u0027 arguments have any bearing on the value of these studies. The fourth study examined whether FCGR2A polymorphism is associated with outcome of an antibody treatment that involving the FC receptor. This strikes me as having a high prior probability, and indeed, we did find that polymorphism predicted outcome. The final study examined  risk factors for teratoma found at lymph node dissection for testicular cancer. We concluded that response to initial chemotherapy and teratoma in the orchiectomy specimen were predictive. Again, one would hardly give this a low prior probability.\u003cbr/\u003e\u003cbr/\u003eIn sum, I agree with Dr Ioannidis\u0027 methods, but not his conclusions.",
    "truncatedBodyWithUrlLinkingNoPTags": "Author: Andrew Vickers\u003cbr/\u003ePosition: Biostatistician\u003cbr/\u003eInstitution: Memorial Sloan Kettering Cancer Center\u003cbr/\u003eE-mail: vickersa@mskcc.org\u003cbr/\u003eSubmitted Date: August 15, 2006\u003cbr/\u003ePublished Date: August 18, 2006\u003cbr/\u003eThis comment was originally posted...",
    "bodyWithHighlightedText": "\u003cp\u003eAuthor: Andrew Vickers\u003cbr/\u003ePosition: Biostatistician\u003cbr/\u003eInstitution: Memorial Sloan Kettering Cancer Center\u003cbr/\u003eE-mail: vickersa@mskcc.org\u003cbr/\u003eSubmitted Date: August 15, 2006\u003cbr/\u003ePublished Date: August 18, 2006\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eI agree entirely with Dr Ioannidis\u0027 general conceptual framework: we undoubtedly need to focus on the relationship between the amount of evidence provide by a study and the prior probability of a hypothesis rather than on p values in isolation. However, the claim that \u0026quot;Most Published Research Findings Are False\u0026quot; is based on a false premise - that medical research is only about hypothesis testing, rather than estimation - and an unsubstantiated generalization about the typical distribution of prior probabilities in medical research.\u003cbr/\u003e\u003cbr/\u003eI just looked up the five most recently published studies that I co-authored. Three were concerned with estimation, for example, the expected survival of patients with a particular type of cancer and the proportion of patients undergoing a surgical procedure that would be eligible for adjuvant chemotherapy. None of Dr Ioannidis\u0027 arguments have any bearing on the value of these studies. The fourth study examined whether FCGR2A polymorphism is associated with outcome of an antibody treatment that involving the FC receptor. This strikes me as having a high prior probability, and indeed, we did find that polymorphism predicted outcome. The final study examined  risk factors for teratoma found at lymph node dissection for testicular cancer. We concluded that response to initial chemotherapy and teratoma in the orchiectomy specimen were predictive. Again, one would hardly give this a low prior probability.\u003cbr/\u003e\u003cbr/\u003eIn sum, I agree with Dr Ioannidis\u0027 methods, but not his conclusions.\u003c/p\u003e",
    "competingInterestStatement": "",
    "truncatedCompetingInterestStatement": "",
    "annotationUri": "10.1371/annotation/bd2bdc2d-71df-4864-bb2c-a0323acf5e39",
    "creatorID": 172479,
    "creatorDisplayName": "plosmedicine",
    "creatorFormattedName": "PLoS Medicine Staff",
    "articleID": 16292,
    "articleDoi": "info:doi/10.1371/journal.pmed.0020124",
    "articleTitle": "Why Most Published Research Findings Are False",
    "created": "2009-03-30T23:58:31Z",
    "createdFormatted": "2009-03-30T23:58:31Z",
    "type": "COMMENT",
    "replies": [],
    "lastReplyDate": "2009-03-30T23:58:31Z",
    "totalNumReplies": 0
  },
  {
    "originalTitle": "In reply",
    "title": "In reply",
    "body": "\u003cp\u003eAuthor: John Ioannidis\u003cbr/\u003ePosition: Professor and Chairman\u003cbr/\u003eInstitution: Dept. of Hygiene and Epidemiology, University of Ioannina School of Medicine\u003cbr/\u003eE-mail: jioannid@cc.uoi.gr\u003cbr/\u003eSubmitted Date: March 19, 2007\u003cbr/\u003ePublished Date: March 19, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eI thank Goodman and Greenland for their interesting comments.  Our methods and results are practically identical. However, some of my arguments are misrepresented.\u003cbr/\u003e\u003cbr/\u003e1. I did not \u0026ldquo;claim that no study or combination of studies can ever provide convincing evidence\u0026rdquo;.  In the illustrative examples (table 4) there is a wide credibility gradient (0.1% to 85%) for different research designs and settings.\u003cbr/\u003e\u003cbr/\u003e2. I did not assume that all significant p-values are around 0.05. Tables 1-3 and the respective PPV equations can use any p-value (alpha). Nevertheless, the p\u003d0.05 threshold is unfortunately entrenched in many scientific fields.  Almost half of the \u0026ldquo;positive\u0026rdquo; findings in recent observational studies have p-values of 0.01-0.05 [1,2]; most \u0026ldquo;positive\u0026rdquo; trials and meta-analyses also have modest p-values.\u003cbr/\u003e\u003cbr/\u003e3. I provided equations for calculating the credibility of research findings with or without bias. Even without any bias, PPV probably remains below 0.50 for most non-randomized, non-large-scale circumstances.  Large trials and meta-analyses represent a minority of the literature.\u003cbr/\u003e\u003cbr/\u003e4. Figure 1 shows that bias can indeed make a difference. The proposed modelling has an additional useful feature: As type I and II errors decrease, PPV(max)\u003d1-[u/(R+u)], meaning that to allow a research finding to become \u0026gt;50% credible, we must first reduce bias at least below the pre-study odds of truth (u less than R).  Numerous studies demonstrate the strong presence of bias across research designs: indicative reference lists appear in [3-5].  We should understand bias and minimize it, not ignore it.\u003cbr/\u003e\u003cbr/\u003e5. \u0026ldquo;Hot fields\u0026rdquo;: Table 3 and figure 2 present \u0026ldquo;the probability that at least one study, among several done on the same question, claims a statistically significant finding\u0026rdquo;. They are not erroneous. Fields with many furtive competing teams may espouse significance-chasing behaviours, selectively highlighting \u0026ldquo;positive\u0026rdquo; results.  Conversely, having many teams with transparent availability of all results and integration of data across teams leads to genuine progress.  We need replication, not just discovery [3].\u003cbr/\u003e\u003cbr/\u003e6. The claim by two leading Bayesian methodologists that a Bayesian approach is somewhat circular and questionable contradicts Greenland\u0026rsquo;s own writings: \u0026ldquo;One misconception (of many) about Bayesian analyses is that prior distributions introduce assumptions that are more questionable than assumptions made by frequentist methods.\u0026rdquo; [6]\u003cbr/\u003e\u003cbr/\u003e7.  Empirical data on the refutation rates for various research designs agree with the estimates obtained in the proposed modelling [7], not with estimates ignoring bias.  Additional empirical research on these fronts would be very useful.\u003cbr/\u003e\u003cbr/\u003eScientific investigation is the noblest pursuit. I think we can improve the respect of the public for researchers by showing how difficult success is.  Confidence in the research enterprise is probably undermined primarily when we claim that discoveries are more certain than they really are and then the public, scientists, and patients suffer the painful refutations.\u003cbr/\u003e\u003cbr/\u003eReferences\u003cbr/\u003e1.\tPocock SJ, Collier TJ, Dandreo KJ, de Stavola BL, Goldman MB, et al (2004) Issues in the reporting of epidemiological studies: a survey of recent practice. BMJ 329:883.\u003cbr/\u003e2.\tKavvoura FK, Liberopoulos G, Ioannidis JP (2007) Selection in reported epidemiological risks: an empirical assessment. PLoS Med 2007;4:e79.\u003cbr/\u003e3.\tIoannidis JP (2006) Evolution and translation of research findings: from bench to where? PLoS Clin Trials 1:e36.\u003cbr/\u003e4.\tGluud LL (2006) Bias in clinical intervention research.  Am J Epidemiol 163:493-501.\u003cbr/\u003e5.\tCochrane Methodology Register. \u003ca href\u003d\"http://www3.cochrane.org/access_data/cmr/accessDB_cmr.asp\" title\u003d\"http://www3.cochrane.org/access_data/cmr/accessDB_cmr.asp\"\u003ehttp://www3.cochrane.org/...\u003c/a\u003e\u003cbr/\u003e6.\tGreenland S (2006) Bayesian perspectives for epidemiological research: I Foundations and basic methods.  Int J Epidemiol 35:765-75.\u003cbr/\u003e7.\tIoannidis JP (2005) Contradicted and initially stronger effects in highly cited clinical research. JAMA 294:218-28.\u003c/p\u003e",
    "originalBody": "Author: John Ioannidis\nPosition: Professor and Chairman\nInstitution: Dept. of Hygiene and Epidemiology, University of Ioannina School of Medicine\nE-mail: jioannid@cc.uoi.gr\nSubmitted Date: March 19, 2007\nPublished Date: March 19, 2007\nThis comment was originally posted as a “Reader Response” on the publication date indicated above. All Reader Responses are now available as comments.\n\nI thank Goodman and Greenland for their interesting comments.  Our methods and results are practically identical. However, some of my arguments are misrepresented.\n\n1. I did not “claim that no study or combination of studies can ever provide convincing evidence”.  In the illustrative examples (table 4) there is a wide credibility gradient (0.1% to 85%) for different research designs and settings.\n\n2. I did not assume that all significant p-values are around 0.05. Tables 1-3 and the respective PPV equations can use any p-value (alpha). Nevertheless, the p\u003d0.05 threshold is unfortunately entrenched in many scientific fields.  Almost half of the “positive” findings in recent observational studies have p-values of 0.01-0.05 [1,2]; most “positive” trials and meta-analyses also have modest p-values.\n\n3. I provided equations for calculating the credibility of research findings with or without bias. Even without any bias, PPV probably remains below 0.50 for most non-randomized, non-large-scale circumstances.  Large trials and meta-analyses represent a minority of the literature.\n\n4. Figure 1 shows that bias can indeed make a difference. The proposed modelling has an additional useful feature: As type I and II errors decrease, PPV(max)\u003d1-[u/(R+u)], meaning that to allow a research finding to become \u003e50% credible, we must first reduce bias at least below the pre-study odds of truth (u less than R).  Numerous studies demonstrate the strong presence of bias across research designs: indicative reference lists appear in [3-5].  We should understand bias and minimize it, not ignore it.\n\n5. “Hot fields”: Table 3 and figure 2 present “the probability that at least one study, among several done on the same question, claims a statistically significant finding”. They are not erroneous. Fields with many furtive competing teams may espouse significance-chasing behaviours, selectively highlighting “positive” results.  Conversely, having many teams with transparent availability of all results and integration of data across teams leads to genuine progress.  We need replication, not just discovery [3].\n\n6. The claim by two leading Bayesian methodologists that a Bayesian approach is somewhat circular and questionable contradicts Greenland’s own writings: “One misconception (of many) about Bayesian analyses is that prior distributions introduce assumptions that are more questionable than assumptions made by frequentist methods.” [6]\n\n7.  Empirical data on the refutation rates for various research designs agree with the estimates obtained in the proposed modelling [7], not with estimates ignoring bias.  Additional empirical research on these fronts would be very useful.\n\nScientific investigation is the noblest pursuit. I think we can improve the respect of the public for researchers by showing how difficult success is.  Confidence in the research enterprise is probably undermined primarily when we claim that discoveries are more certain than they really are and then the public, scientists, and patients suffer the painful refutations.\n\nReferences\n1.\tPocock SJ, Collier TJ, Dandreo KJ, de Stavola BL, Goldman MB, et al (2004) Issues in the reporting of epidemiological studies: a survey of recent practice. BMJ 329:883.\n2.\tKavvoura FK, Liberopoulos G, Ioannidis JP (2007) Selection in reported epidemiological risks: an empirical assessment. PLoS Med 2007;4:e79.\n3.\tIoannidis JP (2006) Evolution and translation of research findings: from bench to where? PLoS Clin Trials 1:e36.\n4.\tGluud LL (2006) Bias in clinical intervention research.  Am J Epidemiol 163:493-501.\n5.\tCochrane Methodology Register. http://www3.cochrane.org/access_data/cmr/accessDB_cmr.asp\n6.\tGreenland S (2006) Bayesian perspectives for epidemiological research: I Foundations and basic methods.  Int J Epidemiol 35:765-75.\n7.\tIoannidis JP (2005) Contradicted and initially stronger effects in highly cited clinical research. JAMA 294:218-28.",
    "truncatedBody": "\u003cp\u003eAuthor: John Ioannidis\u003cbr/\u003ePosition: Professor and Chairman\u003cbr/\u003eInstitution: Dept. of Hygiene and Epidemiology, University of Ioannina School of Medicine\u003cbr/\u003eE-mail: jioannid@cc.uoi.gr\u003cbr/\u003eSubmitted Date: March 19, 2007\u003cbr/\u003ePublished Date: March 19,...\u003c/p\u003e",
    "bodyWithUrlLinkingNoPTags": "Author: John Ioannidis\u003cbr/\u003ePosition: Professor and Chairman\u003cbr/\u003eInstitution: Dept. of Hygiene and Epidemiology, University of Ioannina School of Medicine\u003cbr/\u003eE-mail: jioannid@cc.uoi.gr\u003cbr/\u003eSubmitted Date: March 19, 2007\u003cbr/\u003ePublished Date: March 19, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eI thank Goodman and Greenland for their interesting comments.  Our methods and results are practically identical. However, some of my arguments are misrepresented.\u003cbr/\u003e\u003cbr/\u003e1. I did not \u0026ldquo;claim that no study or combination of studies can ever provide convincing evidence\u0026rdquo;.  In the illustrative examples (table 4) there is a wide credibility gradient (0.1% to 85%) for different research designs and settings.\u003cbr/\u003e\u003cbr/\u003e2. I did not assume that all significant p-values are around 0.05. Tables 1-3 and the respective PPV equations can use any p-value (alpha). Nevertheless, the p\u003d0.05 threshold is unfortunately entrenched in many scientific fields.  Almost half of the \u0026ldquo;positive\u0026rdquo; findings in recent observational studies have p-values of 0.01-0.05 [1,2]; most \u0026ldquo;positive\u0026rdquo; trials and meta-analyses also have modest p-values.\u003cbr/\u003e\u003cbr/\u003e3. I provided equations for calculating the credibility of research findings with or without bias. Even without any bias, PPV probably remains below 0.50 for most non-randomized, non-large-scale circumstances.  Large trials and meta-analyses represent a minority of the literature.\u003cbr/\u003e\u003cbr/\u003e4. Figure 1 shows that bias can indeed make a difference. The proposed modelling has an additional useful feature: As type I and II errors decrease, PPV(max)\u003d1-[u/(R+u)], meaning that to allow a research finding to become \u0026gt;50% credible, we must first reduce bias at least below the pre-study odds of truth (u less than R).  Numerous studies demonstrate the strong presence of bias across research designs: indicative reference lists appear in [3-5].  We should understand bias and minimize it, not ignore it.\u003cbr/\u003e\u003cbr/\u003e5. \u0026ldquo;Hot fields\u0026rdquo;: Table 3 and figure 2 present \u0026ldquo;the probability that at least one study, among several done on the same question, claims a statistically significant finding\u0026rdquo;. They are not erroneous. Fields with many furtive competing teams may espouse significance-chasing behaviours, selectively highlighting \u0026ldquo;positive\u0026rdquo; results.  Conversely, having many teams with transparent availability of all results and integration of data across teams leads to genuine progress.  We need replication, not just discovery [3].\u003cbr/\u003e\u003cbr/\u003e6. The claim by two leading Bayesian methodologists that a Bayesian approach is somewhat circular and questionable contradicts Greenland\u0026rsquo;s own writings: \u0026ldquo;One misconception (of many) about Bayesian analyses is that prior distributions introduce assumptions that are more questionable than assumptions made by frequentist methods.\u0026rdquo; [6]\u003cbr/\u003e\u003cbr/\u003e7.  Empirical data on the refutation rates for various research designs agree with the estimates obtained in the proposed modelling [7], not with estimates ignoring bias.  Additional empirical research on these fronts would be very useful.\u003cbr/\u003e\u003cbr/\u003eScientific investigation is the noblest pursuit. I think we can improve the respect of the public for researchers by showing how difficult success is.  Confidence in the research enterprise is probably undermined primarily when we claim that discoveries are more certain than they really are and then the public, scientists, and patients suffer the painful refutations.\u003cbr/\u003e\u003cbr/\u003eReferences\u003cbr/\u003e1.\tPocock SJ, Collier TJ, Dandreo KJ, de Stavola BL, Goldman MB, et al (2004) Issues in the reporting of epidemiological studies: a survey of recent practice. BMJ 329:883.\u003cbr/\u003e2.\tKavvoura FK, Liberopoulos G, Ioannidis JP (2007) Selection in reported epidemiological risks: an empirical assessment. PLoS Med 2007;4:e79.\u003cbr/\u003e3.\tIoannidis JP (2006) Evolution and translation of research findings: from bench to where? PLoS Clin Trials 1:e36.\u003cbr/\u003e4.\tGluud LL (2006) Bias in clinical intervention research.  Am J Epidemiol 163:493-501.\u003cbr/\u003e5.\tCochrane Methodology Register. \u003ca href\u003d\"http://www3.cochrane.org/access_data/cmr/accessDB_cmr.asp\" title\u003d\"http://www3.cochrane.org/access_data/cmr/accessDB_cmr.asp\"\u003ehttp://www3.cochrane.org/...\u003c/a\u003e\u003cbr/\u003e6.\tGreenland S (2006) Bayesian perspectives for epidemiological research: I Foundations and basic methods.  Int J Epidemiol 35:765-75.\u003cbr/\u003e7.\tIoannidis JP (2005) Contradicted and initially stronger effects in highly cited clinical research. JAMA 294:218-28.",
    "truncatedBodyWithUrlLinkingNoPTags": "Author: John Ioannidis\u003cbr/\u003ePosition: Professor and Chairman\u003cbr/\u003eInstitution: Dept. of Hygiene and Epidemiology, University of Ioannina School of Medicine\u003cbr/\u003eE-mail: jioannid@cc.uoi.gr\u003cbr/\u003eSubmitted Date: March 19, 2007\u003cbr/\u003ePublished Date: March 19,...",
    "bodyWithHighlightedText": "\u003cp\u003eAuthor: John Ioannidis\u003cbr/\u003ePosition: Professor and Chairman\u003cbr/\u003eInstitution: Dept. of Hygiene and Epidemiology, University of Ioannina School of Medicine\u003cbr/\u003eE-mail: jioannid@cc.uoi.gr\u003cbr/\u003eSubmitted Date: March 19, 2007\u003cbr/\u003ePublished Date: March 19, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eI thank Goodman and Greenland for their interesting comments.  Our methods and results are practically identical. However, some of my arguments are misrepresented.\u003cbr/\u003e\u003cbr/\u003e1. I did not \u0026ldquo;claim that no study or combination of studies can ever provide convincing evidence\u0026rdquo;.  In the illustrative examples (table 4) there is a wide credibility gradient (0.1% to 85%) for different research designs and settings.\u003cbr/\u003e\u003cbr/\u003e2. I did not assume that all significant p-values are around 0.05. Tables 1-3 and the respective PPV equations can use any p-value (alpha). Nevertheless, the p\u003d0.05 threshold is unfortunately entrenched in many scientific fields.  Almost half of the \u0026ldquo;positive\u0026rdquo; findings in recent observational studies have p-values of 0.01-0.05 [1,2]; most \u0026ldquo;positive\u0026rdquo; trials and meta-analyses also have modest p-values.\u003cbr/\u003e\u003cbr/\u003e3. I provided equations for calculating the credibility of research findings with or without bias. Even without any bias, PPV probably remains below 0.50 for most non-randomized, non-large-scale circumstances.  Large trials and meta-analyses represent a minority of the literature.\u003cbr/\u003e\u003cbr/\u003e4. Figure 1 shows that bias can indeed make a difference. The proposed modelling has an additional useful feature: As type I and II errors decrease, PPV(max)\u003d1-[u/(R+u)], meaning that to allow a research finding to become \u0026gt;50% credible, we must first reduce bias at least below the pre-study odds of truth (u less than R).  Numerous studies demonstrate the strong presence of bias across research designs: indicative reference lists appear in [3-5].  We should understand bias and minimize it, not ignore it.\u003cbr/\u003e\u003cbr/\u003e5. \u0026ldquo;Hot fields\u0026rdquo;: Table 3 and figure 2 present \u0026ldquo;the probability that at least one study, among several done on the same question, claims a statistically significant finding\u0026rdquo;. They are not erroneous. Fields with many furtive competing teams may espouse significance-chasing behaviours, selectively highlighting \u0026ldquo;positive\u0026rdquo; results.  Conversely, having many teams with transparent availability of all results and integration of data across teams leads to genuine progress.  We need replication, not just discovery [3].\u003cbr/\u003e\u003cbr/\u003e6. The claim by two leading Bayesian methodologists that a Bayesian approach is somewhat circular and questionable contradicts Greenland\u0026rsquo;s own writings: \u0026ldquo;One misconception (of many) about Bayesian analyses is that prior distributions introduce assumptions that are more questionable than assumptions made by frequentist methods.\u0026rdquo; [6]\u003cbr/\u003e\u003cbr/\u003e7.  Empirical data on the refutation rates for various research designs agree with the estimates obtained in the proposed modelling [7], not with estimates ignoring bias.  Additional empirical research on these fronts would be very useful.\u003cbr/\u003e\u003cbr/\u003eScientific investigation is the noblest pursuit. I think we can improve the respect of the public for researchers by showing how difficult success is.  Confidence in the research enterprise is probably undermined primarily when we claim that discoveries are more certain than they really are and then the public, scientists, and patients suffer the painful refutations.\u003cbr/\u003e\u003cbr/\u003eReferences\u003cbr/\u003e1.\tPocock SJ, Collier TJ, Dandreo KJ, de Stavola BL, Goldman MB, et al (2004) Issues in the reporting of epidemiological studies: a survey of recent practice. BMJ 329:883.\u003cbr/\u003e2.\tKavvoura FK, Liberopoulos G, Ioannidis JP (2007) Selection in reported epidemiological risks: an empirical assessment. PLoS Med 2007;4:e79.\u003cbr/\u003e3.\tIoannidis JP (2006) Evolution and translation of research findings: from bench to where? PLoS Clin Trials 1:e36.\u003cbr/\u003e4.\tGluud LL (2006) Bias in clinical intervention research.  Am J Epidemiol 163:493-501.\u003cbr/\u003e5.\tCochrane Methodology Register. \u003ca href\u003d\"http://www3.cochrane.org/access_data/cmr/accessDB_cmr.asp\" title\u003d\"http://www3.cochrane.org/access_data/cmr/accessDB_cmr.asp\"\u003ehttp://www3.cochrane.org/...\u003c/a\u003e\u003cbr/\u003e6.\tGreenland S (2006) Bayesian perspectives for epidemiological research: I Foundations and basic methods.  Int J Epidemiol 35:765-75.\u003cbr/\u003e7.\tIoannidis JP (2005) Contradicted and initially stronger effects in highly cited clinical research. JAMA 294:218-28.\u003c/p\u003e",
    "competingInterestStatement": "",
    "truncatedCompetingInterestStatement": "",
    "annotationUri": "10.1371/annotation/da1831bd-f8dd-40c8-9b42-d61caadfbc64",
    "creatorID": 172479,
    "creatorDisplayName": "plosmedicine",
    "creatorFormattedName": "PLoS Medicine Staff",
    "articleID": 16292,
    "articleDoi": "info:doi/10.1371/journal.pmed.0020124",
    "articleTitle": "Why Most Published Research Findings Are False",
    "created": "2009-03-31T00:06:52Z",
    "createdFormatted": "2009-03-31T00:06:52Z",
    "type": "COMMENT",
    "replies": [],
    "lastReplyDate": "2009-03-31T00:06:52Z",
    "totalNumReplies": 0
  },
  {
    "originalTitle": "Healthy Skepticism",
    "title": "Healthy Skepticism",
    "body": "\u003cp\u003eAuthor: Elizabeth Ubaldo\u003cbr/\u003ePosition: Student\u003cbr/\u003eInstitution: UCSF Dental\u003cbr/\u003eE-mail: be7485@gmail.com\u003cbr/\u003eSubmitted Date: October 06, 2007\u003cbr/\u003ePublished Date: October 8, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eThis article really opened my eyes to how hard it is to find \u0026quot;the gold standard\u0026quot; when searching through research findings.  I also realized that having a healthy skepticism will help me weed though all of the biases and skewed results.\u003c/p\u003e",
    "originalBody": "Author: Elizabeth Ubaldo\nPosition: Student\nInstitution: UCSF Dental\nE-mail: be7485@gmail.com\nSubmitted Date: October 06, 2007\nPublished Date: October 8, 2007\nThis comment was originally posted as a “Reader Response” on the publication date indicated above. All Reader Responses are now available as comments.\n\nThis article really opened my eyes to how hard it is to find \"the gold standard\" when searching through research findings.  I also realized that having a healthy skepticism will help me weed though all of the biases and skewed results.",
    "truncatedBody": "\u003cp\u003eAuthor: Elizabeth Ubaldo\u003cbr/\u003ePosition: Student\u003cbr/\u003eInstitution: UCSF Dental\u003cbr/\u003eE-mail: be7485@gmail.com\u003cbr/\u003eSubmitted Date: October 06, 2007\u003cbr/\u003ePublished Date: October 8, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo;...\u003c/p\u003e",
    "bodyWithUrlLinkingNoPTags": "Author: Elizabeth Ubaldo\u003cbr/\u003ePosition: Student\u003cbr/\u003eInstitution: UCSF Dental\u003cbr/\u003eE-mail: be7485@gmail.com\u003cbr/\u003eSubmitted Date: October 06, 2007\u003cbr/\u003ePublished Date: October 8, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eThis article really opened my eyes to how hard it is to find \u0026quot;the gold standard\u0026quot; when searching through research findings.  I also realized that having a healthy skepticism will help me weed though all of the biases and skewed results.",
    "truncatedBodyWithUrlLinkingNoPTags": "Author: Elizabeth Ubaldo\u003cbr/\u003ePosition: Student\u003cbr/\u003eInstitution: UCSF Dental\u003cbr/\u003eE-mail: be7485@gmail.com\u003cbr/\u003eSubmitted Date: October 06, 2007\u003cbr/\u003ePublished Date: October 8, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo;...",
    "bodyWithHighlightedText": "\u003cp\u003eAuthor: Elizabeth Ubaldo\u003cbr/\u003ePosition: Student\u003cbr/\u003eInstitution: UCSF Dental\u003cbr/\u003eE-mail: be7485@gmail.com\u003cbr/\u003eSubmitted Date: October 06, 2007\u003cbr/\u003ePublished Date: October 8, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eThis article really opened my eyes to how hard it is to find \u0026quot;the gold standard\u0026quot; when searching through research findings.  I also realized that having a healthy skepticism will help me weed though all of the biases and skewed results.\u003c/p\u003e",
    "competingInterestStatement": "",
    "truncatedCompetingInterestStatement": "",
    "annotationUri": "10.1371/annotation/e687ccff-c192-4882-b8b6-eed02139b2a1",
    "creatorID": 172479,
    "creatorDisplayName": "plosmedicine",
    "creatorFormattedName": "PLoS Medicine Staff",
    "articleID": 16292,
    "articleDoi": "info:doi/10.1371/journal.pmed.0020124",
    "articleTitle": "Why Most Published Research Findings Are False",
    "created": "2009-03-31T00:15:13Z",
    "createdFormatted": "2009-03-31T00:15:13Z",
    "type": "COMMENT",
    "replies": [],
    "lastReplyDate": "2009-03-31T00:15:13Z",
    "totalNumReplies": 0
  },
  {
    "originalTitle": "Independent Teams",
    "title": "Independent Teams",
    "body": "\u003cp\u003eAuthor: Grayson Palmer\u003cbr/\u003ePosition: Student\u003cbr/\u003eInstitution: UCSF\u003cbr/\u003eE-mail: GraysonPalmer@gmail.com\u003cbr/\u003eSubmitted Date: October 03, 2007\u003cbr/\u003ePublished Date: October 4, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eDr. Ioannidis is correct in saying that as more researchers look into the same question that, by chance, researchers will obtain incorrect results. However this does not mean that most research findings are false. The chance of obtaining erroneous results is based on the P-value. The strength of research comes by meta-analysis of the \u0026quot;whole\u0026quot; of the research. Those who perform meta analysis\u0026rsquo;s realize there is erroneous research and place strict criteria to eliminate bias in all forms.\u003cbr/\u003e\u003cbr/\u003eAlso large randomized control studies are the best research possible because researchers are able to control variables and create double and even triple blind experiments to greatly reduce bias and even reveal actual cause and effect of variables not just correlation.\u003c/p\u003e",
    "originalBody": "Author: Grayson Palmer\nPosition: Student\nInstitution: UCSF\nE-mail: GraysonPalmer@gmail.com\nSubmitted Date: October 03, 2007\nPublished Date: October 4, 2007\nThis comment was originally posted as a “Reader Response” on the publication date indicated above. All Reader Responses are now available as comments.\n\nDr. Ioannidis is correct in saying that as more researchers look into the same question that, by chance, researchers will obtain incorrect results. However this does not mean that most research findings are false. The chance of obtaining erroneous results is based on the P-value. The strength of research comes by meta-analysis of the \"whole\" of the research. Those who perform meta analysis’s realize there is erroneous research and place strict criteria to eliminate bias in all forms.\n\nAlso large randomized control studies are the best research possible because researchers are able to control variables and create double and even triple blind experiments to greatly reduce bias and even reveal actual cause and effect of variables not just correlation.",
    "truncatedBody": "\u003cp\u003eAuthor: Grayson Palmer\u003cbr/\u003ePosition: Student\u003cbr/\u003eInstitution: UCSF\u003cbr/\u003eE-mail: GraysonPalmer@gmail.com\u003cbr/\u003eSubmitted Date: October 03, 2007\u003cbr/\u003ePublished Date: October 4, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on...\u003c/p\u003e",
    "bodyWithUrlLinkingNoPTags": "Author: Grayson Palmer\u003cbr/\u003ePosition: Student\u003cbr/\u003eInstitution: UCSF\u003cbr/\u003eE-mail: GraysonPalmer@gmail.com\u003cbr/\u003eSubmitted Date: October 03, 2007\u003cbr/\u003ePublished Date: October 4, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eDr. Ioannidis is correct in saying that as more researchers look into the same question that, by chance, researchers will obtain incorrect results. However this does not mean that most research findings are false. The chance of obtaining erroneous results is based on the P-value. The strength of research comes by meta-analysis of the \u0026quot;whole\u0026quot; of the research. Those who perform meta analysis\u0026rsquo;s realize there is erroneous research and place strict criteria to eliminate bias in all forms.\u003cbr/\u003e\u003cbr/\u003eAlso large randomized control studies are the best research possible because researchers are able to control variables and create double and even triple blind experiments to greatly reduce bias and even reveal actual cause and effect of variables not just correlation.",
    "truncatedBodyWithUrlLinkingNoPTags": "Author: Grayson Palmer\u003cbr/\u003ePosition: Student\u003cbr/\u003eInstitution: UCSF\u003cbr/\u003eE-mail: GraysonPalmer@gmail.com\u003cbr/\u003eSubmitted Date: October 03, 2007\u003cbr/\u003ePublished Date: October 4, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on...",
    "bodyWithHighlightedText": "\u003cp\u003eAuthor: Grayson Palmer\u003cbr/\u003ePosition: Student\u003cbr/\u003eInstitution: UCSF\u003cbr/\u003eE-mail: GraysonPalmer@gmail.com\u003cbr/\u003eSubmitted Date: October 03, 2007\u003cbr/\u003ePublished Date: October 4, 2007\u003cbr/\u003eThis comment was originally posted as a \u0026ldquo;Reader Response\u0026rdquo; on the publication date indicated above. All Reader Responses are now available as comments.\u003cbr/\u003e\u003cbr/\u003eDr. Ioannidis is correct in saying that as more researchers look into the same question that, by chance, researchers will obtain incorrect results. However this does not mean that most research findings are false. The chance of obtaining erroneous results is based on the P-value. The strength of research comes by meta-analysis of the \u0026quot;whole\u0026quot; of the research. Those who perform meta analysis\u0026rsquo;s realize there is erroneous research and place strict criteria to eliminate bias in all forms.\u003cbr/\u003e\u003cbr/\u003eAlso large randomized control studies are the best research possible because researchers are able to control variables and create double and even triple blind experiments to greatly reduce bias and even reveal actual cause and effect of variables not just correlation.\u003c/p\u003e",
    "competingInterestStatement": "",
    "truncatedCompetingInterestStatement": "",
    "annotationUri": "10.1371/annotation/eebf5bb2-3e59-4ed4-89da-2f86604f6a44",
    "creatorID": 172479,
    "creatorDisplayName": "plosmedicine",
    "creatorFormattedName": "PLoS Medicine Staff",
    "articleID": 16292,
    "articleDoi": "info:doi/10.1371/journal.pmed.0020124",
    "articleTitle": "Why Most Published Research Findings Are False",
    "created": "2009-03-31T00:14:46Z",
    "createdFormatted": "2009-03-31T00:14:46Z",
    "type": "COMMENT",
    "replies": [],
    "lastReplyDate": "2009-03-31T00:14:46Z",
    "totalNumReplies": 0
  },
  {
    "originalTitle": "",
    "title": "",
    "body": "\u003cp\u003eA classic already.\u003c/p\u003e",
    "originalBody": "A classic already.",
    "truncatedBody": "\u003cp\u003eA classic already.\u003c/p\u003e",
    "bodyWithUrlLinkingNoPTags": "A classic already.",
    "truncatedBodyWithUrlLinkingNoPTags": "A classic already.",
    "bodyWithHighlightedText": "\u003cp\u003eA classic already.\u003c/p\u003e",
    "competingInterestStatement": "",
    "truncatedCompetingInterestStatement": "",
    "annotationUri": "10.1371/annotation/4369cf35-8d46-43d7-ab36-b8d3a437bc85",
    "creatorID": 107797,
    "creatorDisplayName": "Richard1",
    "creatorFormattedName": "Richard Smith",
    "articleID": 16292,
    "articleDoi": "info:doi/10.1371/journal.pmed.0020124",
    "articleTitle": "Why Most Published Research Findings Are False",
    "created": "2009-09-17T15:13:02Z",
    "createdFormatted": "2009-09-17T15:13:02Z",
    "type": "COMMENT",
    "replies": [],
    "lastReplyDate": "2009-09-17T15:13:02Z",
    "totalNumReplies": 0
  },
  {
    "originalTitle": "A Critique of Ioannidis JPA (2005) ",
    "title": "A Critique of Ioannidis JPA (2005) ",
    "body": "\u003cp\u003eA Critique of Ioannidis JPA (2005) Why Most Published Research Findings Are False. PLoS Med 2(8)\u003cbr/\u003eby Hermann Vetter\u003cbr/\u003e\u003cbr/\u003eNull hypotheses are mostly point hypotheses stating that some (frequency-, mean-) difference, or correlation, is zero. Empirically, an exact value of zero occurs virtually never. Hence H0 is virtually always false.  Ioannidis\u0027 R is defined as \u0027the ratio of the number of \u0026quot;true relationships\u0026quot; to \u0026quot;no relationships\u0026quot; among those tested in the field\u0027. H0 being always false means that there are only true relationships, i.e., that R has only a single value (infinity). Hence his analyses involving different values of R do not refer at all to the by far most frequent type of H0, but only to a rare non-point H0 like \u0026quot;boys are better than girls in math\u0026quot;, with the negation \u0026quot;boys  are equal or lower\u0026quot;. This means that his main contention would have to be radically reconsidered for the case of a usual H0.\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e\u003c/p\u003e",
    "originalBody": "A Critique of Ioannidis JPA (2005) Why Most Published Research Findings Are False. PLoS Med 2(8)\nby Hermann Vetter\n\nNull hypotheses are mostly point hypotheses stating that some (frequency-, mean-) difference, or correlation, is zero. Empirically, an exact value of zero occurs virtually never. Hence H0 is virtually always false.  Ioannidis\u0027 R is defined as \u0027the ratio of the number of \"true relationships\" to \"no relationships\" among those tested in the field\u0027. H0 being always false means that there are only true relationships, i.e., that R has only a single value (infinity). Hence his analyses involving different values of R do not refer at all to the by far most frequent type of H0, but only to a rare non-point H0 like \"boys are better than girls in math\", with the negation \"boys  are equal or lower\". This means that his main contention would have to be radically reconsidered for the case of a usual H0.\n\n\n",
    "truncatedBody": "\u003cp\u003eA Critique of Ioannidis JPA (2005) Why Most Published Research Findings Are False. PLoS Med 2(8)\u003cbr/\u003eby Hermann Vetter\u003cbr/\u003e\u003cbr/\u003eNull hypotheses are mostly point hypotheses stating that some (frequency-, mean-) difference, or correlation, is zero....\u003c/p\u003e",
    "bodyWithUrlLinkingNoPTags": "A Critique of Ioannidis JPA (2005) Why Most Published Research Findings Are False. PLoS Med 2(8)\u003cbr/\u003eby Hermann Vetter\u003cbr/\u003e\u003cbr/\u003eNull hypotheses are mostly point hypotheses stating that some (frequency-, mean-) difference, or correlation, is zero. Empirically, an exact value of zero occurs virtually never. Hence H0 is virtually always false.  Ioannidis\u0027 R is defined as \u0027the ratio of the number of \u0026quot;true relationships\u0026quot; to \u0026quot;no relationships\u0026quot; among those tested in the field\u0027. H0 being always false means that there are only true relationships, i.e., that R has only a single value (infinity). Hence his analyses involving different values of R do not refer at all to the by far most frequent type of H0, but only to a rare non-point H0 like \u0026quot;boys are better than girls in math\u0026quot;, with the negation \u0026quot;boys  are equal or lower\u0026quot;. This means that his main contention would have to be radically reconsidered for the case of a usual H0.\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e",
    "truncatedBodyWithUrlLinkingNoPTags": "A Critique of Ioannidis JPA (2005) Why Most Published Research Findings Are False. PLoS Med 2(8)\u003cbr/\u003eby Hermann Vetter\u003cbr/\u003e\u003cbr/\u003eNull hypotheses are mostly point hypotheses stating that some (frequency-, mean-) difference, or correlation, is zero....",
    "bodyWithHighlightedText": "\u003cp\u003eA Critique of Ioannidis JPA (2005) Why Most Published Research Findings Are False. PLoS Med 2(8)\u003cbr/\u003eby Hermann Vetter\u003cbr/\u003e\u003cbr/\u003eNull hypotheses are mostly point hypotheses stating that some (frequency-, mean-) difference, or correlation, is zero. Empirically, an exact value of zero occurs virtually never. Hence H0 is virtually always false.  Ioannidis\u0027 R is defined as \u0027the ratio of the number of \u0026quot;true relationships\u0026quot; to \u0026quot;no relationships\u0026quot; among those tested in the field\u0027. H0 being always false means that there are only true relationships, i.e., that R has only a single value (infinity). Hence his analyses involving different values of R do not refer at all to the by far most frequent type of H0, but only to a rare non-point H0 like \u0026quot;boys are better than girls in math\u0026quot;, with the negation \u0026quot;boys  are equal or lower\u0026quot;. This means that his main contention would have to be radically reconsidered for the case of a usual H0.\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e\u003c/p\u003e",
    "competingInterestStatement": "",
    "truncatedCompetingInterestStatement": "",
    "annotationUri": "10.1371/annotation/6900b9c4-7c49-4adb-a7bf-699caa98ea62",
    "creatorID": 302771,
    "creatorDisplayName": "vetter",
    "creatorFormattedName": "Hermann Vetter",
    "articleID": 16292,
    "articleDoi": "info:doi/10.1371/journal.pmed.0020124",
    "articleTitle": "Why Most Published Research Findings Are False",
    "created": "2013-01-17T16:55:42Z",
    "createdFormatted": "2013-01-17T16:55:42Z",
    "type": "COMMENT",
    "replies": [],
    "lastReplyDate": "2013-01-17T16:55:42Z",
    "totalNumReplies": 0
  },
  {
    "originalTitle": "A Critique of Ioannidis JPA (2005) Why Most Published Research Findings Are False. PLoS Med 2(8)  II: proposals",
    "title": "A Critique of Ioannidis JPA (2005) Why Most Published Research Findings Are False. PLoS Med 2(8)  II: proposals",
    "body": "\u003cp\u003eA Critique of Ioannidis JPA (2005) Why Most Published Research Findings Are False. PLoS Med 2(8)\u003cbr/\u003e\u003cbr/\u003eII: proposals\u003cbr/\u003e\u003cbr/\u003eby Hermann Vetter\u003cbr/\u003e\u003cbr/\u003ePoint null hypotheses are virtually always false. The question is how far the true value of the parameter is distant from zero. If the power function of the test is sound: the greater the distance, the greater the probability of rejecting H0; and the larger the sample, the more quickly the probability rises. Some consequences:\u003cbr/\u003e\u003cbr/\u003e1. The larger the sample, the closer the probability of rejecting H0 will be to unity for any given difference of the value of the parameter from zero. If the sample is too large, differences so small will turn out significant that they would better be considered insignificant in a substantive sense. The golden road is interval estimation (which, by the way, can also be used for hypothesis testing).\u003cbr/\u003e\u003cbr/\u003e2. If, say, in 10% of N tests the difference from zero is large enough to be detected by virtue of the power of the test, and to be considered as substantively significant; and tiny in the other 90%; then there will be 0.1N \u0026quot;genuinely\u0026quot; significant results and pN or 0.05N \u0026quot;spuriously\u0026quot; significant ones; i.e., one third of the significant results is spurious. This rate of infection can be made arbitrarily small by choosing a sufficiently low p. This can be done within a single study, or within any number of studies seen together.\u003cbr/\u003e\u003cbr/\u003eI do not think that as far as hypothesis testing is concerned, the larger the sample, the better; cf. (1) above. But I think that as far as hypothesis testing is not avoided - so to speak -, a very elastic choice of significance level will prevent that most research findings are false.\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e\u003c/p\u003e",
    "originalBody": "A Critique of Ioannidis JPA (2005) Why Most Published Research Findings Are False. PLoS Med 2(8)\n\nII: proposals\n\nby Hermann Vetter\n\nPoint null hypotheses are virtually always false. The question is how far the true value of the parameter is distant from zero. If the power function of the test is sound: the greater the distance, the greater the probability of rejecting H0; and the larger the sample, the more quickly the probability rises. Some consequences:\n\n1. The larger the sample, the closer the probability of rejecting H0 will be to unity for any given difference of the value of the parameter from zero. If the sample is too large, differences so small will turn out significant that they would better be considered insignificant in a substantive sense. The golden road is interval estimation (which, by the way, can also be used for hypothesis testing).\n\n2. If, say, in 10% of N tests the difference from zero is large enough to be detected by virtue of the power of the test, and to be considered as substantively significant; and tiny in the other 90%; then there will be 0.1N \"genuinely\" significant results and pN or 0.05N \"spuriously\" significant ones; i.e., one third of the significant results is spurious. This rate of infection can be made arbitrarily small by choosing a sufficiently low p. This can be done within a single study, or within any number of studies seen together.\n\nI do not think that as far as hypothesis testing is concerned, the larger the sample, the better; cf. (1) above. But I think that as far as hypothesis testing is not avoided - so to speak -, a very elastic choice of significance level will prevent that most research findings are false.\n\n\n\n",
    "truncatedBody": "\u003cp\u003eA Critique of Ioannidis JPA (2005) Why Most Published Research Findings Are False. PLoS Med 2(8)\u003cbr/\u003e\u003cbr/\u003eII: proposals\u003cbr/\u003e\u003cbr/\u003eby Hermann Vetter\u003cbr/\u003e\u003cbr/\u003ePoint null hypotheses are virtually always false. The question is how far the true value of the...\u003c/p\u003e",
    "bodyWithUrlLinkingNoPTags": "A Critique of Ioannidis JPA (2005) Why Most Published Research Findings Are False. PLoS Med 2(8)\u003cbr/\u003e\u003cbr/\u003eII: proposals\u003cbr/\u003e\u003cbr/\u003eby Hermann Vetter\u003cbr/\u003e\u003cbr/\u003ePoint null hypotheses are virtually always false. The question is how far the true value of the parameter is distant from zero. If the power function of the test is sound: the greater the distance, the greater the probability of rejecting H0; and the larger the sample, the more quickly the probability rises. Some consequences:\u003cbr/\u003e\u003cbr/\u003e1. The larger the sample, the closer the probability of rejecting H0 will be to unity for any given difference of the value of the parameter from zero. If the sample is too large, differences so small will turn out significant that they would better be considered insignificant in a substantive sense. The golden road is interval estimation (which, by the way, can also be used for hypothesis testing).\u003cbr/\u003e\u003cbr/\u003e2. If, say, in 10% of N tests the difference from zero is large enough to be detected by virtue of the power of the test, and to be considered as substantively significant; and tiny in the other 90%; then there will be 0.1N \u0026quot;genuinely\u0026quot; significant results and pN or 0.05N \u0026quot;spuriously\u0026quot; significant ones; i.e., one third of the significant results is spurious. This rate of infection can be made arbitrarily small by choosing a sufficiently low p. This can be done within a single study, or within any number of studies seen together.\u003cbr/\u003e\u003cbr/\u003eI do not think that as far as hypothesis testing is concerned, the larger the sample, the better; cf. (1) above. But I think that as far as hypothesis testing is not avoided - so to speak -, a very elastic choice of significance level will prevent that most research findings are false.\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e",
    "truncatedBodyWithUrlLinkingNoPTags": "A Critique of Ioannidis JPA (2005) Why Most Published Research Findings Are False. PLoS Med 2(8)\u003cbr/\u003e\u003cbr/\u003eII: proposals\u003cbr/\u003e\u003cbr/\u003eby Hermann Vetter\u003cbr/\u003e\u003cbr/\u003ePoint null hypotheses are virtually always false. The question is how far the true value of the...",
    "bodyWithHighlightedText": "\u003cp\u003eA Critique of Ioannidis JPA (2005) Why Most Published Research Findings Are False. PLoS Med 2(8)\u003cbr/\u003e\u003cbr/\u003eII: proposals\u003cbr/\u003e\u003cbr/\u003eby Hermann Vetter\u003cbr/\u003e\u003cbr/\u003ePoint null hypotheses are virtually always false. The question is how far the true value of the parameter is distant from zero. If the power function of the test is sound: the greater the distance, the greater the probability of rejecting H0; and the larger the sample, the more quickly the probability rises. Some consequences:\u003cbr/\u003e\u003cbr/\u003e1. The larger the sample, the closer the probability of rejecting H0 will be to unity for any given difference of the value of the parameter from zero. If the sample is too large, differences so small will turn out significant that they would better be considered insignificant in a substantive sense. The golden road is interval estimation (which, by the way, can also be used for hypothesis testing).\u003cbr/\u003e\u003cbr/\u003e2. If, say, in 10% of N tests the difference from zero is large enough to be detected by virtue of the power of the test, and to be considered as substantively significant; and tiny in the other 90%; then there will be 0.1N \u0026quot;genuinely\u0026quot; significant results and pN or 0.05N \u0026quot;spuriously\u0026quot; significant ones; i.e., one third of the significant results is spurious. This rate of infection can be made arbitrarily small by choosing a sufficiently low p. This can be done within a single study, or within any number of studies seen together.\u003cbr/\u003e\u003cbr/\u003eI do not think that as far as hypothesis testing is concerned, the larger the sample, the better; cf. (1) above. But I think that as far as hypothesis testing is not avoided - so to speak -, a very elastic choice of significance level will prevent that most research findings are false.\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e\u003c/p\u003e",
    "competingInterestStatement": "",
    "truncatedCompetingInterestStatement": "",
    "annotationUri": "10.1371/annotation/093dbd50-838f-4e28-aff5-8f8e75190815",
    "creatorID": 302771,
    "creatorDisplayName": "vetter",
    "creatorFormattedName": "Hermann Vetter",
    "articleID": 16292,
    "articleDoi": "info:doi/10.1371/journal.pmed.0020124",
    "articleTitle": "Why Most Published Research Findings Are False",
    "created": "2013-01-19T10:01:03Z",
    "createdFormatted": "2013-01-19T10:01:03Z",
    "type": "COMMENT",
    "replies": [],
    "lastReplyDate": "2013-01-19T10:01:03Z",
    "totalNumReplies": 0
  },
  {
    "originalTitle": "Surely the answer is Bayes theorem?",
    "title": "Surely the answer is Bayes theorem?",
    "body": "\u003cp\u003eThere are many difficulties with using p-values and the 5% cut-off level (alpha) to analyze results: this paper homes in on one of them. The point is so obvious that I think it deserves a more straightforward numerical example working from first principles. Lets assume there are 1000 relationships being probed and that samples are large so we can take the power to be 100% - i.e. if there is a relationship the study will definitely find it. Assume further that 1% of these relationships are true. Then, of the 1000 relationships about 10 will be true (and so found by the studies) and a further 50 (5% of 1000 or strictly 990) or so will be false positives. This means that 10 out the sixty significant results are genuine - i..e 10/60 or 17% of the reported significant results will be true. This is clearly likely to be less with lower power levels and bias.\u003cbr/\u003e\u003cbr/\u003eThis is, however, so obvious that I find it difficult to believe that researchers are not aware of this. The problem is the uncritical use of p-values. There is, of course, a very large literature on these problems.\u003cbr/\u003e\u003cbr/\u003eThere are two obvious solutions, hinted at, but not made explicit in the article. First, avoid the uncritical use of p-values. Second, use Bayes theorem. The Bayesian approach to statistics takes account of prior probabilities explicitly which is what is wanted here.\u003cbr/\u003e\u003cbr/\u003eMichael Wood\u003cbr/\u003ePortsmouth Business School\u003cbr/\u003emichael-wood@myport.ac.uk\u003c/p\u003e",
    "originalBody": "There are many difficulties with using p-values and the 5% cut-off level (alpha) to analyze results: this paper homes in on one of them. The point is so obvious that I think it deserves a more straightforward numerical example working from first principles. Lets assume there are 1000 relationships being probed and that samples are large so we can take the power to be 100% - i.e. if there is a relationship the study will definitely find it. Assume further that 1% of these relationships are true. Then, of the 1000 relationships about 10 will be true (and so found by the studies) and a further 50 (5% of 1000 or strictly 990) or so will be false positives. This means that 10 out the sixty significant results are genuine - i..e 10/60 or 17% of the reported significant results will be true. This is clearly likely to be less with lower power levels and bias.\n\nThis is, however, so obvious that I find it difficult to believe that researchers are not aware of this. The problem is the uncritical use of p-values. There is, of course, a very large literature on these problems.\n\nThere are two obvious solutions, hinted at, but not made explicit in the article. First, avoid the uncritical use of p-values. Second, use Bayes theorem. The Bayesian approach to statistics takes account of prior probabilities explicitly which is what is wanted here.\n\nMichael Wood\nPortsmouth Business School\nmichael-wood@myport.ac.uk",
    "truncatedBody": "\u003cp\u003eThere are many difficulties with using p-values and the 5% cut-off level (alpha) to analyze results: this paper homes in on one of them. The point is so obvious that I think it deserves a more straightforward numerical example working from first...\u003c/p\u003e",
    "bodyWithUrlLinkingNoPTags": "There are many difficulties with using p-values and the 5% cut-off level (alpha) to analyze results: this paper homes in on one of them. The point is so obvious that I think it deserves a more straightforward numerical example working from first principles. Lets assume there are 1000 relationships being probed and that samples are large so we can take the power to be 100% - i.e. if there is a relationship the study will definitely find it. Assume further that 1% of these relationships are true. Then, of the 1000 relationships about 10 will be true (and so found by the studies) and a further 50 (5% of 1000 or strictly 990) or so will be false positives. This means that 10 out the sixty significant results are genuine - i..e 10/60 or 17% of the reported significant results will be true. This is clearly likely to be less with lower power levels and bias.\u003cbr/\u003e\u003cbr/\u003eThis is, however, so obvious that I find it difficult to believe that researchers are not aware of this. The problem is the uncritical use of p-values. There is, of course, a very large literature on these problems.\u003cbr/\u003e\u003cbr/\u003eThere are two obvious solutions, hinted at, but not made explicit in the article. First, avoid the uncritical use of p-values. Second, use Bayes theorem. The Bayesian approach to statistics takes account of prior probabilities explicitly which is what is wanted here.\u003cbr/\u003e\u003cbr/\u003eMichael Wood\u003cbr/\u003ePortsmouth Business School\u003cbr/\u003emichael-wood@myport.ac.uk",
    "truncatedBodyWithUrlLinkingNoPTags": "There are many difficulties with using p-values and the 5% cut-off level (alpha) to analyze results: this paper homes in on one of them. The point is so obvious that I think it deserves a more straightforward numerical example working from first...",
    "bodyWithHighlightedText": "\u003cp\u003eThere are many difficulties with using p-values and the 5% cut-off level (alpha) to analyze results: this paper homes in on one of them. The point is so obvious that I think it deserves a more straightforward numerical example working from first principles. Lets assume there are 1000 relationships being probed and that samples are large so we can take the power to be 100% - i.e. if there is a relationship the study will definitely find it. Assume further that 1% of these relationships are true. Then, of the 1000 relationships about 10 will be true (and so found by the studies) and a further 50 (5% of 1000 or strictly 990) or so will be false positives. This means that 10 out the sixty significant results are genuine - i..e 10/60 or 17% of the reported significant results will be true. This is clearly likely to be less with lower power levels and bias.\u003cbr/\u003e\u003cbr/\u003eThis is, however, so obvious that I find it difficult to believe that researchers are not aware of this. The problem is the uncritical use of p-values. There is, of course, a very large literature on these problems.\u003cbr/\u003e\u003cbr/\u003eThere are two obvious solutions, hinted at, but not made explicit in the article. First, avoid the uncritical use of p-values. Second, use Bayes theorem. The Bayesian approach to statistics takes account of prior probabilities explicitly which is what is wanted here.\u003cbr/\u003e\u003cbr/\u003eMichael Wood\u003cbr/\u003ePortsmouth Business School\u003cbr/\u003emichael-wood@myport.ac.uk\u003c/p\u003e",
    "competingInterestStatement": "",
    "truncatedCompetingInterestStatement": "",
    "annotationUri": "10.1371/annotation/e9db600c-0f7c-4850-a094-0ff4232658e2",
    "creatorID": 357479,
    "creatorDisplayName": "mickofemsworth",
    "creatorFormattedName": "Michael Wood",
    "articleID": 16292,
    "articleDoi": "info:doi/10.1371/journal.pmed.0020124",
    "articleTitle": "Why Most Published Research Findings Are False",
    "created": "2013-07-30T15:29:03Z",
    "createdFormatted": "2013-07-30T15:29:03Z",
    "type": "COMMENT",
    "replies": [],
    "lastReplyDate": "2013-07-30T15:29:03Z",
    "totalNumReplies": 0
  },
  {
    "originalTitle": "A small group research.",
    "title": "A small group research.",
    "body": "\u003cp\u003eThe biggest problem is to work with a small group of patients. Small group af people under investigation don\u0027t give real results. But some times it is impossible to collect data from a large group, because of very expensive research.\u003c/p\u003e",
    "originalBody": "The biggest problem is to work with a small group of patients. Small group af people under investigation don\u0027t give real results. But some times it is impossible to collect data from a large group, because of very expensive research.",
    "truncatedBody": "\u003cp\u003eThe biggest problem is to work with a small group of patients. Small group af people under investigation don\u0027t give real results. But some times it is impossible to collect data from a large group, because of very expensive research.\u003c/p\u003e",
    "bodyWithUrlLinkingNoPTags": "The biggest problem is to work with a small group of patients. Small group af people under investigation don\u0027t give real results. But some times it is impossible to collect data from a large group, because of very expensive research.",
    "truncatedBodyWithUrlLinkingNoPTags": "The biggest problem is to work with a small group of patients. Small group af people under investigation don\u0027t give real results. But some times it is impossible to collect data from a large group, because of very expensive research.",
    "bodyWithHighlightedText": "\u003cp\u003eThe biggest problem is to work with a small group of patients. Small group af people under investigation don\u0027t give real results. But some times it is impossible to collect data from a large group, because of very expensive research.\u003c/p\u003e",
    "competingInterestStatement": "",
    "truncatedCompetingInterestStatement": "",
    "annotationUri": "10.1371/annotation/6f650df8-f3da-4562-bcdc-e4f05b085cfc",
    "creatorID": 268103,
    "creatorDisplayName": "samgul",
    "creatorFormattedName": "Gulnara Samigulina",
    "articleID": 16292,
    "articleDoi": "info:doi/10.1371/journal.pmed.0020124",
    "articleTitle": "Why Most Published Research Findings Are False",
    "created": "2013-10-27T22:03:35Z",
    "createdFormatted": "2013-10-27T22:03:35Z",
    "type": "COMMENT",
    "replies": [],
    "lastReplyDate": "2013-10-27T22:03:35Z",
    "totalNumReplies": 0
  }
]
